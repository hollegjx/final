#!/usr/bin/env python3
"""
è¶…ç±»è®­ç»ƒè„šæœ¬ - åŸºäºGCDé¡¹ç›®çš„contrastive_training.py
æ”¯æŒ15ä¸ªè¶…ç±»çš„ç‹¬ç«‹è®­ç»ƒå’Œæ•°æ®åˆ’åˆ†
"""

import argparse
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from torch.utils.data import DataLoader
import numpy as np
from sklearn.cluster import KMeans
import torch
from torch.optim import SGD, lr_scheduler
from project_utils.cluster_utils import mixed_eval, AverageMeter
from models import vision_transformer as vits

from project_utils.general_utils import init_experiment, get_mean_lr, str2bool, get_dino_head_weights

from data.augmentations import get_transform
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import get_single_superclass_datasets, get_superclass_splits, SUPERCLASS_NAMES
from data.data_utils import MergedDataset
from copy import deepcopy

from tqdm import tqdm

from project_utils.cluster_and_log_utils import log_accs_from_preds
from config import exp_root, dino_pretrain_path

# ä»åŸå§‹è®­ç»ƒè„šæœ¬å¯¼å…¥å¿…è¦çš„ç±»å’Œå‡½æ•°
from methods.contrastive_training.contrastive_training import (
    SupConLoss, ContrastiveLearningViewGenerator, info_nce_logits
)

# å¯¼å…¥è®­ç»ƒå·¥å…·
from utils.training_utils import TrainingSession

# å¯¼å…¥è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
from project_utils.superclass_model_saver import create_superclass_model_saver

# TODO: Debug
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def train_superclass(projection_head, model, train_loader, test_loader, unlabelled_train_loader, args,
                     model_saver=None, progress_parent=None):
    """
    è¶…ç±»è®­ç»ƒå‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„trainå‡½æ•°ä¿®æ”¹

    Returns:
        tuple: (model, projection_head, best_all_acc_test)
    """
    optimizer = SGD(list(projection_head.parameters()) + list(model.parameters()), lr=args.lr, momentum=args.momentum,
                    weight_decay=args.weight_decay)

    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=args.epochs,
            eta_min=args.lr * 1e-3,
        )

    sup_con_crit = SupConLoss()
    # ä½¿ç”¨all_acc_testè¿½è¸ªæœ€ä½³è¡¨ç°ï¼Œä¾›æ¨¡å‹ä¿å­˜å’Œè¿”å›å€¼ä½¿ç”¨
    best_test_acc_lab = 0

    is_grid_search = getattr(args, 'is_grid_search', False)

    # åˆå§‹åŒ–è®­ç»ƒä¼šè¯ç®¡ç†å™¨
    training_session = TrainingSession(args, enable_early_stopping=True, patience=20, quiet=is_grid_search)
    model_info = {
        'name': args.base_model,
        'feat_dim': getattr(args, 'feat_dim', 'Unknown')
    }
    training_session.start_training(model_info)

    epoch_progress = None
    if is_grid_search and progress_parent is not None:
        epoch_progress = tqdm(
            total=args.epochs,
            desc=f"[{args.superclass_name}] è®­ç»ƒ",
            position=1,
            leave=False,
            dynamic_ncols=True
        )

    # åˆå§‹åŒ–è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
    if model_saver is None:
        model_saver = create_superclass_model_saver(args.superclass_name)
    if not is_grid_search:
        print(f"ğŸ—‚ï¸  è¶…ç±»æ¨¡å‹ä¿å­˜å™¨å·²åˆå§‹åŒ–ï¼Œä¿å­˜ç›®å½•: {model_saver.save_dir}")

    for epoch in range(args.epochs):
        if epoch_progress:
            epoch_progress.set_description(f"[{args.superclass_name}] Epoch {epoch+1}/{args.epochs}")

        # å¼€å§‹æ–°è½®æ¬¡
        training_session.start_epoch(epoch)

        loss_record = AverageMeter()
        contrastive_loss_record = AverageMeter()
        sup_con_loss_record = AverageMeter()
        train_acc_record = AverageMeter()

        # å®šä¹‰è½®æ¬¡çº§åˆ«çš„æƒé‡å˜é‡ï¼Œé¿å…æ‰¹æ¬¡å†…é‡å¤å®šä¹‰
        epoch_contrastive_weight = 1 - args.sup_con_weight
        epoch_sup_con_weight = args.sup_con_weight

        projection_head.train()
        model.train()

        if not is_grid_search:
            train_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')
        else:
            train_iter = train_loader

        for batch_idx, batch in enumerate(train_iter):

            images, class_labels, uq_idxs, mask_lab = batch
            mask_lab = mask_lab[:, 0]

            class_labels, mask_lab = class_labels.to(args.device), mask_lab.to(args.device).bool()
            images = torch.cat(images, dim=0).to(args.device)

            # Extract features with base model
            features = model(images)

            # Pass features through projection head
            features = projection_head(features)

            # L2-normalize features
            features = torch.nn.functional.normalize(features, dim=-1)

            # Choose which instances to run the contrastive loss on
            if args.contrast_unlabel_only:
                # Contrastive loss only on unlabelled instances
                f1, f2 = [f[~mask_lab] for f in features.chunk(2)]
                con_feats = torch.cat([f1, f2], dim=0)
            else:
                # Contrastive loss for all examples
                con_feats = features

            contrastive_logits, contrastive_labels = info_nce_logits(features=con_feats, args=args)
            contrastive_loss = torch.nn.CrossEntropyLoss()(contrastive_logits, contrastive_labels)

            # Supervised contrastive loss
            f1, f2 = [f[mask_lab] for f in features.chunk(2)]
            if f1.size(0) > 0:  # åªæœ‰å½“æœ‰æ ‡è®°æ ·æœ¬æ—¶æ‰è®¡ç®—ç›‘ç£å¯¹æ¯”æŸå¤±
                sup_con_feats = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
                sup_con_labels = class_labels[mask_lab]
                sup_con_loss = sup_con_crit(sup_con_feats, labels=sup_con_labels)
            else:
                sup_con_loss = torch.tensor(0.0).to(args.device)

            # æ€»æŸå¤± = æ— ç›‘ç£å¯¹æ¯”æŸå¤± + ç›‘ç£å¯¹æ¯”æŸå¤±
            loss = epoch_contrastive_weight * contrastive_loss + epoch_sup_con_weight * sup_con_loss

            # Train acc
            _, pred = contrastive_logits.max(1)
            acc = (pred == contrastive_labels).float().mean().item()
            train_acc_record.update(acc, pred.size(0))

            # è®°å½•å„ç§æŸå¤±
            loss_record.update(loss.item(), class_labels.size(0))
            contrastive_loss_record.update(contrastive_loss.item(), class_labels.size(0))
            sup_con_loss_record.update(sup_con_loss.item(), class_labels.size(0))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡å®æ—¶æŸå¤±
            if batch_idx % 50 == 0 and batch_idx > 0 and not is_grid_search:
                print(f"   Batch {batch_idx}: loss = {epoch_contrastive_weight:.2f}*{contrastive_loss.item():.4f} + {epoch_sup_con_weight:.2f}*{sup_con_loss.item():.4f} = {loss.item():.4f}")

        with torch.no_grad():
            if not is_grid_search:
                print('ğŸ” Testing on unlabelled examples in the training data...')
            all_acc, old_acc, new_acc = test_kmeans_superclass(model, unlabelled_train_loader,
                                                        epoch=epoch, save_name='Train ACC Unlabelled',
                                                        args=args)

            if not is_grid_search:
                print('ğŸ” Testing on disjoint test set...')
            all_acc_test, old_acc_test, new_acc_test = test_kmeans_superclass(model, test_loader,
                                                                   epoch=epoch, save_name='Test ACC',
                                                                   args=args)

        # ----------------
        # è¾“å‡ºè¯¦ç»†æŸå¤±åˆ†è§£
        # ----------------
        lr_value = get_mean_lr(optimizer)

        if not is_grid_search:
            print(f"\nğŸ“Š Epoch {epoch+1}/{args.epochs} æŸå¤±åˆ†è§£:")
            print(f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*å¯¹æ¯”æŸå¤± + {epoch_sup_con_weight:.2f}*ç›‘ç£å¯¹æ¯”æŸå¤±")
            print(f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*{contrastive_loss_record.avg:.4f} + {epoch_sup_con_weight:.2f}*{sup_con_loss_record.avg:.4f} = {loss_record.avg:.4f}")
            print(f"   å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrastive_loss_record.avg:.4f}")
            print(f"   ç›‘ç£å¯¹æ¯”æŸå¤±: {sup_con_loss_record.avg:.4f}")
            print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_acc_record.avg:.4f}")
            print(f"   å­¦ä¹ ç‡: {lr_value:.6f}")
        elif epoch_progress:
            epoch_progress.update(1)
            epoch_progress.set_postfix({
                'Loss': f"{loss_record.avg:.4f}",
                'TrainAcc': f"{train_acc_record.avg:.4f}",
                'TestAll': f"{all_acc_test:.4f}",
                'Best': f"{best_test_acc_lab:.4f}"
            })

        # ----------------
        # LOG
        # ----------------
        args.writer.add_scalar('Loss/Total', loss_record.avg, epoch)
        args.writer.add_scalar('Loss/Contrastive', contrastive_loss_record.avg, epoch)
        args.writer.add_scalar('Loss/SupCon', sup_con_loss_record.avg, epoch)
        args.writer.add_scalar('Train Acc Labelled Data', train_acc_record.avg, epoch)
        args.writer.add_scalar('LR', get_mean_lr(optimizer), epoch)

        # ä½¿ç”¨è®­ç»ƒä¼šè¯ç®¡ç†å™¨å¤„ç†è½®æ¬¡ç»“æŸ
        should_stop = training_session.end_epoch(
            epoch=epoch,
            train_acc=train_acc_record.avg,
            loss_avg=loss_record.avg,
            all_acc=all_acc,
            old_acc=old_acc,
            new_acc=new_acc,
            all_acc_test=all_acc_test,
            old_acc_test=old_acc_test,
            new_acc_test=new_acc_test
        )

        # Step schedule
        exp_lr_scheduler.step()

        # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨æ–°çš„è¶…ç±»æ¨¡å‹ä¿å­˜æœºåˆ¶
        # ä½¿ç”¨è¶…ç±»æ¨¡å‹ä¿å­˜å™¨ä¿å­˜æœ€ä½³æ¨¡å‹ (ä»…ä¿å­˜æœ€ä½³ï¼Œä¸å†ä¿å­˜æ¯è½®æ¨¡å‹)
        if all_acc_test > best_test_acc_lab:
            # ä½¿ç”¨æ–°çš„ä¿å­˜æœºåˆ¶ï¼Œä¼šè‡ªåŠ¨ç®¡ç†æ–‡ä»¶å‘½åå’Œåˆ é™¤æ—§æ¨¡å‹
            best_model_path, best_proj_path = model_saver.save_best_model(
                model=model,
                projection_head=projection_head,
                acc=all_acc_test,
                metadata={
                    'epoch': epoch + 1,
                    'train_loss': loss_record.avg,
                    'all_acc_test': all_acc_test,
                    'old_acc_test': old_acc_test,
                    'new_acc_test': new_acc_test,
                    'train_acc': train_acc_record.avg
                }
            )
            best_test_acc_lab = all_acc_test

        # æ£€æŸ¥æ—©åœ
        if should_stop:
            if not is_grid_search:
                print(f"\nğŸ›‘ è¶…ç±» '{args.superclass_name}' æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
            training_session.finish_training(epoch, early_stopped=True)
            break

    else:
        # æ­£å¸¸å®Œæˆæ‰€æœ‰è½®æ¬¡
        training_session.finish_training(args.epochs - 1, early_stopped=False)

    if epoch_progress:
        epoch_progress.close()

    # è®­ç»ƒç»“æŸï¼Œæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    if not is_grid_search:
        print(f"\nğŸ“Š è¶…ç±» '{args.superclass_name}' è®­ç»ƒå®Œæˆ")
        best_model_info = model_saver.get_best_model_info()
        print(f"ğŸ† æœ€ä½³æ¨¡å‹ä¿¡æ¯:")
        print(f"   æœ€ä½³ACC: {best_model_info['best_acc']:.4f}")
        if best_model_info['model_path']:
            print(f"   æ¨¡å‹æ–‡ä»¶: {os.path.basename(best_model_info['model_path'])}")
            print(f"   æŠ•å½±å¤´: {os.path.basename(best_model_info['proj_path'])}")
            print(f"   ä¿å­˜ç›®å½•: {best_model_info['save_dir']}")
        else:
            print("   æœªä¿å­˜ä»»ä½•æœ€ä½³æ¨¡å‹ (å¯èƒ½æœªè¾¾åˆ°ä¿å­˜æ¡ä»¶)")

    return model, projection_head, best_test_acc_lab

def test_kmeans_superclass(model, test_loader, epoch, save_name, args):
    """
    è¶…ç±»K-meansæµ‹è¯•å‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„test_kmeansä¿®æ”¹
    """
    model.eval()

    all_feats = []
    targets = np.array([])
    mask = np.array([])

    is_grid_search = getattr(args, 'is_grid_search', False)

    if not is_grid_search:
        print('Collating features...')
        test_iter = tqdm(test_loader, desc='Collating features')
    else:
        test_iter = test_loader

    # First extract all features
    for batch_idx, (images, label, _) in enumerate(test_iter):

        images = images.to(args.device)

        # Pass features through base model only (no projection head for evaluation)
        feats = model(images)
        feats = torch.nn.functional.normalize(feats, dim=-1)

        all_feats.append(feats.cpu().numpy())
        targets = np.append(targets, label.cpu().numpy())
        mask = np.append(mask, np.array([True if x.item() in range(len(args.train_classes))
                                         else False for x in label]))

    # -----------------------
    # K-MEANS
    # -----------------------
    if not is_grid_search:
        print('Fitting K-Means...')
    all_feats = np.concatenate(all_feats)
    kmeans = KMeans(n_clusters=args.num_labeled_classes + args.num_unlabeled_classes, random_state=0, n_init=10).fit(all_feats)
    preds = kmeans.labels_
    if not is_grid_search:
        print('Done!')

    # -----------------------
    # EVALUATE
    # -----------------------
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥ç±»
    mask = np.array(mask, dtype=bool)  # ç¡®ä¿maskæ˜¯numpyæ•°ç»„
    has_unknown_classes = args.num_unlabeled_classes > 0 and (~mask).sum() > 0

    if has_unknown_classes:
        # æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šæ­£å¸¸è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        all_acc, old_acc, new_acc = log_accs_from_preds(y_true=targets, y_pred=preds, mask=mask,
                                                        T=epoch, eval_funcs=args.eval_funcs, save_name=save_name,
                                                        writer=args.writer)
    else:
        # æ²¡æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šåªè®¡ç®—å·²çŸ¥ç±»å‡†ç¡®ç‡
        from project_utils.cluster_utils import cluster_acc
        old_acc = cluster_acc(targets, preds)
        all_acc = old_acc  # å½“æ²¡æœ‰æœªçŸ¥ç±»æ—¶ï¼ŒAll ACC = Old ACC
        new_acc = 0.0      # æ²¡æœ‰æœªçŸ¥ç±»ï¼ŒNew ACCä¸º0

        if not is_grid_search:
            print(f"âš ï¸  æ³¨æ„: è¶…ç±» '{args.superclass_name}' ä¸­æ²¡æœ‰æœªçŸ¥ç±»æ ·æœ¬ï¼Œä»…è®¡ç®—Old ACC")

        # å†™å…¥TensorBoardæ—¥å¿—
        if args.writer is not None:
            args.writer.add_scalars(f'{save_name}_v1',
                                   {'Old': old_acc, 'New': new_acc, 'All': all_acc}, epoch)

    return all_acc, old_acc, new_acc



def train_all_superclasses(args):
    """
    è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»
    """
    print("ğŸŒŸ å¼€å§‹è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
    print("=" * 80)

    superclass_splits = get_superclass_splits()

    for superclass_name in SUPERCLASS_NAMES:
        print(f"\nğŸ¯ å‡†å¤‡è®­ç»ƒè¶…ç±»: {superclass_name}")

        # æ£€æŸ¥è¯¥è¶…ç±»æ˜¯å¦æœ‰è¶³å¤Ÿçš„å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»
        split_info = superclass_splits[superclass_name]
        if len(split_info['known_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰å·²çŸ¥ç±»")
            continue
        if len(split_info['unknown_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰æœªçŸ¥ç±»")
            continue

        # è®¾ç½®å½“å‰è¶…ç±»çš„å‚æ•°
        current_args = argparse.Namespace(**vars(args))
        current_args.superclass_name = superclass_name
        current_args.dataset_name = 'cifar100_superclass'

        # è·å–ç±»åˆ«åˆ’åˆ†
        current_args = get_class_splits(current_args)
        current_args.num_labeled_classes = len(current_args.train_classes)
        current_args.num_unlabeled_classes = len(current_args.unlabeled_classes)

        # åˆ›å»ºå®éªŒç›®å½•
        exp_name = f'superclass_{superclass_name}_{current_args.model_name}'
        current_args.exp_name = exp_name
        init_experiment(current_args, runner_name=['superclass_train'])

        try:
            # è®­ç»ƒå•ä¸ªè¶…ç±»
            train_single_superclass(current_args)
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¶…ç±» '{superclass_name}' æ—¶å‡ºé”™: {e}")
            continue

    print("\nğŸ‰ æ‰€æœ‰è¶…ç±»è®­ç»ƒå®Œæˆ!")



def train_single_superclass(args, model_saver=None, progress_parent=None):
    """
    è®­ç»ƒå•ä¸ªè¶…ç±»

    Args:
        args: è®­ç»ƒé…ç½®
        model_saver: å¯é€‰ï¼Œè¦†ç›–é»˜è®¤çš„è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
    """
    device = args.device
    is_grid_search = getattr(args, 'is_grid_search', False)

    # ----------------------
    # BASE MODEL
    # ----------------------
    if args.base_model == 'vit_dino':

        args.interpolation = 3
        args.crop_pct = 0.875
        pretrain_path = dino_pretrain_path

        model = vits.__dict__['vit_base']()

        state_dict = torch.load(pretrain_path, map_location='cpu')
        model.load_state_dict(state_dict)

        if args.warmup_model_dir is not None:
            if not getattr(args, 'is_grid_search', False):
                print(f'Loading weights from {args.warmup_model_dir}')
            model.load_state_dict(torch.load(args.warmup_model_dir, map_location='cpu'))

        model.to(device)

        # NOTE: Hardcoded image size as we do not finetune the entire ViT model
        args.image_size = 224
        args.feat_dim = 768
        args.num_mlp_layers = 3
        args.mlp_out_dim = 65536

        # ----------------------
        # HOW MUCH OF BASE MODEL TO FINETUNE
        # ----------------------
        for m in model.parameters():
            m.requires_grad = False

        # Only finetune layers from block 'args.grad_from_block' onwards
        for name, m in model.named_parameters():
            if 'block' in name:
                block_num = int(name.split('.')[1])
                if block_num >= args.grad_from_block:
                    m.requires_grad = True

    else:
        raise NotImplementedError

    # --------------------
    # CONTRASTIVE TRANSFORM
    # --------------------
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)
    train_transform = ContrastiveLearningViewGenerator(base_transform=train_transform, n_views=args.n_views)

    # --------------------
    # DATASETS - ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†
    # --------------------
    # ç›´æ¥ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†è·å–å‡½æ•°
    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed,
        verbose=not is_grid_search
    )

    # Train split (labelled and unlabelled classes) for training
    train_dataset = MergedDataset(labelled_dataset=deepcopy(datasets['train_labelled']),
                                  unlabelled_dataset=deepcopy(datasets['train_unlabelled']))

    test_dataset = datasets['test']
    unlabelled_train_examples_test = deepcopy(datasets['train_unlabelled'])
    unlabelled_train_examples_test.transform = test_transform

    # --------------------
    # SAMPLER
    # Sampler which balances labelled and unlabelled examples in each batch
    # --------------------
    label_len = len(train_dataset.labelled_dataset)
    unlabelled_len = len(train_dataset.unlabelled_dataset)
    sample_weights = [1 if i < label_len else label_len / unlabelled_len for i in range(len(train_dataset))]
    sample_weights = torch.DoubleTensor(sample_weights)
    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(train_dataset))

    # --------------------
    # DATALOADERS
    # --------------------
    train_loader = DataLoader(train_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False,
                              sampler=sampler, drop_last=True)
    test_loader_unlabelled = DataLoader(unlabelled_train_examples_test, num_workers=args.num_workers,
                                        batch_size=args.batch_size, shuffle=False)
    test_loader_labelled = DataLoader(test_dataset, num_workers=args.num_workers,
                                      batch_size=args.batch_size, shuffle=False)

    # ----------------------
    # PROJECTION HEAD
    # ----------------------
    projection_head = vits.__dict__['DINOHead'](in_dim=args.feat_dim,
                               out_dim=args.mlp_out_dim, nlayers=args.num_mlp_layers)
    projection_head.to(device)

    # ----------------------
    # TRAIN
    # ----------------------
    return train_superclass(
        projection_head,
        model,
        train_loader,
        test_loader_labelled,
        test_loader_unlabelled,
        args,
        model_saver=model_saver,
        progress_parent=progress_parent
    )


def build_superclass_train_parser(add_help=True):
    parser = argparse.ArgumentParser(
            description='Train GCD on CIFAR-100 superclasses',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            add_help=add_help)

    # =====================================================================
    # æ•°æ®é›†å’Œè®­ç»ƒåŸºç¡€å‚æ•°
    # =====================================================================
    parser.add_argument('--batch_size', default=128, type=int,
                        help='è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤å€¼: 128ã€‚è¾ƒå¤§çš„batch_sizeå¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œä½†éœ€è¦æ›´å¤šGPUå†…å­˜')
    parser.add_argument('--num_workers', default=8, type=int,
                        help='æ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°ã€‚é»˜è®¤å€¼: 8ã€‚æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼Œè¿‡å¤§å¯èƒ½å¯¼è‡´å†…å­˜å¼€é”€å¢åŠ ')
    parser.add_argument('--eval_funcs', nargs='+', default=['v1', 'v2'],
                        help='ä½¿ç”¨çš„è¯„ä¼°å‡½æ•°åˆ—è¡¨ã€‚é»˜è®¤å€¼: [v1, v2]ã€‚v1ä¸ºæ ‡å‡†èšç±»å‡†ç¡®ç‡ï¼Œv2ä¸ºåŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å‡†ç¡®ç‡')

    # =====================================================================
    # æ¨¡å‹æ¶æ„å‚æ•°
    # =====================================================================
    parser.add_argument('--warmup_model_dir', type=str, default=None,
                        help='é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœæä¾›ï¼Œå°†ä»è¯¥è·¯å¾„åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--model_name', type=str, default='vit_dino',
                        help='æ¨¡å‹åç§°æ ‡è¯†ã€‚é»˜è®¤å€¼: vit_dinoã€‚æ ¼å¼ä¸º{æ¨¡å‹æ¶æ„}_{é¢„è®­ç»ƒæ–¹æ³•}ï¼Œç”¨äºå®éªŒå‘½åå’Œæ—¥å¿—è®°å½•')
    parser.add_argument('--dataset_name', type=str, default='cifar100_superclass',
                        help='æ•°æ®é›†åç§°ã€‚é»˜è®¤å€¼: cifar100_superclassã€‚å›ºå®šä½¿ç”¨CIFAR-100è¶…ç±»æ•°æ®é›†ï¼Œä¸å»ºè®®ä¿®æ”¹')
    parser.add_argument('--prop_train_labels', type=float, default=0.8,
                        help='å·²çŸ¥ç±»æ ·æœ¬ä¸­ç”¨äºè®­ç»ƒçš„æ¯”ä¾‹ã€‚é»˜è®¤å€¼: 0.8ã€‚å‰©ä½™0.2ç”¨ä½œéªŒè¯é›†ï¼Œç¬¦åˆGCDæ ‡å‡†åˆ’åˆ†')

    # =====================================================================
    # è¶…ç±»è®­ç»ƒæ¨¡å¼å‚æ•°
    # =====================================================================
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¦è®­ç»ƒçš„å•ä¸ªè¶…ç±»åç§°ã€‚é»˜è®¤å€¼: Noneã€‚ä¾‹å¦‚: aquatic_mammals, fish, flowersç­‰ã€‚'
                             'å¯ç”¨è¶…ç±»è¯·æŸ¥çœ‹CIFAR-100è¶…ç±»å®šä¹‰')
    parser.add_argument('--train_all_superclasses', action='store_true',
                        help='æ‰¹é‡è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»çš„æ ‡å¿—ã€‚é»˜è®¤å€¼: Falseã€‚å¯ç”¨åå°†ä¾æ¬¡è®­ç»ƒæ‰€æœ‰è¶…ç±»ï¼Œå¿½ç•¥--superclass_nameå‚æ•°')

    # =====================================================================
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡å‚æ•°
    # =====================================================================
    parser.add_argument('--grad_from_block', type=int, default=11,
                        help='ViTæ¨¡å‹ä»ç¬¬å‡ ä¸ªblockå¼€å§‹è®¡ç®—æ¢¯åº¦ã€‚é»˜è®¤å€¼: 11ã€‚ViT-Baseå…±12ä¸ªblocks (0-11)ï¼Œ'
                             'ä»…å¾®è°ƒæœ€åå‡ å±‚å¯ä»¥å‡å°‘è®¡ç®—é‡å¹¶é˜²æ­¢è¿‡æ‹Ÿåˆ')
    parser.add_argument('--lr', type=float, default=0.1,
                        help='åˆå§‹å­¦ä¹ ç‡ã€‚é»˜è®¤å€¼: 0.1ã€‚ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé€æ¸é™ä½è‡³lr*1e-3')
    parser.add_argument('--save_best_thresh', type=float, default=None,
                        help='ä¿å­˜æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡é˜ˆå€¼ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœè®¾ç½®ï¼Œä»…å½“å‡†ç¡®ç‡è¶…è¿‡è¯¥é˜ˆå€¼æ—¶æ‰ä¿å­˜æ¨¡å‹')
    parser.add_argument('--gamma', type=float, default=0.1,
                        help='å­¦ä¹ ç‡è¡°å‡å› å­ã€‚é»˜è®¤å€¼: 0.1ã€‚ç”¨äºStepLRè°ƒåº¦å™¨ï¼ˆå½“å‰ä½¿ç”¨CosineAnnealingï¼Œæ­¤å‚æ•°æš‚æœªä½¿ç”¨ï¼‰')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='SGDä¼˜åŒ–å™¨çš„åŠ¨é‡ç³»æ•°ã€‚é»˜è®¤å€¼: 0.9ã€‚æœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘æŒ¯è¡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ç³»æ•°(L2æ­£åˆ™åŒ–)ã€‚é»˜è®¤å€¼: 1e-4ã€‚é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ')
    parser.add_argument('--epochs', default=20, type=int,
                        help='è®­ç»ƒæ€»è½®æ•°ã€‚é»˜è®¤å€¼: 20ã€‚é…åˆæ—©åœæœºåˆ¶(patience=20)ï¼Œå®é™…è®­ç»ƒå¯èƒ½æå‰ç»“æŸ')

    # =====================================================================
    # è·¯å¾„å’Œå®éªŒé…ç½®å‚æ•°
    # =====================================================================
    parser.add_argument('--exp_root', type=str, default=exp_root,
                        help=f'å®éªŒè¾“å‡ºæ ¹ç›®å½•ã€‚é»˜è®¤å€¼: {exp_root}ã€‚æ‰€æœ‰æ¨¡å‹ã€æ—¥å¿—ã€TensorBoardæ–‡ä»¶å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸‹')
    parser.add_argument('--transform', type=str, default='imagenet',
                        help='æ•°æ®å¢å¼ºæ–¹æ¡ˆåç§°ã€‚é»˜è®¤å€¼: imagenetã€‚ä½¿ç”¨ImageNetæ ‡å‡†çš„æ•°æ®å¢å¼ºç­–ç•¥')
    parser.add_argument('--seed', default=1, type=int,
                        help='éšæœºç§å­ã€‚é»˜è®¤å€¼: 1ã€‚ç”¨äºä¿è¯å®éªŒå¯å¤ç°æ€§')
    parser.add_argument('--gpu', type=int, default=0,
                        help='ä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ã€‚é»˜è®¤å€¼: 0ã€‚å¦‚æœæœ‰å¤šå—GPUï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨å“ªä¸€å—(0, 1, 2...)')

    # =====================================================================
    # å¯¹æ¯”å­¦ä¹ æ¡†æ¶å‚æ•°
    # =====================================================================
    parser.add_argument('--base_model', type=str, default='vit_dino',
                        help='åŸºç¡€æ¨¡å‹æ¶æ„ã€‚é»˜è®¤å€¼: vit_dinoã€‚å½“å‰ä»…æ”¯æŒvit_dino (ViT-Base with DINOé¢„è®­ç»ƒ)')
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦ç³»æ•°ã€‚é»˜è®¤å€¼: 1.0ã€‚æ§åˆ¶softmaxçš„å¹³æ»‘åº¦ï¼Œè¾ƒå°çš„å€¼ä½¿åˆ†å¸ƒæ›´å°–é”')
    parser.add_argument('--sup_con_weight', type=float, default=0.5,
                        help='ç›‘ç£å¯¹æ¯”æŸå¤±çš„æƒé‡ã€‚é»˜è®¤å€¼: 0.5ã€‚æ€»æŸå¤± = (1-w)*å¯¹æ¯”æŸå¤± + w*ç›‘ç£å¯¹æ¯”æŸå¤±ã€‚'
                             'èŒƒå›´[0,1]ï¼Œ0è¡¨ç¤ºçº¯æ— ç›‘ç£ï¼Œ1è¡¨ç¤ºçº¯ç›‘ç£')
    parser.add_argument('--n_views', default=2, type=int,
                        help='å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡ã€‚é»˜è®¤å€¼: 2ã€‚æ¯ä¸ªæ ·æœ¬ç”Ÿæˆnä¸ªå¢å¼ºè§†å›¾ç”¨äºå¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--contrast_unlabel_only', type=str2bool, default=False,
                        help='æ˜¯å¦ä»…å¯¹æœªæ ‡è®°æ ·æœ¬è®¡ç®—å¯¹æ¯”æŸå¤±ã€‚é»˜è®¤å€¼: Falseã€‚Trueæ—¶å¯¹æ¯”æŸå¤±ä»…ç”¨äºæœªçŸ¥ç±»æ ·æœ¬')

    return parser


def main():
    parser = build_superclass_train_parser()
    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    print("ğŸš€ CIFAR-100è¶…ç±»GCDè®­ç»ƒè„šæœ¬")
    print("=" * 80)
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # å°†deviceæ·»åŠ åˆ°argsä¸­ï¼Œæ–¹ä¾¿å…¶ä»–å‡½æ•°ä½¿ç”¨
    args.device = device

    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    if args.train_all_superclasses:
        print("ğŸŒŸ æ¨¡å¼: è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        train_all_superclasses(args)
    elif args.superclass_name:
        print(f"ğŸ¯ æ¨¡å¼: è®­ç»ƒå•ä¸ªè¶…ç±» '{args.superclass_name}'")

        # éªŒè¯è¶…ç±»åç§°
        if args.superclass_name not in SUPERCLASS_NAMES:
            print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{args.superclass_name}'")
            print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
            sys.exit(1)

        # è·å–ç±»åˆ«åˆ’åˆ†
        args = get_class_splits(args)
        args.num_labeled_classes = len(args.train_classes)
        args.num_unlabeled_classes = len(args.unlabeled_classes)

        # åˆå§‹åŒ–å®éªŒ
        exp_name = f'superclass_{args.superclass_name}_{args.model_name}'
        args.exp_name = exp_name
        init_experiment(args, runner_name=['superclass_train'])
        print(f'Using evaluation function {args.eval_funcs[0]} to print results')

        # è®­ç»ƒ
        train_single_superclass(args)
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€:")
        print("   --superclass_name <åç§°>  : è®­ç»ƒå•ä¸ªè¶…ç±»")
        print("   --train_all_superclasses  : è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
        sys.exit(1)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
