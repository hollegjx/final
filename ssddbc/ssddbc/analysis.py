#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SS-DDBCèšç±»ç»“æœåˆ†ææ¨¡å—
æä¾›è¯¦ç»†çš„èšç±»è´¨é‡åˆ†æå’Œå¯è§†åŒ–
"""

import numpy as np


def analyze_ssddbc_clustering_result(clusters, cluster_labels, labeled_mask, targets, known_mask):
    """
    åˆ†æSS-DDBCèšç±»æ„å»ºæ­¥éª¤çš„ç»“æœ

    Args:
        clusters: èšç±»åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»æ©ç 
    """
    print(f"\nğŸ“Š SS-DDBCèšç±»æ„å»ºç»“æœåˆ†æ:")
    print("="*80)

    total_samples = len(cluster_labels)
    assigned_samples = np.sum(cluster_labels != -1)
    unassigned_samples = total_samples - assigned_samples

    print(f"æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   å·²åˆ†é…æ ·æœ¬: {assigned_samples}")
    print(f"   æœªåˆ†é…æ ·æœ¬: {unassigned_samples}")
    print(f"   èšç±»æ•°é‡: {len(clusters)}")

    print(f"\nå„èšç±»è¯¦ç»†åˆ†æ:")

    for cluster_id, cluster in enumerate(clusters):
        cluster_indices = list(cluster)
        cluster_size = len(cluster_indices)

        # ç»Ÿè®¡æœ‰æ ‡ç­¾/æ— æ ‡ç­¾æ ·æœ¬
        cluster_labeled_mask = labeled_mask[cluster_indices]
        labeled_count = np.sum(cluster_labeled_mask)
        unlabeled_count = cluster_size - labeled_count

        # ç»Ÿè®¡å·²çŸ¥ç±»/æœªçŸ¥ç±»æ ·æœ¬
        cluster_known_mask = known_mask[cluster_indices]
        known_count = np.sum(cluster_known_mask)
        unknown_count = cluster_size - known_count

        # åˆ†ææ ‡ç­¾åˆ†å¸ƒï¼ˆæœ‰æ ‡ç­¾æ ·æœ¬ï¼‰
        label_distribution = {}
        dominant_label = None
        label_purity = 0.0

        if labeled_count > 0:
            labeled_targets = targets[cluster_indices][cluster_labeled_mask]
            unique_labels, counts = np.unique(labeled_targets, return_counts=True)
            label_distribution = dict(zip(unique_labels, counts))
            dominant_label = unique_labels[np.argmax(counts)]
            label_purity = np.max(counts) / labeled_count

        # åˆ†æçœŸå®ç±»åˆ«åˆ†å¸ƒï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰
        all_targets = targets[cluster_indices]
        all_unique_labels, all_counts = np.unique(all_targets, return_counts=True)
        true_distribution = dict(zip(all_unique_labels, all_counts))

        # è¾“å‡ºèšç±»ä¿¡æ¯
        print(f"\nèšç±» #{cluster_id} (å¤§å°: {cluster_size}):")
        print(f"   æ ·æœ¬ç»„æˆ:")
        print(f"     æœ‰æ ‡ç­¾æ ·æœ¬: {labeled_count} ä¸ª")
        print(f"     æ— æ ‡ç­¾æ ·æœ¬: {unlabeled_count} ä¸ª")
        print(f"     å·²çŸ¥ç±»æ ·æœ¬: {known_count} ä¸ª")
        print(f"     æœªçŸ¥ç±»æ ·æœ¬: {unknown_count} ä¸ª")

        if labeled_count > 0:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ:")
            print(f"     ä¸»å¯¼æ ‡ç­¾: {dominant_label} (çº¯åº¦: {label_purity:.3f})")
            print(f"     è¯¦ç»†åˆ†å¸ƒ: {label_distribution}")

            # æ£€æŸ¥æ ‡ç­¾å†²çª
            if len(label_distribution) > 1:
                print(f"     âš ï¸  æ ‡ç­¾å†²çª: åŒ…å«{len(label_distribution)}ç§ä¸åŒæ ‡ç­¾")
        else:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: æ— æœ‰æ ‡ç­¾æ ·æœ¬ (æ½œåœ¨æœªçŸ¥ç±»)")

        print(f"   çœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬): {true_distribution}")

    # æœªåˆ†é…æ ·æœ¬åˆ†æ
    if unassigned_samples > 0:
        print(f"\næœªåˆ†é…æ ·æœ¬åˆ†æ ({unassigned_samples}ä¸ª):")
        unassigned_indices = np.where(cluster_labels == -1)[0]
        unassigned_labeled = np.sum(labeled_mask[unassigned_indices])
        unassigned_unlabeled = unassigned_samples - unassigned_labeled
        unassigned_known = np.sum(known_mask[unassigned_indices])
        unassigned_unknown = unassigned_samples - unassigned_known

        print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {unassigned_labeled} ä¸ª")
        print(f"   æ— æ ‡ç­¾æ ·æœ¬: {unassigned_unlabeled} ä¸ª")
        print(f"   å·²çŸ¥ç±»æ ·æœ¬: {unassigned_known} ä¸ª")
        print(f"   æœªçŸ¥ç±»æ ·æœ¬: {unassigned_unknown} ä¸ª")

        if unassigned_labeled > 0:
            unassigned_targets = targets[unassigned_indices][labeled_mask[unassigned_indices]]
            unassigned_unique, unassigned_counts = np.unique(unassigned_targets, return_counts=True)
            unassigned_distribution = dict(zip(unassigned_unique, unassigned_counts))
            print(f"   æœªåˆ†é…æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: {unassigned_distribution}")

    print("="*80)


def analyze_cluster_composition(predictions, targets, known_mask, labeled_mask, unknown_clusters):
    """
    åˆ†ææ¯ä¸ªèšç±»çš„å†…éƒ¨ç»„æˆæƒ…å†µ

    Args:
        predictions: èšç±»é¢„æµ‹ç»“æœ
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç 
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•åˆ—è¡¨
    """
    print(f"\nğŸ” èšç±»å†…éƒ¨ç»„æˆåˆ†æ:")
    print("="*80)

    unique_clusters = np.unique(predictions)

    for cluster_id in sorted(unique_clusters):
        cluster_mask = predictions == cluster_id
        cluster_indices = np.where(cluster_mask)[0]

        if len(cluster_indices) == 0:
            continue

        # åŸºæœ¬ä¿¡æ¯
        cluster_size = len(cluster_indices)
        is_unknown_cluster = cluster_id in unknown_clusters

        # æ ‡ç­¾ä¿¡æ¯
        cluster_targets = targets[cluster_indices]
        cluster_known_mask = known_mask[cluster_indices]
        cluster_labeled_mask = labeled_mask[cluster_indices]

        # ç»Ÿè®¡æœ‰æ ‡ç­¾æ ·æœ¬
        labeled_samples = cluster_indices[cluster_labeled_mask]
        unlabeled_samples = cluster_indices[~cluster_labeled_mask]

        # ç»Ÿè®¡å·²çŸ¥ç±»/æœªçŸ¥ç±»æ ·æœ¬
        known_samples = cluster_indices[cluster_known_mask]
        unknown_samples = cluster_indices[~cluster_known_mask]

        # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
        if len(labeled_samples) > 0:
            labeled_targets = cluster_targets[cluster_labeled_mask]
            unique_labels, label_counts = np.unique(labeled_targets, return_counts=True)
            label_distribution = dict(zip(unique_labels, label_counts))
            dominant_label = unique_labels[np.argmax(label_counts)]
            label_purity = np.max(label_counts) / len(labeled_samples)
        else:
            label_distribution = {}
            dominant_label = None
            label_purity = 0.0

        # è¾“å‡ºèšç±»ä¿¡æ¯
        cluster_type = "ğŸ” æ½œåœ¨æœªçŸ¥ç±»" if is_unknown_cluster else "ğŸ“Š å¸¸è§„èšç±»"
        print(f"\n{cluster_type} #{cluster_id} - å¤§å°: {cluster_size}")
        print(f"   æ ·æœ¬ç»„æˆ:")
        print(f"     æœ‰æ ‡ç­¾æ ·æœ¬: {len(labeled_samples)} ä¸ª")
        print(f"     æ— æ ‡ç­¾æ ·æœ¬: {len(unlabeled_samples)} ä¸ª")
        print(f"     å·²çŸ¥ç±»æ ·æœ¬: {len(known_samples)} ä¸ª")
        print(f"     æœªçŸ¥ç±»æ ·æœ¬: {len(unknown_samples)} ä¸ª")

        if len(labeled_samples) > 0:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ:")
            print(f"     ä¸»å¯¼æ ‡ç­¾: {dominant_label} (çº¯åº¦: {label_purity:.3f})")
            print(f"     è¯¦ç»†åˆ†å¸ƒ: {label_distribution}")

            # æ£€æŸ¥æ ‡ç­¾å†²çª
            if len(unique_labels) > 1:
                print(f"     âš ï¸  æ ‡ç­¾å†²çª: åŒ…å«{len(unique_labels)}ç§ä¸åŒæ ‡ç­¾")
        else:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: æ— æœ‰æ ‡ç­¾æ ·æœ¬")

        # åˆ†æçœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬æ— æ ‡ç­¾çš„)
        all_targets = cluster_targets  # æ‰€æœ‰èšç±»å†…æ ·æœ¬çš„çœŸå®æ ‡ç­¾
        all_unique_labels, all_label_counts = np.unique(all_targets, return_counts=True)
        all_label_distribution = dict(zip(all_unique_labels, all_label_counts))

        print(f"   çœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬):")
        print(f"     è¯¦ç»†åˆ†å¸ƒ: {all_label_distribution}")

        # åˆ†æå·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»çš„çœŸå®åˆ†å¸ƒ
        if len(known_samples) > 0:
            known_targets = cluster_targets[cluster_known_mask]
            known_unique, known_counts = np.unique(known_targets, return_counts=True)
            known_class_distribution = dict(zip(known_unique, known_counts))
            print(f"   å·²çŸ¥ç±»æ ·æœ¬åˆ†å¸ƒ: {known_class_distribution}")

        if len(unknown_samples) > 0:
            unknown_targets = cluster_targets[~cluster_known_mask]
            unknown_unique, unknown_counts = np.unique(unknown_targets, return_counts=True)
            unknown_class_distribution = dict(zip(unknown_unique, unknown_counts))
            print(f"   æœªçŸ¥ç±»æ ·æœ¬åˆ†å¸ƒ: {unknown_class_distribution}")

        # åˆ†æèšç±»è´¨é‡
        if len(labeled_samples) > 1:
            # è®¡ç®—å†…éƒ¨ä¸€è‡´æ€§
            same_label_pairs = 0
            total_pairs = 0
            for i in range(len(labeled_samples)):
                for j in range(i+1, len(labeled_samples)):
                    if labeled_targets[i] == labeled_targets[j]:
                        same_label_pairs += 1
                    total_pairs += 1

            if total_pairs > 0:
                consistency = same_label_pairs / total_pairs
                print(f"   è´¨é‡è¯„ä¼°:")
                print(f"     å†…éƒ¨ä¸€è‡´æ€§: {consistency:.3f}")

                if consistency >= 0.9:
                    print(f"     è´¨é‡è¯„çº§: âœ… ä¼˜ç§€")
                elif consistency >= 0.7:
                    print(f"     è´¨é‡è¯„çº§: âœ… è‰¯å¥½")
                elif consistency >= 0.5:
                    print(f"     è´¨é‡è¯„çº§: âš ï¸  ä¸€èˆ¬")
                else:
                    print(f"     è´¨é‡è¯„çº§: âŒ è¾ƒå·®")

    # å…¨å±€ç»Ÿè®¡
    print(f"\nğŸ“Š å…¨å±€èšç±»ç»Ÿè®¡:")
    print(f"   æ€»èšç±»æ•°: {len(unique_clusters)}")
    print(f"   æ½œåœ¨æœªçŸ¥ç±»èšç±»æ•°: {len(unknown_clusters)}")
    print(f"   å¸¸è§„èšç±»æ•°: {len(unique_clusters) - len(unknown_clusters)}")

    # åˆ†æèšç±»å¤§å°åˆ†å¸ƒ
    cluster_sizes = []
    for cluster_id in unique_clusters:
        cluster_size = np.sum(predictions == cluster_id)
        cluster_sizes.append(cluster_size)

    print(f"   èšç±»å¤§å°ç»Ÿè®¡:")
    print(f"     å¹³å‡å¤§å°: {np.mean(cluster_sizes):.1f}")
    print(f"     æœ€å¤§èšç±»: {np.max(cluster_sizes)} ä¸ªæ ·æœ¬")
    print(f"     æœ€å°èšç±»: {np.min(cluster_sizes)} ä¸ªæ ·æœ¬")
    print(f"     å¤§å°æ ‡å‡†å·®: {np.std(cluster_sizes):.1f}")


def print_prototype_distance_matrix(X, predictions):
    """
    æ‰“å°ç°‡é—´åŸå‹è·ç¦»çŸ©é˜µ

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        predictions: èšç±»é¢„æµ‹ç»“æœ
    """
    unique_clusters = np.unique(predictions)
    n_clusters = len(unique_clusters)

    if n_clusters == 0:
        print("   æ²¡æœ‰èšç±»")
        return

    # è®¡ç®—æ¯ä¸ªç°‡çš„åŸå‹ï¼ˆä¸­å¿ƒï¼‰
    prototypes = []
    for cluster_id in unique_clusters:
        cluster_mask = predictions == cluster_id
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) > 0:
            prototype = np.mean(X[cluster_indices], axis=0)
            prototypes.append(prototype)
        else:
            prototypes.append(None)

    # è®¡ç®—ç°‡é—´è·ç¦»çŸ©é˜µ
    distance_matrix = np.zeros((n_clusters, n_clusters))
    for i in range(n_clusters):
        for j in range(n_clusters):
            if prototypes[i] is not None and prototypes[j] is not None:
                if i == j:
                    distance_matrix[i, j] = 0.0
                else:
                    distance_matrix[i, j] = np.linalg.norm(prototypes[i] - prototypes[j])

    # æ‰“å°çŸ©é˜µ
    print(f"   èšç±»æ•°é‡: {n_clusters}")

    # å¦‚æœèšç±»æ•°é‡å¤ªå¤šï¼Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if n_clusters >= 25:
        print(f"   èšç±»æ•°é‡è¾ƒå¤š({n_clusters}ä¸ª)ï¼Œä»…æ˜¾ç¤ºè·ç¦»ç»Ÿè®¡:")
        # æå–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
        upper_tri_indices = np.triu_indices(n_clusters, k=1)
        distances = distance_matrix[upper_tri_indices]

        print(f"   ç°‡é—´è·ç¦»ç»Ÿè®¡:")
        print(f"     æœ€å°è·ç¦»: {np.min(distances):.4f}")
        print(f"     æœ€å¤§è·ç¦»: {np.max(distances):.4f}")
        print(f"     å¹³å‡è·ç¦»: {np.mean(distances):.4f}")
        print(f"     ä¸­ä½è·ç¦»: {np.median(distances):.4f}")

        # æ˜¾ç¤ºè·ç¦»æœ€è¿‘çš„5å¯¹ç°‡
        print(f"   è·ç¦»æœ€è¿‘çš„5å¯¹ç°‡:")
        flat_indices = np.argsort(distances)[:5]
        for idx in flat_indices:
            i, j = upper_tri_indices[0][idx], upper_tri_indices[1][idx]
            cluster_i = unique_clusters[i]
            cluster_j = unique_clusters[j]
            dist = distances[idx]
            size_i = np.sum(predictions == cluster_i)
            size_j = np.sum(predictions == cluster_j)
            print(f"     ç°‡{cluster_i}({size_i}æ ·æœ¬) â†” ç°‡{cluster_j}({size_j}æ ·æœ¬): {dist:.4f}")
    else:
        # æ‰“å°å®Œæ•´çŸ©é˜µ
        print(f"   å®Œæ•´è·ç¦»çŸ©é˜µ:")

        # æ‰“å°è¡¨å¤´
        header = "       "
        for cluster_id in unique_clusters:
            header += f"  ç°‡{cluster_id:<4}"
        print(header)
        print("   " + "-" * (7 + 7 * n_clusters))

        # æ‰“å°æ¯ä¸€è¡Œ
        for i, cluster_i in enumerate(unique_clusters):
            row = f"   ç°‡{cluster_i:<4}â”‚"
            for j in range(n_clusters):
                if i == j:
                    row += "   -   "
                else:
                    row += f" {distance_matrix[i, j]:5.2f} "
            print(row)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        upper_tri_indices = np.triu_indices(n_clusters, k=1)
        distances = distance_matrix[upper_tri_indices]
        print(f"\n   è·ç¦»ç»Ÿè®¡: æœ€å°={np.min(distances):.4f}, æœ€å¤§={np.max(distances):.4f}, å¹³å‡={np.mean(distances):.4f}")


def print_prototype_distance_matrix_ground_truth(X, targets):
    """
    æ‰“å°åŸºäºçœŸå®æ ‡ç­¾çš„åŸå‹è·ç¦»çŸ©é˜µï¼ˆä¸Šå¸è§†è§’ï¼‰

    ç”¨äºå¯¹æ¯”ï¼šå±•ç¤ºå¦‚æœæ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾éƒ½å·²çŸ¥ï¼Œç†æƒ³çš„ç°‡åŸå‹åº”è¯¥æ˜¯ä»€ä¹ˆæ ·çš„

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        targets: çœŸå®æ ‡ç­¾
    """
    unique_classes = np.unique(targets)
    n_classes = len(unique_classes)

    if n_classes == 0:
        print("   æ²¡æœ‰ç±»åˆ«")
        return

    # è®¡ç®—æ¯ä¸ªçœŸå®ç±»åˆ«çš„åŸå‹ï¼ˆä¸­å¿ƒï¼‰
    prototypes = []
    class_sizes = []
    for class_id in unique_classes:
        class_mask = targets == class_id
        class_indices = np.where(class_mask)[0]
        if len(class_indices) > 0:
            prototype = np.mean(X[class_indices], axis=0)
            prototypes.append(prototype)
            class_sizes.append(len(class_indices))
        else:
            prototypes.append(None)
            class_sizes.append(0)

    # è®¡ç®—ç±»é—´è·ç¦»çŸ©é˜µ
    distance_matrix = np.zeros((n_classes, n_classes))
    for i in range(n_classes):
        for j in range(n_classes):
            if prototypes[i] is not None and prototypes[j] is not None:
                if i == j:
                    distance_matrix[i, j] = 0.0
                else:
                    distance_matrix[i, j] = np.linalg.norm(prototypes[i] - prototypes[j])

    # æ‰“å°çŸ©é˜µ
    print(f"   çœŸå®ç±»åˆ«æ•°é‡: {n_classes}")

    # å¦‚æœç±»åˆ«æ•°é‡å¤ªå¤šï¼Œåªæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if n_classes >= 25:
        print(f"   ç±»åˆ«æ•°é‡è¾ƒå¤š({n_classes}ä¸ª)ï¼Œä»…æ˜¾ç¤ºè·ç¦»ç»Ÿè®¡:")
        # æå–ä¸Šä¸‰è§’ï¼ˆä¸åŒ…æ‹¬å¯¹è§’çº¿ï¼‰
        upper_tri_indices = np.triu_indices(n_classes, k=1)
        distances = distance_matrix[upper_tri_indices]

        print(f"   ç±»é—´è·ç¦»ç»Ÿè®¡:")
        print(f"     æœ€å°è·ç¦»: {np.min(distances):.4f}")
        print(f"     æœ€å¤§è·ç¦»: {np.max(distances):.4f}")
        print(f"     å¹³å‡è·ç¦»: {np.mean(distances):.4f}")
        print(f"     ä¸­ä½è·ç¦»: {np.median(distances):.4f}")

        # æ˜¾ç¤ºè·ç¦»æœ€è¿‘çš„5å¯¹ç±»
        print(f"   è·ç¦»æœ€è¿‘çš„5å¯¹ç±»åˆ«:")
        flat_indices = np.argsort(distances)[:5]
        for idx in flat_indices:
            i, j = upper_tri_indices[0][idx], upper_tri_indices[1][idx]
            class_i = unique_classes[i]
            class_j = unique_classes[j]
            dist = distances[idx]
            size_i = class_sizes[i]
            size_j = class_sizes[j]
            print(f"     ç±»{class_i}({size_i}æ ·æœ¬) â†” ç±»{class_j}({size_j}æ ·æœ¬): {dist:.4f}")
    else:
        # æ‰“å°å®Œæ•´çŸ©é˜µ
        print(f"   å®Œæ•´è·ç¦»çŸ©é˜µ:")

        # æ‰“å°è¡¨å¤´
        header = "       "
        for class_id in unique_classes:
            header += f"  ç±»{class_id:<4}"
        print(header)
        print("   " + "-" * (7 + 7 * n_classes))

        # æ‰“å°æ¯ä¸€è¡Œ
        for i, class_i in enumerate(unique_classes):
            row = f"   ç±»{class_i:<4}â”‚"
            for j in range(n_classes):
                if i == j:
                    row += "   -   "
                else:
                    row += f" {distance_matrix[i, j]:5.2f} "
            print(row)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        upper_tri_indices = np.triu_indices(n_classes, k=1)
        distances = distance_matrix[upper_tri_indices]
        print(f"\n   è·ç¦»ç»Ÿè®¡: æœ€å°={np.min(distances):.4f}, æœ€å¤§={np.max(distances):.4f}, å¹³å‡={np.mean(distances):.4f}")


def analyze_intra_inter_class_distances(X, targets):
    """
    åˆ†æç±»å†…å’Œç±»é—´æ ·æœ¬è·ç¦»ï¼ˆä¸Šå¸è§†è§’ï¼‰

    å¯¹æ¯ä¸ªç±»åˆ«è®¡ç®—ï¼š
    - ç±»å†…è·ç¦»ï¼šåŒç±»åˆ«æ ·æœ¬ä¹‹é—´çš„è·ç¦»ç»Ÿè®¡
    - ç±»é—´è·ç¦»ï¼šè¯¥ç±»åˆ«ä¸å…¶ä»–æ¯ä¸ªç±»åˆ«ä¹‹é—´çš„æ ·æœ¬è·ç¦»ç»Ÿè®¡

    Args:
        X: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
        targets: çœŸå®æ ‡ç­¾ (n_samples,)
    """
    print(f"\nğŸ“Š ä¸Šå¸è§†è§’ï¼šç±»å†…å’Œç±»é—´æ ·æœ¬è·ç¦»è¯¦ç»†åˆ†æ")
    print("="*80)

    unique_classes = np.unique(targets)
    n_classes = len(unique_classes)

    if n_classes == 0:
        print("   æ²¡æœ‰ç±»åˆ«")
        return

    print(f"   ç±»åˆ«æ•°é‡: {n_classes}")
    print(f"   ç‰¹å¾å·²L2å½’ä¸€åŒ–ï¼Œè·ç¦»èŒƒå›´ [0, 2]")
    print("="*80)

    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œåˆ†æ
    for i, class_i in enumerate(unique_classes):
        # è·å–ç±»åˆ«içš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        class_i_mask = targets == class_i
        class_i_indices = np.where(class_i_mask)[0]
        n_samples_i = len(class_i_indices)

        print(f"\nğŸ“Œ ç±»åˆ« {class_i} ({n_samples_i}ä¸ªæ ·æœ¬):")
        print("-"*80)

        # 1. è®¡ç®—ç±»å†…è·ç¦»
        if n_samples_i > 1:
            intra_distances = []
            for idx1 in range(len(class_i_indices)):
                for idx2 in range(idx1+1, len(class_i_indices)):
                    dist = np.linalg.norm(X[class_i_indices[idx1]] - X[class_i_indices[idx2]])
                    intra_distances.append(dist)

            intra_distances = np.array(intra_distances)
            print(f"   ğŸ”¹ ç±»å†…è·ç¦»ç»Ÿè®¡ (å…±{len(intra_distances)}å¯¹):")
            print(f"      å¹³å‡è·ç¦»: {np.mean(intra_distances):.4f}")
            print(f"      æœ€å°è·ç¦»: {np.min(intra_distances):.4f}")
            print(f"      æœ€å¤§è·ç¦»: {np.max(intra_distances):.4f}")
            print(f"      ä¸­ä½è·ç¦»: {np.median(intra_distances):.4f}")
            print(f"      æ ‡å‡†å·®:   {np.std(intra_distances):.4f}")
        else:
            print(f"   ğŸ”¹ ç±»å†…è·ç¦»ç»Ÿè®¡: åªæœ‰1ä¸ªæ ·æœ¬ï¼Œæ— æ³•è®¡ç®—")

        # 2. è®¡ç®—ç±»é—´è·ç¦»ï¼ˆä¸å…¶ä»–æ¯ä¸ªç±»åˆ«ï¼‰
        print(f"\n   ğŸ”¸ ç±»é—´è·ç¦»ç»Ÿè®¡ (ç±»{class_i} vs å…¶ä»–ç±»åˆ«):")

        for j, class_j in enumerate(unique_classes):
            if i == j:  # è·³è¿‡è‡ªå·±
                continue

            # è·å–ç±»åˆ«jçš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
            class_j_mask = targets == class_j
            class_j_indices = np.where(class_j_mask)[0]
            n_samples_j = len(class_j_indices)

            # è®¡ç®—ç±»iå’Œç±»jä¹‹é—´æ‰€æœ‰æ ·æœ¬å¯¹çš„è·ç¦»
            inter_distances = []
            for idx_i in class_i_indices:
                for idx_j in class_j_indices:
                    dist = np.linalg.norm(X[idx_i] - X[idx_j])
                    inter_distances.append(dist)

            inter_distances = np.array(inter_distances)

            print(f"      vs ç±»{class_j:>2} ({n_samples_j:>3}æ ·æœ¬): "
                  f"å¹³å‡={np.mean(inter_distances):.4f}, "
                  f"æœ€å°={np.min(inter_distances):.4f}, "
                  f"æœ€å¤§={np.max(inter_distances):.4f}, "
                  f"ä¸­ä½={np.median(inter_distances):.4f}")

    print("\n" + "="*80)
    print("ğŸ’¡ è·ç¦»è§£è¯» (L2å½’ä¸€åŒ–åçš„æ¬§æ°è·ç¦»):")
    print("   - 0.0~0.5: éå¸¸ç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ > 0.875)")
    print("   - 0.5~1.0: æ¯”è¾ƒç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ 0.5~0.875)")
    print("   - 1.0~1.4: ä¸­ç­‰ç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ 0~0.5)")
    print("   - 1.4~2.0: ä¸ç›¸ä¼¼   (ä½™å¼¦ç›¸ä¼¼åº¦ < 0)")
    print("="*80)


def analyze_high_density_intra_inter_class_distances(X, targets, high_density_mask):
    """
    åˆ†æé«˜å¯†åº¦ç‚¹ä¹‹é—´çš„ç±»å†…å’Œç±»é—´è·ç¦»ï¼ˆä¸Šå¸è§†è§’ï¼‰

    ä»…ç»Ÿè®¡è¢«æ ‡è®°ä¸ºé«˜å¯†åº¦ç‚¹çš„æ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼ŒåŒºåˆ†åŒç±»å’Œå¼‚ç±»ï¼š
    - ç±»å†…è·ç¦»ï¼šåŒç±»åˆ«çš„é«˜å¯†åº¦ç‚¹ä¹‹é—´çš„è·ç¦»ç»Ÿè®¡
    - ç±»é—´è·ç¦»ï¼šä¸åŒç±»åˆ«çš„é«˜å¯†åº¦ç‚¹ä¹‹é—´çš„è·ç¦»ç»Ÿè®¡

    Args:
        X: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
        targets: çœŸå®æ ‡ç­¾ (n_samples,)
        high_density_mask: é«˜å¯†åº¦ç‚¹æ©ç  (n_samples,)
    """
    print(f"\nğŸ“Š ä¸Šå¸è§†è§’ï¼šé«˜å¯†åº¦ç‚¹ç±»å†…å’Œç±»é—´è·ç¦»è¯¦ç»†åˆ†æ")
    print("="*80)

    # è¿‡æ»¤å‡ºé«˜å¯†åº¦ç‚¹
    high_density_indices = np.where(high_density_mask)[0]
    n_high_density = len(high_density_indices)

    if n_high_density == 0:
        print("   âš ï¸  æ²¡æœ‰é«˜å¯†åº¦ç‚¹ï¼Œæ— æ³•åˆ†æ")
        return

    X_high_density = X[high_density_indices]
    targets_high_density = targets[high_density_indices]

    unique_classes = np.unique(targets_high_density)
    n_classes = len(unique_classes)

    print(f"   æ€»æ ·æœ¬æ•°: {len(X)}")
    print(f"   é«˜å¯†åº¦ç‚¹æ•°é‡: {n_high_density} ({n_high_density/len(X)*100:.1f}%)")
    print(f"   æ¶‰åŠç±»åˆ«æ•°: {n_classes}")
    print(f"   ç‰¹å¾å·²L2å½’ä¸€åŒ–ï¼Œè·ç¦»èŒƒå›´ [0, 2]")
    print("="*80)

    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œåˆ†æ
    for i, class_i in enumerate(unique_classes):
        # è·å–ç±»åˆ«içš„é«˜å¯†åº¦ç‚¹ç´¢å¼•
        class_i_mask = targets_high_density == class_i
        class_i_indices = np.where(class_i_mask)[0]
        n_samples_i = len(class_i_indices)

        print(f"\nğŸ“Œ ç±»åˆ« {class_i} (é«˜å¯†åº¦ç‚¹: {n_samples_i}ä¸ª):")
        print("-"*80)

        # 1. è®¡ç®—ç±»å†…è·ç¦»ï¼ˆé«˜å¯†åº¦ç‚¹ä¹‹é—´ï¼‰
        if n_samples_i > 1:
            intra_distances = []
            for idx1 in range(len(class_i_indices)):
                for idx2 in range(idx1+1, len(class_i_indices)):
                    dist = np.linalg.norm(
                        X_high_density[class_i_indices[idx1]] - X_high_density[class_i_indices[idx2]]
                    )
                    intra_distances.append(dist)

            intra_distances = np.array(intra_distances)
            print(f"   ğŸ”¹ ç±»å†…é«˜å¯†åº¦ç‚¹è·ç¦»ç»Ÿè®¡ (å…±{len(intra_distances)}å¯¹):")
            print(f"      å¹³å‡è·ç¦»: {np.mean(intra_distances):.4f}")
            print(f"      æœ€å°è·ç¦»: {np.min(intra_distances):.4f}")
            print(f"      æœ€å¤§è·ç¦»: {np.max(intra_distances):.4f}")
            print(f"      ä¸­ä½è·ç¦»: {np.median(intra_distances):.4f}")
            print(f"      æ ‡å‡†å·®:   {np.std(intra_distances):.4f}")
        else:
            print(f"   ğŸ”¹ ç±»å†…é«˜å¯†åº¦ç‚¹è·ç¦»ç»Ÿè®¡: åªæœ‰1ä¸ªé«˜å¯†åº¦ç‚¹ï¼Œæ— æ³•è®¡ç®—")

        # 2. è®¡ç®—ç±»é—´è·ç¦»ï¼ˆé«˜å¯†åº¦ç‚¹ä¹‹é—´ï¼‰
        print(f"\n   ğŸ”¸ ç±»é—´é«˜å¯†åº¦ç‚¹è·ç¦»ç»Ÿè®¡ (ç±»{class_i} vs å…¶ä»–ç±»åˆ«):")

        for j, class_j in enumerate(unique_classes):
            if i == j:  # è·³è¿‡è‡ªå·±
                continue

            # è·å–ç±»åˆ«jçš„é«˜å¯†åº¦ç‚¹ç´¢å¼•
            class_j_mask = targets_high_density == class_j
            class_j_indices = np.where(class_j_mask)[0]
            n_samples_j = len(class_j_indices)

            # è®¡ç®—ç±»iå’Œç±»jçš„é«˜å¯†åº¦ç‚¹ä¹‹é—´çš„æ‰€æœ‰è·ç¦»
            inter_distances = []
            for idx_i in class_i_indices:
                for idx_j in class_j_indices:
                    dist = np.linalg.norm(X_high_density[idx_i] - X_high_density[idx_j])
                    inter_distances.append(dist)

            inter_distances = np.array(inter_distances)

            print(f"      vs ç±»{class_j:>2} ({n_samples_j:>3}ä¸ªé«˜å¯†åº¦ç‚¹): "
                  f"å¹³å‡={np.mean(inter_distances):.4f}, "
                  f"æœ€å°={np.min(inter_distances):.4f}, "
                  f"æœ€å¤§={np.max(inter_distances):.4f}, "
                  f"ä¸­ä½={np.median(inter_distances):.4f}")

    print("\n" + "="*80)
    print("ğŸ’¡ è·ç¦»è§£è¯» (L2å½’ä¸€åŒ–åçš„æ¬§æ°è·ç¦»):")
    print("   - 0.0~0.5: éå¸¸ç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ > 0.875)")
    print("   - 0.5~1.0: æ¯”è¾ƒç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ 0.5~0.875)")
    print("   - 1.0~1.4: ä¸­ç­‰ç›¸ä¼¼ (ä½™å¼¦ç›¸ä¼¼åº¦ 0~0.5)")
    print("   - 1.4~2.0: ä¸ç›¸ä¼¼   (ä½™å¼¦ç›¸ä¼¼åº¦ < 0)")
    print("="*80)




def evaluate_high_density_clustering(cluster_labels, targets, known_mask, eval_version='v1', X=None, silent=False):
    """
    è¯„ä¼°é«˜å¯†åº¦ç‚¹çš„èšç±»å‡†ç¡®ç‡ï¼ˆä»…è¯„ä¼°å·²åˆ†é…çš„é«˜å¯†åº¦ç‚¹ï¼‰

    Args:
        cluster_labels: é«˜å¯†åº¦ç‚¹çš„èšç±»æ ‡ç­¾ (-1è¡¨ç¤ºæœªåˆ†é…)
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»æ©ç 
        eval_version: è¯„ä¼°ç‰ˆæœ¬ ('v1' æˆ– 'v2')
        X: ç‰¹å¾çŸ©é˜µ (å·²åºŸå¼ƒï¼Œä¸ºä¿æŒå…¼å®¹æ€§ä¿ç•™)
        silent: é™é»˜æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰

    Returns:
        all_acc: æ‰€æœ‰æ ·æœ¬å‡†ç¡®ç‡
        old_acc: å·²çŸ¥ç±»å‡†ç¡®ç‡
        new_acc: æœªçŸ¥ç±»å‡†ç¡®ç‡
        n_clusters: èšç±»æ•°é‡
        dbcv_score: DBCVåˆ†æ•° (å·²ç§»é™¤ï¼Œå§‹ç»ˆè¿”å›None)
    """

    # åªè¯„ä¼°å·²åˆ†é…çš„é«˜å¯†åº¦ç‚¹
    assigned_mask = cluster_labels != -1
    n_total = len(cluster_labels)
    n_assigned = np.sum(assigned_mask)

    if not silent:
        print(f"\nğŸ“Š é«˜å¯†åº¦ç‚¹èšç±»è¯„ä¼°:")
        print(f"   æ€»æ ·æœ¬æ•°: {n_total}")
        print(f"   å·²åˆ†é…é«˜å¯†åº¦ç‚¹: {n_assigned} ({n_assigned/n_total*100:.1f}%)")
        print(f"   æœªåˆ†é…ä½å¯†åº¦ç‚¹: {n_total - n_assigned} ({(n_total - n_assigned)/n_total*100:.1f}%)")

    if n_assigned == 0:
        if not silent:
            print("   âš ï¸  æ²¡æœ‰å·²åˆ†é…çš„é«˜å¯†åº¦ç‚¹ï¼Œæ— æ³•è¯„ä¼°")
        return 0.0, 0.0, 0.0, 0, None

    # æå–å·²åˆ†é…çš„é«˜å¯†åº¦ç‚¹
    assigned_predictions = cluster_labels[assigned_mask]
    assigned_targets = targets[assigned_mask]
    assigned_known_mask = known_mask[assigned_mask]

    # ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬çš„ACCè®¡ç®—æ–¹æ³•
    if eval_version == 'v1':
        from project_utils.cluster_and_log_utils import split_cluster_acc_v1
        all_acc, old_acc, new_acc = split_cluster_acc_v1(assigned_targets, assigned_predictions, assigned_known_mask)
    else:  # v2
        from project_utils.cluster_and_log_utils import split_cluster_acc_v2
        all_acc, old_acc, new_acc = split_cluster_acc_v2(assigned_targets, assigned_predictions, assigned_known_mask)

    n_clusters = len(np.unique(assigned_predictions))

    if not silent:
        print(f"\nğŸ“ˆ é«˜å¯†åº¦ç‚¹èšç±»ç»“æœ:")
        print(f"   èšç±»æ•°é‡: {n_clusters}")
        print(f"   All ACC: {all_acc:.4f}")
        print(f"   Old ACC: {old_acc:.4f}")
        print(f"   New ACC: {new_acc:.4f}")

    # DBCVå·²ç§»é™¤
    return all_acc, old_acc, new_acc, n_clusters, None
