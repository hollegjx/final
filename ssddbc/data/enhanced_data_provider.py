#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¢å¼ºå‹æ•°æ®æä¾›å™¨
æä¾›åŠŸèƒ½æ›´å…¨é¢ã€æ•ˆç‡æ›´é«˜çš„æ•°æ®è¯»å–æ–¹æ¡ˆ
åŒ…å«æ‰€æœ‰éœ€è¦çš„ä¿¡æ¯ï¼Œé¿å…åç»­é‡å¤è®¡ç®—
"""

import numpy as np
from config import feature_cache_dir
from .data_provider import DataProvider
from .dataset_config import get_superclass_info
from .model_loader import ModelLoader
from .dataset_loader import DatasetLoader


class EnhancedDataset:
    """
    å¢å¼ºå‹æ•°æ®é›†ç±»
    åŒ…å«æ‰€æœ‰èšç±»æ‰€éœ€çš„ä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
    """

    def __init__(self, feature_dict, dataset_name='unknown', use_l2=True, source='unknown'):
        """
        åˆå§‹åŒ–å¢å¼ºå‹æ•°æ®é›†

        Args:
            feature_dict: ç‰¹å¾æ•°æ®å­—å…¸
            dataset_name: æ•°æ®é›†åç§°
            use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ–
            source: æ•°æ®æ¥æº ('cache' or 'extraction')
        """
        # åŸºç¡€æ•°æ®
        self.all_features = feature_dict['all_features']
        self.all_targets = feature_dict['all_targets']
        self.all_known_mask = feature_dict['all_known_mask']
        self.all_labeled_mask = feature_dict['all_labeled_mask']

        self.train_features = feature_dict.get('train_features')
        self.train_targets = feature_dict.get('train_targets')
        self.train_known_mask = feature_dict.get('train_known_mask')
        self.train_labeled_mask = feature_dict.get('train_labeled_mask')

        self.test_features = feature_dict.get('test_features')
        self.test_targets = feature_dict.get('test_targets')
        self.test_known_mask = feature_dict.get('test_known_mask')
        self.test_labeled_mask = feature_dict.get('test_labeled_mask')

        # å…ƒä¿¡æ¯
        self.dataset_name = dataset_name
        self.use_l2 = use_l2
        self.source = source  # 'cache' or 'extraction'

        # é¢„è®¡ç®—çš„ä¿¡æ¯ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self.n_samples = len(self.all_features)
        self.feat_dim = self.all_features.shape[1] if len(self.all_features.shape) > 1 else self.all_features.shape[0]

        # è®­ç»ƒé›†/æµ‹è¯•é›†ä¿¡æ¯
        if self.train_features is not None:
            self.train_size = len(self.train_features)
            self.test_size = len(self.test_features) if self.test_features is not None else 0
            self.test_start_idx = self.train_size  # æµ‹è¯•é›†åœ¨åˆå¹¶æ•°æ®ä¸­çš„èµ·å§‹ç´¢å¼•
            self.has_train_test_split = True
        else:
            self.train_size = 0
            self.test_size = self.n_samples
            self.test_start_idx = 0
            self.has_train_test_split = False

        # ç»Ÿè®¡ä¿¡æ¯
        self.n_known = np.sum(self.all_known_mask)
        self.n_unknown = np.sum(~self.all_known_mask)
        self.n_labeled = np.sum(self.all_labeled_mask)
        self.n_unlabeled = np.sum(~self.all_labeled_mask)

        # ç±»åˆ«ä¿¡æ¯
        self.n_classes = len(np.unique(self.all_targets))
        self.n_known_classes = len(np.unique(self.all_targets[self.all_known_mask]))
        self.n_unknown_classes = len(np.unique(self.all_targets[~self.all_known_mask]))

    def get_test_subset(self, predictions=None):
        """
        è·å–æµ‹è¯•é›†å­é›†ï¼ˆç”¨äºACCè®¡ç®—ï¼‰

        Args:
            predictions: å…¨å±€é¢„æµ‹ç»“æœï¼ˆå¯é€‰ï¼‰

        Returns:
            dict: æµ‹è¯•é›†æ•°æ®å­—å…¸
        """
        if not self.has_train_test_split:
            # æ²¡æœ‰è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†ï¼Œè¿”å›å…¨éƒ¨æ•°æ®
            result = {
                'features': self.all_features,
                'targets': self.all_targets,
                'known_mask': self.all_known_mask,
                'labeled_mask': self.all_labeled_mask,
                'n_samples': self.n_samples
            }
            if predictions is not None:
                result['predictions'] = predictions
        else:
            # æœ‰è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†ï¼Œåªè¿”å›æµ‹è¯•é›†
            result = {
                'features': self.test_features,
                'targets': self.test_targets,
                'known_mask': self.test_known_mask,
                'labeled_mask': self.test_labeled_mask,
                'n_samples': self.test_size
            }
            if predictions is not None:
                result['predictions'] = predictions[self.test_start_idx:]

        return result

    def get_clustering_input(self):
        """
        è·å–èšç±»ç®—æ³•æ‰€éœ€çš„è¾“å…¥æ•°æ®

        Returns:
            tuple: (X, targets, known_mask, labeled_mask, train_size)
        """
        train_size = self.train_size if self.has_train_test_split else None
        return (
            self.all_features,
            self.all_targets,
            self.all_known_mask,
            self.all_labeled_mask,
            train_size
        )

    def print_summary(self, silent=False):
        """
        æ‰“å°æ•°æ®é›†æ‘˜è¦ä¿¡æ¯

        Args:
            silent: æ˜¯å¦é™é»˜æ¨¡å¼
        """
        if silent:
            return

        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"   åç§°: {self.dataset_name}")
        print(f"   æ•°æ®æ¥æº: {self.source}")
        print(f"   L2å½’ä¸€åŒ–: {'æ˜¯' if self.use_l2 else 'å¦'}")
        print(f"   ç‰¹å¾ç»´åº¦: {self.feat_dim}")
        print(f"\nğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {self.n_samples}")
        if self.has_train_test_split:
            print(f"   è®­ç»ƒé›†: {self.train_size} æ ·æœ¬")
            print(f"   æµ‹è¯•é›†: {self.test_size} æ ·æœ¬")
        print(f"   å·²çŸ¥ç±»æ ·æœ¬: {self.n_known} ({self.n_known/self.n_samples*100:.1f}%)")
        print(f"   æœªçŸ¥ç±»æ ·æœ¬: {self.n_unknown} ({self.n_unknown/self.n_samples*100:.1f}%)")
        print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {self.n_labeled} ({self.n_labeled/self.n_samples*100:.1f}%)")
        print(f"   æ— æ ‡ç­¾æ ·æœ¬: {self.n_unlabeled} ({self.n_unlabeled/self.n_samples*100:.1f}%)")
        print(f"\nğŸ“Š ç±»åˆ«ç»Ÿè®¡:")
        print(f"   æ€»ç±»åˆ«æ•°: {self.n_classes}")
        print(f"   å·²çŸ¥ç±»åˆ«æ•°: {self.n_known_classes}")
        print(f"   æœªçŸ¥ç±»åˆ«æ•°: {self.n_unknown_classes}")


class EnhancedDataProvider:
    """
    å¢å¼ºå‹æ•°æ®æä¾›å™¨
    åœ¨åŸæœ‰DataProvideråŸºç¡€ä¸Šï¼Œè¿”å›EnhancedDatasetå¯¹è±¡
    """

    def __init__(self, cache_base_dir=None):
        """
        åˆå§‹åŒ–å¢å¼ºå‹æ•°æ®æä¾›å™¨

        Args:
            cache_base_dir: ç¼“å­˜åŸºç¡€ç›®å½•
        """
        cache_dir = cache_base_dir or feature_cache_dir
        self.data_provider = DataProvider(cache_base_dir=cache_dir)
        self.cache_base_dir = cache_dir

    def load_dataset(self, dataset_name, model_path=None, use_l2=True,
                    use_train_and_test=True, silent=False):
        """
        åŠ è½½æ•°æ®é›†ï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼‰

        Args:
            dataset_name: æ•°æ®é›†åç§°
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆç¼“å­˜ä¸å­˜åœ¨æ—¶éœ€è¦ï¼‰
            use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ–
            use_train_and_test: æ˜¯å¦ä½¿ç”¨è®­ç»ƒ+æµ‹è¯•é›†
            silent: æ˜¯å¦é™é»˜æ¨¡å¼

        Returns:
            EnhancedDataset: å¢å¼ºå‹æ•°æ®é›†å¯¹è±¡
        """
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        feature_dict = self.data_provider.feature_loader.load(
            dataset_name, use_l2=use_l2, silent=silent
        )

        if feature_dict is not None:
            # ç¼“å­˜å‘½ä¸­
            if not silent:
                print(f"âœ… ä½¿ç”¨ç¼“å­˜ç‰¹å¾")
            source = 'cache'
        else:
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦å®æ—¶æå–
            if model_path is None:
                raise ValueError("ç¼“å­˜ä¸å­˜åœ¨ä¸”æœªæä¾›æ¨¡å‹è·¯å¾„ï¼Œæ— æ³•æå–ç‰¹å¾")

            if not silent:
                print(f"âš ï¸  ç¼“å­˜ä¸å­˜åœ¨ï¼Œå¼€å§‹å®æ—¶ç‰¹å¾æå–...")

            # åŠ è½½æ¨¡å‹
            import torch
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

            model_loader = ModelLoader(
                model_path=model_path,
                base_model='vit_dino',
                feat_dim=768,
                device=device
            )
            model = model_loader.load(silent=silent)

            # åŠ è½½æ•°æ®é›†
            dataset_loader = DatasetLoader(
                superclass_name=dataset_name,
                image_size=224,
                batch_size=64,
                prop_train_labels=0.8,
                seed=0
            )
            data_loaders = dataset_loader.load(silent=silent)

            # æå–ç‰¹å¾
            feature_dict, source = self.data_provider.get_features(
                dataset_name=dataset_name,
                model=model,
                data_loaders=(data_loaders['train_loader'], data_loaders['test_loader']),
                use_l2=use_l2,
                use_train_and_test=use_train_and_test,
                silent=silent
            )

        # åˆ›å»ºå¢å¼ºå‹æ•°æ®é›†å¯¹è±¡
        enhanced_dataset = EnhancedDataset(
            feature_dict=feature_dict,
            dataset_name=dataset_name,
            use_l2=use_l2,
            source=source
        )

        return enhanced_dataset
