#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¶…ç±»æµ‹è¯•æ¨¡å—
åœ¨CIFAR-100è¶…ç±»æ•°æ®é›†ä¸Šæµ‹è¯•SS-DDBCèšç±»ç®—æ³•
ä½¿ç”¨ssddbc/dataå¢å¼ºå‹æ•°æ®æä¾›å™¨ï¼Œé«˜æ•ˆè¯»å–æ•°æ®
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import torch
import numpy as np

# ä½¿ç”¨å¢å¼ºå‹æ•°æ®æä¾›å™¨ï¼ˆé«˜æ•ˆã€æ— é‡å¤é€»è¾‘ï¼‰
from ssddbc.data import (
    EnhancedDataProvider,
    get_superclass_info
)

# èšç±»ç›¸å…³æ¨¡å—
from ..ssddbc.adaptive_clustering import adaptive_density_clustering
from ..ssddbc.analysis import analyze_cluster_composition
from ..baseline.kmeans import test_kmeans_baseline
from ..evaluation.loss_function import compute_total_loss


def test_adaptive_clustering_on_superclass(superclass_name, model_path,
                                         use_train_and_test=True, k=10,
                                         density_percentile=75, random_state=0,
                                         eval_version='v1',
                                         run_kmeans_baseline=False,
                                         use_l2=True,
                                         feature_cache_dir=None,
                                         eval_dense=False, fast_mode=False,
                                         dense_method=0, assign_model=2, voting_k=5,
                                         co_mode=2, co_manual=None, detail_dense=False,
                                         label_guide=False,
                                         use_cluster_quality=False,
                                         cluster_distance_method=1,
                                         l1_type='cross_entropy',
                                         separation_weight=1.0,
                                         penalty_weight=1.0,
                                         l2_components=None,
                                         l2_component_weights=None,
                                         l2_component_params=None):
    """
    åœ¨æŒ‡å®šè¶…ç±»ä¸Šæµ‹è¯•è‡ªé€‚åº”èšç±»ç®—æ³•

    Args:
        superclass_name: è¶…ç±»åç§°
        model_path: æ¨¡å‹è·¯å¾„
        use_train_and_test: æ˜¯å¦ä½¿ç”¨è®­ç»ƒ+æµ‹è¯•é›†
        k: Kè¿‘é‚»æ•°é‡
        density_percentile: é«˜å¯†åº¦ç‚¹ç™¾åˆ†ä½é˜ˆå€¼
        random_state: éšæœºç§å­
        eval_version: è¯„ä¼°ç‰ˆæœ¬ ('v1' or 'v2')
        run_kmeans_baseline: æ˜¯å¦è¿è¡ŒK-meansåŸºçº¿
        use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ–ç‰¹å¾
        feature_cache_dir: ç‰¹å¾ç¼“å­˜æ ¹ç›®å½•ï¼ˆNone æ—¶ä½¿ç”¨é…ç½®é»˜è®¤è·¯å¾„ï¼‰
        eval_dense: æ˜¯å¦ä»…è¯„ä¼°é«˜å¯†åº¦ç‚¹
        fast_mode: å¿«é€Ÿæ¨¡å¼ï¼Œè·³è¿‡ä¸å¿…è¦çš„è®¡ç®—ï¼ˆæœªçŸ¥ç°‡è¯†åˆ«ã€ç°‡ç±»åˆ«æ ‡ç­¾ï¼‰ï¼Œé»˜è®¤False
        dense_method: å¯†åº¦è®¡ç®—æ–¹æ³•
        assign_model: ç¨€ç–ç‚¹åˆ†é…ç­–ç•¥
        voting_k: KNNæŠ•ç¥¨é‚»å±…æ•°é‡
        co_mode: coè®¡ç®—æ¨¡å¼
        co_manual: æ‰‹åŠ¨æŒ‡å®šçš„coå€¼
        detail_dense: æ˜¯å¦è®°å½•éª¨å¹²ç½‘ç»œèšç±»è¯¦ç»†æ—¥å¿—
        label_guide: æ˜¯å¦å¯ç”¨æ ‡ç­¾å¼•å¯¼æ¨¡å¼
        l1_type: L1æŸå¤±ç±»å‹ ('accuracy'æˆ–'cross_entropy'ï¼Œé»˜è®¤'cross_entropy')
        separation_weight: L2æŸå¤±ä¸­ç°‡é—´åˆ†ç¦»åº¦çš„æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰
        penalty_weight: L2æŸå¤±ä¸­å±€éƒ¨å¯†åº¦æƒ©ç½šçš„æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰

    Returns:
        results: ç»“æœå­—å…¸
    """
    if not fast_mode:
        print(f"ğŸ§ª æµ‹è¯•è‡ªé€‚åº”èšç±» - è¶…ç±»: {superclass_name}")
        print("="*80)

    # ========== æ­¥éª¤1: è·å–è¶…ç±»é…ç½®ä¿¡æ¯ ==========
    superclass_info = get_superclass_info(superclass_name)
    known_classes_mapped = superclass_info['known_classes_mapped']

    if not fast_mode:
        print(f"ğŸ“Š è¶…ç±»ä¿¡æ¯:")
        print(f"   åŸå§‹å·²çŸ¥ç±»: {superclass_info['known_classes']} -> æ˜ å°„å: {sorted(list(known_classes_mapped))}")
        print(f"   åŸå§‹æœªçŸ¥ç±»: {superclass_info['unknown_classes']} -> æ˜ å°„å: {sorted(list(superclass_info['unknown_classes_mapped']))}")

    # ========== æ­¥éª¤2: åŠ è½½æ•°æ®é›†ï¼ˆä½¿ç”¨å¢å¼ºå‹æ•°æ®æä¾›å™¨ï¼Œä¸€æ¬¡è°ƒç”¨è·å–æ‰€æœ‰æ•°æ®ï¼‰ ==========
    provider = EnhancedDataProvider(cache_base_dir=feature_cache_dir)
    dataset = provider.load_dataset(
        dataset_name=superclass_name,
        model_path=model_path,
        use_l2=use_l2,
        use_train_and_test=use_train_and_test,
        silent=fast_mode
    )

    # æ‰“å°æ•°æ®é›†æ‘˜è¦ï¼ˆæ›¿ä»£åŸæ¥çš„8è¡Œæ‰‹åŠ¨æ‰“å°ï¼‰
    dataset.print_summary(silent=fast_mode)

    # ========== æ­¥éª¤3: è·å–èšç±»è¾“å…¥ï¼ˆä¸€è¡Œä»£ç ï¼‰ ==========
    X, targets, known_mask, labeled_mask, train_size = dataset.get_clustering_input()

    # ========== æ­¥éª¤4: è¿è¡ŒSS-DDBCè‡ªé€‚åº”èšç±» ==========
    clustering_result = adaptive_density_clustering(
        X, targets, known_mask, labeled_mask,
        k=k, density_percentile=density_percentile, random_state=random_state,
        train_size=train_size,
        eval_dense=eval_dense, eval_version=eval_version,
        fast_mode=fast_mode,  # ä¼ é€’ fast_mode å‚æ•°
        dense_method=dense_method,
        assign_model=assign_model,
        voting_k=voting_k,
        co_mode=co_mode,
        co_manual=co_manual,
        detail_dense=False if fast_mode else detail_dense,
        label_guide=label_guide,
    )

    # è½»é‡ç²¾ç®€ï¼šä¸å†æ›´æ–°æ—¥å¿—ä¸­çš„æ•°æ®é›†åç§°ï¼ˆdetail_dense åŠŸèƒ½ä¿ç•™åœ¨æ ¸å¿ƒç®—æ³•ä¸­ï¼‰

    # ========== æ­¥éª¤5: å¤„ç†èšç±»ç»“æœå’Œè®¡ç®—ACC ==========
    if eval_dense:
        (predictions, n_clusters, unknown_clusters, all_acc, old_acc, new_acc, _,
         neighbors, core_clusters, densities) = clustering_result
        clusters = None  # updated_clusters
        cluster_category_labels = {}  # eval_denseæ¨¡å¼ä¸è¿”å›cluster_category_labels
        if not fast_mode:
            print(f"\nğŸ“Š eval_denseæ¨¡å¼: ä½¿ç”¨é«˜å¯†åº¦ç‚¹è¯„ä¼°ç»“æœ")
    else:
        (predictions, n_clusters, unknown_clusters, clusters,
         cluster_category_labels, neighbors, core_clusters, densities) = clustering_result

        # è·å–æµ‹è¯•é›†æ•°æ®ï¼ˆä½¿ç”¨datasetä¾¿æ·æ–¹æ³•ï¼Œä¸€è¡Œä»£ç æ›¿ä»£10è¡Œï¼‰
        test_data = dataset.get_test_subset(predictions)

        if not fast_mode:
            print(f"ğŸ“Š ACCè®¡ç®—èŒƒå›´: {'æµ‹è¯•é›†' if dataset.has_train_test_split else 'å…¨éƒ¨æ•°æ®'} ({test_data['n_samples']}ä¸ªæ ·æœ¬)")

        # ä½¿ç”¨ç°‡IDç›´æ¥è®¡ç®—ACCï¼ˆä¸è½¬æ¢ä¸ºç±»åˆ«æ ‡ç­¾ï¼‰
        # ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬çš„ACCè®¡ç®—æ–¹æ³•
        if eval_version == 'v1':
            from project_utils.cluster_and_log_utils import split_cluster_acc_v1
            all_acc, old_acc, new_acc = split_cluster_acc_v1(
                test_data['targets'], test_data['predictions'], test_data['known_mask']
            )
        else:  # v2
            from project_utils.cluster_and_log_utils import split_cluster_acc_v2
            all_acc, old_acc, new_acc = split_cluster_acc_v2(
                test_data['targets'], test_data['predictions'], test_data['known_mask']
            )

        # è®¡ç®—é”™è¯¯æ ·æœ¬æ•°é‡ï¼ˆè°ƒè¯•ç‰ˆæœ¬ï¼‰
        from project_utils.cluster_utils import linear_assignment
        import numpy as np

        # ä½¿ç”¨æµ‹è¯•é›†æ•°æ®è®¡ç®—çº¿æ€§åˆ†é…ï¼ˆä¸split_cluster_acc_v2ä¸€è‡´ï¼‰
        # ä½¿ç”¨ç°‡IDç›´æ¥è®¡ç®—
        test_targets_int = test_data['targets'].astype(int)
        test_predictions_int = test_data['predictions'].astype(int)

        D = max(test_predictions_int.max(), test_targets_int.max()) + 1
        w = np.zeros((D, D), dtype=int)
        for i in range(len(test_predictions_int)):
            w[test_predictions_int[i], test_targets_int[i]] += 1

        ind = linear_assignment(w.max() - w)
        ind = np.vstack(ind).T

        # indæ ¼å¼: [[é¢„æµ‹ç°‡, çœŸå®æ ‡ç­¾], ...]
        # åˆ›å»ºåå‘æ˜ å°„: é¢„æµ‹ç°‡ -> çœŸå®æ ‡ç­¾
        cluster_to_label = {i: j for i, j in ind}

        # è®¡ç®—ACCï¼ˆä¸split_cluster_acc_v2ç¬¬57è¡Œä¸€è‡´ï¼‰
        acc_from_w = sum([w[i, j] for i, j in ind]) * 1.0 / len(test_predictions_int)

        # ç»Ÿè®¡é”™è¯¯æ ·æœ¬
        error_count = 0
        for idx in range(test_data['n_samples']):
            pred_cluster = test_predictions_int[idx]
            true_label = test_targets_int[idx]
            # é¢„æµ‹ç°‡åº”è¯¥å¯¹åº”çš„çœŸå®æ ‡ç­¾
            assigned_label = cluster_to_label.get(pred_cluster, -1)
            if assigned_label != true_label:
                error_count += 1

        correct_count = test_data['n_samples'] - error_count

        if not fast_mode:
            print(f"\n[DEBUG] æ··æ·†çŸ©é˜µå‰3è¡Œ:")
            for i in range(min(3, len(w))):
                print(f"  ç°‡{i}: {w[i][:10]} (æ€»å’Œ={w[i].sum()})")
            print(f"[DEBUG] çº¿æ€§åˆ†é… (é¢„æµ‹ç°‡->çœŸå®æ ‡ç­¾): {dict(list(cluster_to_label.items())[:5])}")
            print(f"[DEBUG] ä»æ··æ·†çŸ©é˜µè®¡ç®—çš„ACC: {acc_from_w:.4f}")
            print(f"[DEBUG] å®˜æ–¹split_cluster_acc_v2çš„ACC: {all_acc:.4f}")
            print(f"[DEBUG] å·®å¼‚: {abs(acc_from_w - all_acc):.6f}")
            print(f"\n   æ­£ç¡®æ ·æœ¬: {correct_count}/{test_data['n_samples']}")
            print(f"   é”™è¯¯æ ·æœ¬: {error_count}/{test_data['n_samples']} ({error_count/test_data['n_samples']:.2%})")

    # ========== æ­¥éª¤6: æ˜¾ç¤ºèšç±»ç»„æˆåˆ†æ ==========
    if not eval_dense and not fast_mode:
        analyze_cluster_composition(predictions, targets, known_mask, labeled_mask, unknown_clusters)
    elif not fast_mode:
        print(f"\nğŸ’¡ eval_denseæ¨¡å¼: è·³è¿‡èšç±»ç»„æˆåˆ†æï¼ˆä»…è¯„ä¼°é«˜å¯†åº¦éª¨å¹²ç½‘ç»œï¼‰")

    # ========== æ­¥éª¤7: è®¡ç®—ç»¼åˆæŸå¤±L ==========
    # å°†cluster_distance_methodæ•°å­—è½¬æ¢ä¸ºæ–¹æ³•å
    method_map = {1: 'nearest_k', 2: 'all_pairs', 3: 'prototype'}
    cluster_distance_method_str = method_map.get(cluster_distance_method, 'nearest_k')

    # è§£æ L2 ç»„ä»¶é…ç½®
    resolved_l2_components = l2_components
    if resolved_l2_components is None:
        resolved_l2_components = ['separation', 'penalty'] if use_cluster_quality else []
    elif isinstance(resolved_l2_components, str):
        resolved_l2_components = [comp.strip() for comp in resolved_l2_components.split(',') if comp.strip()]
    else:
        resolved_l2_components = list(resolved_l2_components)

    resolved_l2_weights = {}
    if l2_component_weights:
        resolved_l2_weights = {str(k): float(v) for k, v in l2_component_weights.items()}

    if 'separation' in resolved_l2_components and 'separation' not in resolved_l2_weights:
        resolved_l2_weights['separation'] = separation_weight
    if 'penalty' in resolved_l2_components and 'penalty' not in resolved_l2_weights:
        resolved_l2_weights['penalty'] = penalty_weight

    resolved_l2_params = {str(k): dict(v) for k, v in (l2_component_params or {}).items()}

    # è‹¥æ˜¾å¼é…ç½®ç»„ä»¶ï¼Œåˆ™ä¸å†ä½¿ç”¨ use_cluster_quality æ—§å¼€å…³
    use_cluster_quality_flag = use_cluster_quality and l2_components is None

    clusters_for_l2 = None
    if use_cluster_quality_flag:
        clusters_for_l2 = core_clusters
    elif resolved_l2_components and 'separation' in resolved_l2_components:
        if core_clusters is None:
            raise ValueError("å¯ç”¨ separation ç»„ä»¶æ—¶éœ€è¦æ ¸å¿ƒç°‡ä¿¡æ¯ï¼Œå½“å‰é…ç½®æœªæä¾› core_clusters")
        clusters_for_l2 = core_clusters

    loss_dict = compute_total_loss(
        X=X,
        predictions=predictions,
        targets=targets,
        labeled_mask=labeled_mask,
        cluster_category_labels=cluster_category_labels,
        l1_weight=1.0,
        l2_weight=1.0,
        l1_type=l1_type,  # ä¼ é€’L1æŸå¤±ç±»å‹
        use_cluster_quality=use_cluster_quality_flag,
        clusters=clusters_for_l2,
        k=k,
        cluster_distance_method=cluster_distance_method_str,
        neighbors=neighbors,  # ä¼ é€’é¢„è®¡ç®—çš„neighborsï¼Œé¿å…é‡å¤è®¡ç®—KNN
        separation_weight=separation_weight,  # L2ä¸­ç°‡é—´åˆ†ç¦»åº¦æƒé‡
        penalty_weight=penalty_weight,  # L2ä¸­å±€éƒ¨å¯†åº¦æƒ©ç½šæƒé‡
        silent=fast_mode,
        l2_components=resolved_l2_components,
        l2_component_weights=resolved_l2_weights,
        l2_component_params=resolved_l2_params
    )

    # ========== æ­¥éª¤7.5: è®¡ç®—æœ‰æ ‡ç­¾æ ·æœ¬ACCï¼ˆè€ƒè™‘unknown_clustersæƒ©ç½šï¼‰ ==========
    if not eval_dense:
        from ..information.labeled_acc_calculation import compute_labeled_acc_with_unknown_penalty
        labeled_acc_new, labeled_acc_metrics = compute_labeled_acc_with_unknown_penalty(
            predictions=predictions,
            targets=targets,
            labeled_mask=labeled_mask,
            unknown_clusters=unknown_clusters,
            silent=fast_mode
        )
        # ç”¨æ–°çš„labeled_accè¦†ç›–åŸæ¥çš„
        loss_dict['l1_metrics']['accuracy'] = labeled_acc_new
        loss_dict['l1_metrics'].update(labeled_acc_metrics)

    if not fast_mode:
        print(f"ğŸ“ˆ èšç±»ç»“æœ:")
        print(f"   èšç±»æ•°é‡: {n_clusters}")
        print(f"   æ½œåœ¨æœªçŸ¥ç±»: {len(unknown_clusters)}ä¸ª")
        print(f"   All ACC: {all_acc:.4f}")
        print(f"   Old ACC: {old_acc:.4f}")
        print(f"   New ACC: {new_acc:.4f}")

    # ========== æ­¥éª¤8: K-meansåŸºçº¿å¯¹æ¯”ï¼ˆå¯é€‰ï¼Œä½¿ç”¨test_dataï¼‰ ==========
    kmeans_results = {}
    if run_kmeans_baseline:
        # è·å–æµ‹è¯•é›†æ•°æ®ï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰è·å–ï¼‰
        if eval_dense:
            test_data = dataset.get_test_subset()
        # else: test_data å·²åœ¨æ­¥éª¤5ä¸­è·å–

        n_true_classes = len(np.unique(test_data['targets']))

        kmeans_baseline = test_kmeans_baseline(
            test_data['features'],
            test_data['targets'],
            test_data['known_mask'],
            n_clusters=n_true_classes,
            random_state=0,
            eval_version=eval_version
        )

        kmeans_results = {
            'kmeans_all_acc': kmeans_baseline['all_acc'],
            'kmeans_old_acc': kmeans_baseline['old_acc'],
            'kmeans_new_acc': kmeans_baseline['new_acc'],
            'kmeans_n_clusters': kmeans_baseline['n_clusters']
        }

    # ========== æ­¥éª¤9: è¿”å›ç»“æœ ==========
    results = {
        'method': 'SS-DDBC',
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'n_clusters': n_clusters,
        'unknown_clusters': unknown_clusters,
        'dbcv': None,  # DBCVå·²ç§»é™¤
        'labeled_acc': loss_dict['l1_metrics'].get('accuracy'),  # æœ‰æ ‡ç­¾æ ·æœ¬çš„åˆ†é…å‡†ç¡®ç‡
        'loss': loss_dict['total_loss'],
        'l1': loss_dict['l1'],
        'l2': loss_dict['l2'],
        'loss_dict': loss_dict,
        'l2_components': resolved_l2_components,
        'l2_component_weights': resolved_l2_weights,
        'l2_component_params': resolved_l2_params,
        'dataset': dataset,  # åŒ…å«æ‰€æœ‰æ•°æ®å’Œå…ƒä¿¡æ¯
        'test_features': dataset.test_features,
        'test_targets': dataset.test_targets,
        'test_known_mask': dataset.test_known_mask,
        'train_features': dataset.train_features,
        'train_targets': dataset.train_targets,
        'train_known_mask': dataset.train_known_mask,
        'train_labeled_mask': dataset.train_labeled_mask,

        # ==== ä¸ºè®­ç»ƒ/ä¸Šå±‚è°ƒç”¨å‡†å¤‡çš„å…³é”®ä¿¡æ¯ ====
        # å®Œæ•´èšç±»æ ‡ç­¾ï¼ˆå«æ ¸å¿ƒç‚¹+ç¨€ç–ç‚¹ï¼‰
        'labels': predictions,
        # ç®€å•æ ¸å¿ƒç‚¹ç´¢å¼•ï¼šå½“å‰å®ç°ä¸­ï¼Œfinal_labels != -1 å³è§†ä¸ºå·²åˆ†é…ç‚¹
        # åç»­å¦‚éœ€æ›´ç²¾ç»†å®šä¹‰ï¼Œå¯æ”¹ä¸ºåŸºäº core_clusters æˆ–é«˜å¯†åº¦æ©ç ã€‚
        'core_points': np.where(np.asarray(predictions) != -1)[0].tolist(),
        # å½“å‰è¿™æ¬¡è°ƒç”¨ä½¿ç”¨çš„è¶…å‚æ•°ï¼ˆå•ç‚¹è°ƒç”¨æ—¶å³â€œæœ€ä½³å‚æ•°â€ï¼‰
        'best_params': {
            'k': k,
            'density_percentile': density_percentile,
        },
    }

    # åˆå¹¶K-meansç»“æœ
    results.update(kmeans_results)

    return results
