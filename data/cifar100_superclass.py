"""
CIFAR-100è¶…ç±»æ•°æ®é›†å®ç° - ä¸ºGCDé¡¹ç›®å®šåˆ¶ç‰ˆæœ¬
åŸºäºDCCLé¡¹ç›®çš„15ä¸ªè¶…ç±»åˆ’åˆ†æ–¹æ¡ˆï¼Œé€‚é…GCDé¡¹ç›®çš„æ•°æ®ç»“æ„
"""

import json
import numpy as np
import os
from torchvision.datasets import CIFAR100
from copy import deepcopy
from data.data_utils import subsample_instances
from config import cifar_100_root

# CIFAR-100 15ä¸ªè¶…ç±»æ˜ å°„ï¼ˆä¸DCCLé¡¹ç›®ä¿æŒä¸€è‡´ï¼‰
CIFAR100_SUPERCLASSES = {
    'trees': [47, 52, 56, 59, 96],
    'flowers': [54, 62, 70, 82, 92],
    'fruits_vegetables': [0, 51, 53, 57, 83],
    'mammals': [3, 4, 21, 31, 34, 36, 38, 43, 50, 55, 63, 64, 65, 66, 72, 74, 75, 15, 19, 42, 80, 88, 97],
    'marine_animals': [1, 26, 27, 30, 32, 45, 67, 73, 95, 91],
    'insects_arthropods': [6, 7, 14, 24, 77, 18, 79, 99],
    'reptiles': [44, 78, 29, 93],
    'humans': [11, 35, 46, 2, 98],
    'furniture': [5, 20, 25, 84, 94],
    'containers': [16, 9, 10, 28, 61],
    'vehicles': [8, 13, 48, 58, 69, 81, 85, 89, 90],
    'electronic_devices': [22, 39, 40, 41, 86, 87],
    'buildings': [12, 17, 37, 76],
    'terrain': [33, 49, 60, 68, 71],
    'weather_phenomena': [23]
}

# åˆ›å»ºç±»åˆ«åˆ°è¶…ç±»çš„æ˜ å°„
CLASS_TO_SUPERCLASS = {}
SUPERCLASS_NAMES = list(CIFAR100_SUPERCLASSES.keys())
for superclass_id, (superclass_name, class_list) in enumerate(CIFAR100_SUPERCLASSES.items()):
    for class_id in class_list:
        CLASS_TO_SUPERCLASS[class_id] = superclass_id


class CustomCIFAR100Superclass(CIFAR100):
    """
    æ”¯æŒè¶…ç±»åˆ’åˆ†çš„è‡ªå®šä¹‰CIFAR-100æ•°æ®é›†
    ä¸GCDé¡¹ç›®çš„CustomCIFAR100ä¿æŒå…¼å®¹
    """

    def __init__(self, *args, target_transform=None, **kwargs):
        self.verbose = kwargs.pop('verbose', True)
        super(CustomCIFAR100Superclass, self).__init__(*args, **kwargs)

        self.original_target_transform = target_transform
        self.uq_idxs = np.array(range(len(self)))

        # æ ‡ç­¾æ˜ å°„å­—å…¸ï¼ˆç”¨äºè¶…ç±»å†…éƒ¨è®­ç»ƒæ—¶çš„è¿ç»­æ ‡ç­¾æ˜ å°„ï¼‰
        self.label_mapping = None
        self.reverse_label_mapping = None

        # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªè¶…ç±»æ•°æ®é›†ï¼Œä¸åº”è¯¥è¢«å¤–éƒ¨çš„target_transformè¦†ç›–
        self.is_superclass_dataset = True

    @property
    def target_transform(self):
        """è·å–target_transform"""
        return self._target_transform if hasattr(self, '_target_transform') else None

    @target_transform.setter
    def target_transform(self, value):
        """è®¾ç½®target_transformï¼Œä½†å¦‚æœå·²æœ‰label_mappingåˆ™å¿½ç•¥å¤–éƒ¨è®¾ç½®"""
        if hasattr(self, 'label_mapping') and self.label_mapping is not None:
            # å¦‚æœå·²ç»æœ‰äº†æ ‡ç­¾æ˜ å°„ï¼Œå¿½ç•¥å¤–éƒ¨çš„target_transformè®¾ç½®
            if self.verbose:
                print(f"å¿½ç•¥å¤–éƒ¨target_transformè®¾ç½®ï¼Œä½¿ç”¨å†…éƒ¨æ ‡ç­¾æ˜ å°„")
            return
        self._target_transform = value

    def get_superclass_label(self, class_id):
        """è·å–ç±»åˆ«å¯¹åº”çš„è¶…ç±»æ ‡ç­¾"""
        return CLASS_TO_SUPERCLASS.get(class_id, -1)

    def create_label_mapping(self, class_list):
        """
        ä¸ºæŒ‡å®šçš„ç±»åˆ«åˆ—è¡¨åˆ›å»ºè¿ç»­æ ‡ç­¾æ˜ å°„
        Args:
            class_list: åŸå§‹ç±»åˆ«IDåˆ—è¡¨ï¼Œå¦‚ [47, 52, 56, 59]
        """
        # æ’åºç¡®ä¿æ˜ å°„ä¸€è‡´æ€§
        sorted_classes = sorted(class_list)

        # åˆ›å»ºåŸå§‹æ ‡ç­¾åˆ°è¿ç»­æ ‡ç­¾çš„æ˜ å°„
        self.label_mapping = {original_class: idx for idx, original_class in enumerate(sorted_classes)}

        # åˆ›å»ºåå‘æ˜ å°„ï¼ˆè¿ç»­æ ‡ç­¾åˆ°åŸå§‹æ ‡ç­¾ï¼‰
        self.reverse_label_mapping = {idx: original_class for idx, original_class in enumerate(sorted_classes)}

        if self.verbose:
            print(f"   åˆ›å»ºæ ‡ç­¾æ˜ å°„: {self.label_mapping}")
        return self.label_mapping

    def __getitem__(self, item):
        img, label = super().__getitem__(item)
        uq_idx = self.uq_idxs[item]

        # åº”ç”¨æ ‡ç­¾é‡æ–°æ˜ å°„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.label_mapping is not None and label in self.label_mapping:
            label = self.label_mapping[label]
            # å¦‚æœæœ‰è‡ªå·±çš„æ ‡ç­¾æ˜ å°„ï¼Œå°±ä¸è¦å†åº”ç”¨å¤–éƒ¨çš„target_transformäº†
        else:
            # åªæœ‰åœ¨æ²¡æœ‰è‡ªå·±çš„æ ‡ç­¾æ˜ å°„æ—¶æ‰åº”ç”¨target_transform
            if self.target_transform is not None:
                try:
                    label = self.target_transform(label)
                except KeyError as e:
                    print(f"KeyError: æ ‡ç­¾ {label} ä¸åœ¨target_transformæ˜ å°„ä¸­")
                    print(f"å¯ç”¨çš„æ˜ å°„é”®: {list(self.target_transform.__closure__[0].cell_contents.keys()) if hasattr(self.target_transform, '__closure__') else 'N/A'}")
                    raise e
            elif self.original_target_transform is not None:
                label = self.original_target_transform(label)

        # è¿”å›æ ¼å¼ä¸GCDé¡¹ç›®ä¸€è‡´ï¼š(img, label, uq_idx)
        return img, label, uq_idx

    def __len__(self):
        return len(self.targets)


def subsample_classes_superclass(dataset, include_classes):
    """
    æŒ‰æŒ‡å®šç±»åˆ«ç­›é€‰æ•°æ®é›†ï¼Œé€‚é…GCDé¡¹ç›®ç»“æ„
    """
    include_classes = np.array(include_classes)

    if hasattr(dataset, 'targets'):
        targets = np.array(dataset.targets)
    else:
        targets = np.array([dataset[i][1] for i in range(len(dataset))])

    # æ‰¾åˆ°å±äºæŒ‡å®šç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
    class_mask = np.isin(targets, include_classes)
    indices = np.where(class_mask)[0]

    # åˆ›å»ºæ–°çš„æ•°æ®é›†
    new_dataset = deepcopy(dataset)

    if hasattr(new_dataset, 'data'):
        new_dataset.data = dataset.data[indices]
    if hasattr(new_dataset, 'targets'):
        new_dataset.targets = [dataset.targets[i] for i in indices]
    if hasattr(new_dataset, 'uq_idxs'):
        new_dataset.uq_idxs = dataset.uq_idxs[indices]

    return new_dataset


def subsample_dataset_superclass(dataset, idxs):
    """
    æŒ‰ç´¢å¼•å­é‡‡æ ·æ•°æ®é›†ï¼Œé€‚é…GCDé¡¹ç›®ç»“æ„
    """
    if len(idxs) > 0:
        if hasattr(dataset, 'data'):
            dataset.data = dataset.data[idxs]
        if hasattr(dataset, 'targets'):
            dataset.targets = np.array(dataset.targets)[idxs].tolist()
        if hasattr(dataset, 'uq_idxs'):
            dataset.uq_idxs = dataset.uq_idxs[idxs]

    return dataset


def get_train_val_indices_superclass(train_dataset, val_split=0.2):
    """
    è·å–è®­ç»ƒéªŒè¯é›†ç´¢å¼•ï¼Œé€‚é…è¶…ç±»æ•°æ®é›†
    """
    train_classes = np.unique(train_dataset.targets)

    # Get train/test indices
    train_idxs = []
    val_idxs = []
    for cls in train_classes:
        cls_idxs = np.where(np.array(train_dataset.targets) == cls)[0]

        v_ = np.random.choice(cls_idxs, replace=False, size=((int(val_split * len(cls_idxs))),))
        t_ = [x for x in cls_idxs if x not in v_]

        train_idxs.extend(t_)
        val_idxs.extend(v_)

    return train_idxs, val_idxs


def get_cifar100_superclass_datasets(train_transform, test_transform, train_classes,
                                   prop_train_labels=0.8, split_train_val=False, seed=0,
                                   verbose=True):
    """
    è·å–åŸºäºè¶…ç±»çš„CIFAR-100æ•°æ®é›†ï¼Œä¸GCDé¡¹ç›®çš„æ¥å£ä¿æŒä¸€è‡´

    Args:
        train_transform: è®­ç»ƒæ•°æ®å˜æ¢
        test_transform: æµ‹è¯•æ•°æ®å˜æ¢
        train_classes: è®­ç»ƒç±»åˆ«åˆ—è¡¨ï¼ˆå·²çŸ¥ç±»ï¼‰
        prop_train_labels: æœ‰æ ‡ç­¾æ ·æœ¬æ¯”ä¾‹
        split_train_val: æ˜¯å¦åˆ†ç¦»è®­ç»ƒéªŒè¯é›†
        seed: éšæœºç§å­
        verbose: æ˜¯å¦æ‰“å°æ•°æ®åŠ è½½ä¿¡æ¯
    """
    np.random.seed(seed)

    # ä½¿ç”¨å®šåˆ¶çš„CIFAR-100è¶…ç±»æ•°æ®é›†
    whole_training_set = CustomCIFAR100Superclass(
        root=cifar_100_root, transform=train_transform, train=True, verbose=verbose
    )

    test_dataset = CustomCIFAR100Superclass(
        root=cifar_100_root, transform=test_transform, train=False, verbose=verbose
    )

    # æ ‡å‡†çš„ç±»åˆ«ç­›é€‰æµç¨‹ï¼ˆä¸GCDé¡¹ç›®ä¸€è‡´ï¼‰
    train_dataset_labelled = subsample_classes_superclass(deepcopy(whole_training_set), include_classes=train_classes)
    subsample_indices = subsample_instances(train_dataset_labelled, prop_indices_to_subsample=prop_train_labels)
    train_dataset_labelled = subsample_dataset_superclass(train_dataset_labelled, subsample_indices)

    # è·å–æ— æ ‡ç­¾æ•°æ®
    unlabelled_indices = set(whole_training_set.uq_idxs) - set(train_dataset_labelled.uq_idxs)
    train_dataset_unlabelled = subsample_dataset_superclass(
        deepcopy(whole_training_set), np.array(list(unlabelled_indices))
    )

    # è®­ç»ƒéªŒè¯é›†åˆ†ç¦»ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if split_train_val:
        train_idxs, val_idxs = get_train_val_indices_superclass(train_dataset_labelled)
        train_dataset_labelled_split = subsample_dataset_superclass(deepcopy(train_dataset_labelled), train_idxs)
        val_dataset_labelled_split = subsample_dataset_superclass(deepcopy(train_dataset_labelled), val_idxs)
        val_dataset_labelled_split.transform = test_transform

        train_dataset_labelled = train_dataset_labelled_split
        val_dataset_labelled = val_dataset_labelled_split
    else:
        val_dataset_labelled = None

    all_datasets = {
        'train_labelled': train_dataset_labelled,
        'train_unlabelled': train_dataset_unlabelled,
        'val': val_dataset_labelled,
        'test': test_dataset
    }

    return all_datasets


def get_superclass_splits():
    """
    è·å–15ä¸ªè¶…ç±»çš„å·²çŸ¥/æœªçŸ¥ç±»åˆ’åˆ†
    è¿”å›æ¯ä¸ªè¶…ç±»ä¸­çš„å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»åˆ—è¡¨
    """
    superclass_splits = {}

    for superclass_name, class_list in CIFAR100_SUPERCLASSES.items():
        # æŒ‰ç…§GCDè®¾å®šï¼šå‰80ä¸ªä¸ºå·²çŸ¥ç±»ï¼Œå20ä¸ªä¸ºæœªçŸ¥ç±»
        known_classes = [cls for cls in class_list if cls < 80]
        unknown_classes = [cls for cls in class_list if cls >= 80]

        superclass_splits[superclass_name] = {
            'known_classes': known_classes,
            'unknown_classes': unknown_classes,
            'superclass_id': SUPERCLASS_NAMES.index(superclass_name)
        }

    return superclass_splits


def get_single_superclass_datasets(superclass_name, train_transform, test_transform,
                                 prop_train_labels=0.8, split_train_val=False, seed=0,
                                 verbose: bool = True):
    """
    è·å–å•ä¸ªè¶…ç±»çš„æ•°æ®é›†ï¼Œç”¨äºè¶…ç±»å†…éƒ¨çš„GCDè®­ç»ƒ

    Args:
        superclass_name: è¶…ç±»åç§°ï¼Œå¦‚ 'trees', 'flowers' ç­‰
        å…¶ä»–å‚æ•°åŒ get_cifar100_superclass_datasets

    Returns:
        åŒ…å«è¯¥è¶…ç±»æ‰€æœ‰ç±»åˆ«çš„æ•°æ®é›†ï¼ŒæŒ‰GCDè®¾ç½®åˆ’åˆ†å·²çŸ¥/æœªçŸ¥ç±»
    """
    if superclass_name not in CIFAR100_SUPERCLASSES:
        raise ValueError(f"æœªçŸ¥è¶…ç±»åç§°: {superclass_name}")

    # è·å–è¯¥è¶…ç±»åŒ…å«çš„æ‰€æœ‰ç±»åˆ«
    superclass_classes = CIFAR100_SUPERCLASSES[superclass_name]

    # æŒ‰GCDè®¾å®šåˆ’åˆ†å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»
    known_classes = [cls for cls in superclass_classes if cls < 80]
    unknown_classes = [cls for cls in superclass_classes if cls >= 80]

    if verbose:
        print(f"è¶…ç±» '{superclass_name}' åŒ…å«ç±»åˆ«: {superclass_classes}")
        print(f"å·²çŸ¥ç±» ({len(known_classes)}): {known_classes}")
        print(f"æœªçŸ¥ç±» ({len(unknown_classes)}): {unknown_classes}")

    # å¦‚æœè¯¥è¶…ç±»æ²¡æœ‰å·²çŸ¥ç±»æˆ–æœªçŸ¥ç±»ï¼Œç»™å‡ºè­¦å‘Š
    if len(known_classes) == 0 and verbose:
        print(f"è­¦å‘Šï¼šè¶…ç±» '{superclass_name}' æ²¡æœ‰å·²çŸ¥ç±»ï¼ˆç±»åˆ«ID < 80ï¼‰")
    if len(unknown_classes) == 0 and verbose:
        print(f"è­¦å‘Šï¼šè¶…ç±» '{superclass_name}' æ²¡æœ‰æœªçŸ¥ç±»ï¼ˆç±»åˆ«ID >= 80ï¼‰")

    # ä½¿ç”¨æ ‡å‡†çš„CIFAR-100è¶…ç±»æ•°æ®è·å–å‡½æ•°
    all_classes = known_classes + unknown_classes
    datasets = get_cifar100_superclass_datasets(
        train_transform=train_transform,
        test_transform=test_transform,
        train_classes=known_classes,  # åªæœ‰å·²çŸ¥ç±»ä½œä¸ºè®­ç»ƒç±»
        prop_train_labels=prop_train_labels,
        split_train_val=split_train_val,
        seed=seed,
        verbose=verbose
    )

    # è¿‡æ»¤æ•°æ®é›†ï¼Œåªä¿ç•™è¯¥è¶…ç±»çš„æ ·æœ¬ï¼Œå¹¶åˆ›å»ºæ ‡ç­¾æ˜ å°„
    filtered_datasets = {}
    for split_name, dataset in datasets.items():
        if dataset is not None:
            if verbose:
                print(f"\nğŸ“Š å¤„ç†{split_name}æ•°æ®é›†:")
            # è¿‡æ»¤æ ·æœ¬ï¼Œåªä¿ç•™å±äºå½“å‰è¶…ç±»çš„æ ·æœ¬
            filtered_dataset = filter_dataset_by_classes(dataset, all_classes, split_name, verbose=verbose)

            # ä¸ºè¿‡æ»¤åçš„æ•°æ®é›†åˆ›å»ºè¿ç»­æ ‡ç­¾æ˜ å°„
            if hasattr(filtered_dataset, 'create_label_mapping'):
                filtered_dataset.create_label_mapping(all_classes)

            filtered_datasets[split_name] = filtered_dataset
        else:
            filtered_datasets[split_name] = None

    return filtered_datasets


def filter_dataset_by_classes(dataset, target_classes, split_name="æ•°æ®é›†", verbose: bool = True):
    """
    è¿‡æ»¤æ•°æ®é›†ï¼Œåªä¿ç•™æŒ‡å®šç±»åˆ«çš„æ ·æœ¬
    """
    target_classes = set(target_classes)

    # è·å–æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾
    if hasattr(dataset, 'targets'):
        all_labels = np.array(dataset.targets)
    else:
        all_labels = np.array([dataset[i][1] for i in range(len(dataset))])

    # æ‰¾åˆ°å±äºç›®æ ‡ç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
    valid_mask = np.isin(all_labels, list(target_classes))
    valid_indices = np.where(valid_mask)[0]

    if len(valid_indices) == 0:
        if verbose:
            print(f"   âš ï¸ è­¦å‘Š: {split_name}ä¸­æ²¡æœ‰æ‰¾åˆ°å±äºç±»åˆ« {target_classes} çš„æ ·æœ¬")
        return dataset

    # åˆ›å»ºè¿‡æ»¤åçš„æ•°æ®é›†
    filtered_dataset = deepcopy(dataset)

    if hasattr(filtered_dataset, 'data'):
        filtered_dataset.data = dataset.data[valid_indices]
    if hasattr(filtered_dataset, 'targets'):
        filtered_dataset.targets = [dataset.targets[i] for i in valid_indices]
    if hasattr(filtered_dataset, 'uq_idxs'):
        filtered_dataset.uq_idxs = dataset.uq_idxs[valid_indices]

    if verbose:
        print(f"   è¿‡æ»¤å{split_name}å¤§å°: {len(filtered_dataset)} (åŸå§‹: {len(dataset)})")

    return filtered_dataset


# ä¸ºäº†ä¸GCDé¡¹ç›®çš„get_datasets.pyé›†æˆï¼Œæ·»åŠ ä»¥ä¸‹å‡½æ•°
def subsample_classes(dataset, include_classes):
    """
    ä¸GCDé¡¹ç›®ä¸­çš„subsample_classesä¿æŒå…¼å®¹çš„æ¥å£
    """
    return subsample_classes_superclass(dataset, include_classes)


if __name__ == '__main__':
    # æµ‹è¯•è¶…ç±»æ•°æ®é›†åŠŸèƒ½
    from torchvision import transforms

    # ç®€å•çš„å˜æ¢
    train_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    test_transform = transforms.ToTensor()

    # æµ‹è¯•å•ä¸ªè¶…ç±»æ•°æ®é›†
    superclass_name = 'trees'
    datasets = get_single_superclass_datasets(
        superclass_name=superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=0.8,
        split_train_val=False,
        seed=0
    )

    print(f'\nè¶…ç±» "{superclass_name}" æ•°æ®é›†ç»Ÿè®¡:')
    for k, v in datasets.items():
        if v is not None:
            print(f'{k}: {len(v)} æ ·æœ¬')

    # æµ‹è¯•æ‰€æœ‰è¶…ç±»çš„åˆ’åˆ†
    print(f'\næ‰€æœ‰15ä¸ªè¶…ç±»çš„åˆ’åˆ†:')
    superclass_splits = get_superclass_splits()
    for name, split_info in superclass_splits.items():
        print(f'{name} (ID: {split_info["superclass_id"]}): '
              f'{len(split_info["known_classes"])} å·²çŸ¥ç±», '
              f'{len(split_info["unknown_classes"])} æœªçŸ¥ç±»')
