#!/usr/bin/env python3
"""
CIFAR-100è¶…ç±»åŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨GCDé¡¹ç›®ä¸­ä½¿ç”¨15ä¸ªè¶…ç±»æ•°æ®åˆ’åˆ†
"""

import os
import sys
from torchvision import transforms

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.cifar100_superclass import (
    get_single_superclass_datasets,
    get_superclass_splits,
    SUPERCLASS_NAMES,
    CIFAR100_SUPERCLASSES
)


def demo_superclass_info():
    """æ¼”ç¤ºè¶…ç±»ä¿¡æ¯æŸ¥çœ‹"""
    print("ğŸŒŸ CIFAR-100 15ä¸ªè¶…ç±»ä¿¡æ¯")
    print("=" * 80)

    superclass_splits = get_superclass_splits()

    for i, (name, info) in enumerate(superclass_splits.items()):
        print(f"\nè¶…ç±» {i}: {name}")
        print(f"  åŒ…å«ç±»åˆ«æ€»æ•°: {len(CIFAR100_SUPERCLASSES[name])}")
        print(f"  åŸå§‹ç±»åˆ«ID: {CIFAR100_SUPERCLASSES[name]}")
        print(f"  å·²çŸ¥ç±» (< 80): {info['known_classes']} (å…±{len(info['known_classes'])}ä¸ª)")
        print(f"  æœªçŸ¥ç±» (>= 80): {info['unknown_classes']} (å…±{len(info['unknown_classes'])}ä¸ª)")

        # æ£€æŸ¥æ˜¯å¦é€‚åˆGCDè®­ç»ƒ
        if len(info['known_classes']) == 0:
            print(f"  âš ï¸  è­¦å‘Š: æ²¡æœ‰å·²çŸ¥ç±»ï¼Œä¸é€‚åˆGCDè®­ç»ƒ")
        elif len(info['unknown_classes']) == 0:
            print(f"  âš ï¸  è­¦å‘Š: æ²¡æœ‰æœªçŸ¥ç±»ï¼Œä¸é€‚åˆGCDè®­ç»ƒ")
        else:
            print(f"  âœ… é€‚åˆGCDè®­ç»ƒ")


def demo_single_superclass_dataset():
    """æ¼”ç¤ºå•ä¸ªè¶…ç±»æ•°æ®é›†åŠ è½½"""
    print("\nğŸ¯ å•ä¸ªè¶…ç±»æ•°æ®é›†åŠ è½½æ¼”ç¤º")
    print("=" * 80)

    # é€‰æ‹©ä¸€ä¸ªæœ‰å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»çš„è¶…ç±»è¿›è¡Œæ¼”ç¤º
    superclass_name = 'mammals'  # å“ºä¹³åŠ¨ç‰©ç±»ï¼Œé€šå¸¸åŒ…å«è¾ƒå¤šæ ·æœ¬
    print(f"æ¼”ç¤ºè¶…ç±»: {superclass_name}")

    # å®šä¹‰ç®€å•çš„æ•°æ®å˜æ¢
    train_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    test_transform = transforms.ToTensor()

    try:
        # è·å–è¶…ç±»æ•°æ®é›†
        datasets = get_single_superclass_datasets(
            superclass_name=superclass_name,
            train_transform=train_transform,
            test_transform=test_transform,
            prop_train_labels=0.8,  # 80%æœ‰æ ‡ç­¾æ ·æœ¬
            split_train_val=False,
            seed=42
        )

        print(f"\nğŸ“Š è¶…ç±» '{superclass_name}' æ•°æ®é›†ç»Ÿè®¡:")
        for split_name, dataset in datasets.items():
            if dataset is not None:
                print(f"  {split_name}: {len(dataset)} æ ·æœ¬")

                # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
                if hasattr(dataset, 'targets'):
                    unique_labels = set(dataset.targets)
                    print(f"    åŒ…å«æ ‡ç­¾: {sorted(unique_labels)}")
            else:
                print(f"  {split_name}: None")

        # æµ‹è¯•æ•°æ®åŠ è½½
        if datasets['train_labelled'] is not None:
            sample_img, sample_label, sample_idx = datasets['train_labelled'][0]
            print(f"\nğŸ” æ ·æœ¬æ£€æŸ¥:")
            print(f"  å›¾åƒå½¢çŠ¶: {sample_img.shape}")
            print(f"  æ ‡ç­¾: {sample_label}")
            print(f"  å”¯ä¸€ç´¢å¼•: {sample_idx}")

    except Exception as e:
        print(f"âŒ åŠ è½½è¶…ç±»æ•°æ®é›†æ—¶å‡ºé”™: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿CIFAR-100æ•°æ®é›†å·²ä¸‹è½½")


def demo_data_split_usage():
    """æ¼”ç¤ºæ•°æ®åˆ’åˆ†ä½¿ç”¨"""
    print("\nğŸ“Š æ•°æ®åˆ’åˆ†ä½¿ç”¨æ¼”ç¤º")
    print("=" * 80)

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°æ®åˆ’åˆ†æ–‡ä»¶
    splits_dir = './data_splits'
    if os.path.exists(splits_dir):
        print(f"âœ… æ‰¾åˆ°æ•°æ®åˆ’åˆ†ç›®å½•: {splits_dir}")

        # åˆ—å‡ºæ‰€æœ‰åˆ’åˆ†æ–‡ä»¶
        split_files = [f for f in os.listdir(splits_dir) if f.startswith('superclass_') and f.endswith('.json')]
        print(f"ğŸ“ æ‰¾åˆ° {len(split_files)} ä¸ªè¶…ç±»åˆ’åˆ†æ–‡ä»¶:")

        for file in sorted(split_files)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"  - {file}")

        if len(split_files) > 5:
            print(f"  ... è¿˜æœ‰ {len(split_files) - 5} ä¸ªæ–‡ä»¶")

        # è¯»å–æ€»ç»“æ–‡ä»¶
        summary_file = os.path.join(splits_dir, 'gcd_superclass_splits_summary.json')
        if os.path.exists(summary_file):
            import json
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)

            print(f"\nğŸ“ˆ æ•°æ®åˆ’åˆ†æ€»ç»“:")
            total_stats = summary['gcd_superclass_splits_summary']['total_statistics']
            print(f"  æ€»è®­ç»ƒæ ·æœ¬: {total_stats['total_train_samples']}")
            print(f"  æ€»éªŒè¯æ ·æœ¬: {total_stats['total_val_samples']}")
            print(f"  æ€»æµ‹è¯•æ ·æœ¬: {total_stats['total_test_samples']}")
            print(f"  æ€»è®¡æ ·æœ¬: {total_stats['grand_total']}")

    else:
        print(f"âŒ æœªæ‰¾åˆ°æ•°æ®åˆ’åˆ†ç›®å½•: {splits_dir}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ data_split_generator.py ç”Ÿæˆæ•°æ®åˆ’åˆ†")


def demo_training_commands():
    """æ¼”ç¤ºè®­ç»ƒå‘½ä»¤"""
    print("\nğŸš€ è®­ç»ƒå‘½ä»¤ç¤ºä¾‹")
    print("=" * 80)

    print("1. è®­ç»ƒå•ä¸ªè¶…ç±»:")
    print("   python train_superclass.py --superclass_name mammals --epochs 20")
    print("   python train_superclass.py --superclass_name trees --epochs 20")

    print("\n2. è®­ç»ƒæ‰€æœ‰è¶…ç±»:")
    print("   python train_superclass.py --train_all_superclasses --epochs 20")

    print("\n3. è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ:")
    print("   python train_superclass.py --superclass_name flowers \\")
    print("                               --epochs 50 \\")
    print("                               --batch_size 64 \\")
    print("                               --lr 0.05 \\")
    print("                               --prop_train_labels 0.8")

    print("\n4. ç”Ÿæˆæ•°æ®åˆ’åˆ†:")
    print("   python data_split_generator.py --output_dir ./data_splits")


def demo_integration_check():
    """æ¼”ç¤ºé›†æˆæ£€æŸ¥"""
    print("\nğŸ”§ é›†æˆæ£€æŸ¥")
    print("=" * 80)

    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    required_files = [
        'data/cifar100_superclass.py',
        'train_superclass.py',
        'data_split_generator.py'
    ]

    print("ğŸ“‹ æ£€æŸ¥å¿…è¦æ–‡ä»¶:")
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path} (ç¼ºå¤±)")

    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dirs = ['./data', '../data']
    print(f"\nğŸ“ æ£€æŸ¥CIFAR-100æ•°æ®ç›®å½•:")
    found_data = False
    for data_dir in data_dirs:
        if os.path.exists(data_dir):
            print(f"  âœ… {data_dir}")
            cifar_dir = os.path.join(data_dir, 'cifar-100-python')
            if os.path.exists(cifar_dir):
                print(f"    âœ… CIFAR-100æ•°æ®å·²ä¸‹è½½")
                found_data = True
            else:
                print(f"    âš ï¸  CIFAR-100æ•°æ®æœªä¸‹è½½")
        else:
            print(f"  âŒ {data_dir} (ä¸å­˜åœ¨)")

    if not found_data:
        print("ğŸ’¡ å¦‚æœCIFAR-100æ•°æ®æœªä¸‹è½½ï¼Œé¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½")

    # æ£€æŸ¥ä¾èµ–åº“
    print(f"\nğŸ“¦ æ£€æŸ¥å…³é”®ä¾èµ–:")
    try:
        import torch
        print(f"  âœ… PyTorch {torch.__version__}")
    except ImportError:
        print(f"  âŒ PyTorch (æœªå®‰è£…)")

    try:
        import torchvision
        print(f"  âœ… torchvision {torchvision.__version__}")
    except ImportError:
        print(f"  âŒ torchvision (æœªå®‰è£…)")

    try:
        from sklearn.cluster import KMeans
        print(f"  âœ… scikit-learn")
    except ImportError:
        print(f"  âŒ scikit-learn (æœªå®‰è£…)")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸŒŸ CIFAR-100è¶…ç±»åŠŸèƒ½æ¼”ç¤º")
    print("åŸºäºDCCLé¡¹ç›®çš„15ä¸ªè¶…ç±»åˆ’åˆ†ï¼Œé€‚é…GCDé¡¹ç›®")
    print("=" * 80)

    try:
        # 1. è¶…ç±»ä¿¡æ¯æ¼”ç¤º
        demo_superclass_info()

        # 2. æ•°æ®é›†åŠ è½½æ¼”ç¤º
        demo_single_superclass_dataset()

        # 3. æ•°æ®åˆ’åˆ†ä½¿ç”¨æ¼”ç¤º
        demo_data_split_usage()

        # 4. è®­ç»ƒå‘½ä»¤æ¼”ç¤º
        demo_training_commands()

        # 5. é›†æˆæ£€æŸ¥
        demo_integration_check()

        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨è¶…ç±»åŠŸèƒ½è¿›è¡ŒGCDè®­ç»ƒäº†ã€‚")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()