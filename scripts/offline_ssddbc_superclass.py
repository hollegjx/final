#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¦»çº¿ SSDDBC ç®¡çº¿å°è£…è„šæœ¬ï¼ˆå•è¶…ç±»ï¼‰

è®¾è®¡ç›®æ ‡ï¼š
- åœ¨ã€Œä¿å­˜å¥½ ckptã€ä¹‹åï¼Œç¦»çº¿æ‰§è¡Œçš„æµç¨‹åº”å½“ä¸åŸå…ˆæ‰‹åŠ¨è¿è¡Œçš„
  `scripts/cache_features.py` + `python -m ssddbc.grid_search.batch_runner` å®Œå…¨ä¸€è‡´ï¼š
    1) ä½¿ç”¨æŒ‡å®š ckpt æå–å¹¶ç¼“å­˜ç‰¹å¾ï¼›
    2) è°ƒç”¨ batch_runner åœ¨ç‰¹å¾ç¼“å­˜ä¸Šåšç½‘æ ¼æœç´¢ã€‚

ç‰¹ç‚¹ï¼š
- æœ¬è„šæœ¬æœ¬èº«ä¸åšç‰¹å¾è®¡ç®—å’Œèšç±»å®ç°ï¼Œåªæ˜¯é¡ºåºè°ƒç”¨å·²æœ‰è„šæœ¬ï¼›
- æç‰¹å¾é˜¶æ®µåœ¨ä¸€ä¸ªè¿›ç¨‹é‡Œå®Œæˆï¼Œç»“æŸåè¿›ç¨‹é€€å‡ºï¼›
- èšç±»é˜¶æ®µç”± batch_runner ç‹¬ç«‹è¿›ç¨‹å®Œæˆï¼Œè¡Œä¸ºä¸è¿‡å»ä½¿ç”¨æ–¹å¼ä¸€è‡´ã€‚
"""

from __future__ import annotations

import argparse
import os
import subprocess
import sys
from datetime import datetime
from typing import Dict, Any

import numpy as np

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from config import feature_cache_dir as DEFAULT_FEATURE_CACHE_DIR
from utils.data.feature_loader import FeatureLoader
from utils.pseudo_labels import save_pseudo_labels
from ssddbc.grid_search.api import run_clustering_search_on_features


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="ç¦»çº¿ SSDDBCï¼šckpt -> ç‰¹å¾ç¼“å­˜ -> batch_runnerï¼ˆå•è¶…ç±»ï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # å¿…è¦å‚æ•°
    parser.add_argument(
        "--superclass_name",
        type=str,
        required=True,
        help="è¦å¤„ç†çš„è¶…ç±»åç§°ï¼Œå¦‚ 'trees'ã€‚",
    )
    parser.add_argument(
        "--ckpt_path",
        type=str,
        required=True,
        help="è®­ç»ƒé˜¶æ®µä¿å­˜çš„å®Œæ•´ ckpt è·¯å¾„ï¼ˆæ¨¡å‹+æŠ•å½±å¤´ï¼‰ï¼Œå°†ä½œä¸ºç‰¹å¾æå–ç”¨çš„æ¨¡å‹ã€‚",
    )

    # ç‰¹å¾ç¼“å­˜å‚æ•°ï¼ˆä¿æŒä¸åŸæœ‰ scripts/cache_features.py ä¸€è‡´ï¼‰
    parser.add_argument(
        "--feature_cache_dir",
        type=str,
        default=DEFAULT_FEATURE_CACHE_DIR,
        help="ç‰¹å¾ç¼“å­˜æ ¹ç›®å½•ï¼ˆä¸ä¹‹å‰çš„ FEATURE_CACHE_DIR ä¸€è‡´ï¼‰ã€‚",
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=64,
        help="ç‰¹å¾æå–æ‰¹å¤§å°ï¼ˆæ²¿ç”¨ cache_features çš„é»˜è®¤ï¼‰ã€‚",
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,
        help="ç‰¹å¾æå– DataLoader çš„å·¥ä½œçº¿ç¨‹æ•°ã€‚",
    )
    parser.add_argument(
        "--gpu",
        type=int,
        default=0,
        help="ç‰¹å¾æå–æ‰€ç”¨ GPU ç¼–å·ã€‚",
    )
    parser.add_argument(
        "--prop_train_labels",
        type=float,
        default=0.8,
        help="è®­ç»ƒé›†ä¸­æœ‰æ ‡ç­¾æ ·æœ¬å æ¯”ï¼ˆéœ€ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰ã€‚",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=0,
        help="ç‰¹å¾æå–é˜¶æ®µçš„æ•°æ®åˆ’åˆ†éšæœºç§å­ï¼ˆä¸ cache_features/test_feature ä¿æŒä¸€è‡´ï¼‰ã€‚",
    )

    # SSDDBC ç½‘æ ¼æœç´¢å‚æ•°ï¼ˆä¿æŒåŸæ¥ batch_runner çš„ä½¿ç”¨æ–¹å¼ï¼‰
    parser.add_argument("--k_min", type=int, default=3, help="KNN k æœ€å°å€¼ï¼ˆåŒ…å«ï¼‰ã€‚")
    parser.add_argument("--k_max", type=int, default=21, help="KNN k æœ€å¤§å€¼ä¸Šç•Œï¼ˆä¸å«ï¼‰ã€‚")
    parser.add_argument(
        "--density_min",
        type=int,
        default=40,
        help="å¯†åº¦ç™¾åˆ†ä½æœ€å°å€¼ï¼ˆåŒ…å«ï¼‰ã€‚",
    )
    parser.add_argument(
        "--density_max",
        type=int,
        default=100,
        help="å¯†åº¦ç™¾åˆ†ä½æœ€å¤§å€¼ä¸Šç•Œï¼ˆä¸å«ï¼‰ã€‚",
    )
    parser.add_argument(
        "--density_step",
        type=int,
        default=5,
        help="å¯†åº¦ç™¾åˆ†ä½æ­¥é•¿ã€‚",
    )
    parser.add_argument(
        "--max_workers",
        type=int,
        default=None,
        help="batch_runner å¹¶è¡Œè¿›ç¨‹æ•°ä¸Šé™ï¼ˆNone è¡¨ç¤ºç”±è„šæœ¬è‡ªè¡Œå†³å®šï¼Œä½¿ç”¨ CPU ä¸€åŠæ ¸å¿ƒï¼‰ã€‚",
    )
    parser.add_argument(
        "--pseudo_output_dir",
        type=str,
        default=None,
        help="ä¼ªæ ‡ç­¾è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤å†™å…¥ feature_cache_dir/<superclass_name>/pseudo_labelsï¼‰ã€‚",
    )
    parser.add_argument(
        "--skip_feature_extraction",
        action="store_true",
        help="è·³è¿‡ç‰¹å¾æå–é˜¶æ®µï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰ç¼“å­˜ï¼ˆé€‚ç”¨äº pipeline åœºæ™¯ï¼‰ã€‚",
    )

    return parser


def run_cache_features(args: argparse.Namespace) -> None:
    """
    è°ƒç”¨ scripts/cache_features.pyï¼Œä½¿ç”¨æŒ‡å®š ckpt æå–å¹¶ç¼“å­˜ç‰¹å¾ã€‚
    """
    cmd = [
        sys.executable,
        "scripts/cache_features.py",
        "--superclass_name",
        args.superclass_name,
        "--model_path",
        args.ckpt_path,
        "--auto_find_best",
        "False",
        "--cache_dir",
        args.feature_cache_dir,
        "--batch_size",
        str(args.batch_size),
        "--num_workers",
        str(args.num_workers),
        "--gpu",
        str(args.gpu),
        "--prop_train_labels",
        str(args.prop_train_labels),
        "--seed",
        str(args.seed),
    ]

    print("ğŸš€ [Stage 1/2] ä½¿ç”¨ ckpt æå–å¹¶ç¼“å­˜ç‰¹å¾ï¼š")
    print("    ", " ".join(cmd))
    subprocess.run(cmd, check=True)


def _build_pseudo_metadata(args: argparse.Namespace, result: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "ckpt_path": args.ckpt_path,
        "superclass": args.superclass_name,
        "feature_cache_dir": args.feature_cache_dir,
        "generated_at": datetime.now().isoformat(),
        **result,
    }


def _resolve_pseudo_output_path(args: argparse.Namespace, filename: str) -> str:
    if args.pseudo_output_dir:
        base_dir = args.pseudo_output_dir
    else:
        base_dir = os.path.join(args.feature_cache_dir, args.superclass_name, "pseudo_labels")
    os.makedirs(base_dir, exist_ok=True)
    return os.path.join(base_dir, filename)


def _load_cached_features(args: argparse.Namespace) -> Dict[str, Any]:
    loader = FeatureLoader(cache_base_dir=args.feature_cache_dir)
    feature_dict = loader.load(args.superclass_name, use_l2=True, silent=True)
    if feature_dict is None:
        cache_path = loader.get_cache_path(args.superclass_name, use_l2=True)
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç‰¹å¾ç¼“å­˜ï¼Œè¯·ç¡®è®¤ Stage1 æˆåŠŸå®Œæˆ: {cache_path}")
    return feature_dict


def run_offline_clustering(args: argparse.Namespace) -> str:
    """
    è¯»å–ç¼“å­˜ç‰¹å¾å¹¶æ‰§è¡Œå†…å­˜çº§ SSDDBC æœç´¢ï¼Œç”Ÿæˆä¼ªæ ‡ç­¾æ–‡ä»¶ã€‚
    """
    feature_dict = _load_cached_features(args)
    features = feature_dict["all_features"]
    targets = feature_dict["all_targets"]
    known_mask = feature_dict["all_known_mask"]
    labeled_mask = feature_dict["all_labeled_mask"]
    indices = feature_dict.get("all_indices")
    if indices is None:
        print("âš ï¸  ç¼“å­˜ç¼ºå°‘ all_indices å­—æ®µï¼Œå°†é»˜è®¤ä½¿ç”¨é¡ºåºç´¢å¼•ã€‚å»ºè®®é‡æ–°ç”Ÿæˆç¼“å­˜ã€‚")
        indices = np.arange(features.shape[0], dtype=np.int64)

    # æ˜¾ç¤ºæœç´¢é…ç½®
    k_range = range(args.k_min, args.k_max)
    density_range = range(args.density_min, args.density_max, args.density_step)
    n_configs = len(list(k_range)) * len(list(density_range))

    mode_str = f"å¹¶è¡Œæ¨¡å¼ (max_workers={args.max_workers})" if args.max_workers != 1 else "å•è¿›ç¨‹æ¨¡å¼"
    print(f"ğŸš€ [Stage 2/2] åœ¨ç¼“å­˜ç‰¹å¾ä¸Šæ‰§è¡Œ SSDDBC ç½‘æ ¼æœç´¢ ({mode_str})...")
    print(f"   æœç´¢ç©ºé—´: k={list(k_range)}, density={list(density_range)} (å…± {n_configs} ä¸ªé…ç½®)")

    search_result = run_clustering_search_on_features(
        features=features,
        targets=targets,
        known_mask=known_mask,
        labeled_mask=labeled_mask,
        k_range=k_range,
        density_range=density_range,
        random_state=0,
        silent=True,
        max_workers=args.max_workers,
    )

    core_mask = np.zeros_like(indices, dtype=bool)
    if len(search_result.core_points) > 0:
        core_mask[search_result.core_points] = True

    ckpt_base = os.path.splitext(os.path.basename(args.ckpt_path))[0]
    filename = (
        f"{args.superclass_name}_{ckpt_base}"
        f"_k{search_result.best_params['k']}_dp{search_result.best_params['density_percentile']}.npz"
    )
    pseudo_path = _resolve_pseudo_output_path(args, filename)

    metadata = _build_pseudo_metadata(
        args,
        {
            "score": search_result.loss,  # å­—æ®µåä¿æŒlossï¼Œä½†å®é™…æ˜¯scoreï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
            "n_clusters": search_result.n_clusters,
            "num_core_points": int(core_mask.sum()),
        },
    )

    save_pseudo_labels(
        pseudo_path,
        indices=indices,
        labels=search_result.labels,
        core_mask=core_mask,
        best_params=search_result.best_params,
        metadata=metadata,
        densities=search_result.densities,
    )

    print(
        f"âœ… ä¼ªæ ‡ç­¾å·²ä¿å­˜: {pseudo_path}\n"
        f"   Score = {search_result.loss:.4f} (è¶Šå¤§è¶Šå¥½)\n"
        f"   æ ¸å¿ƒç‚¹: {core_mask.sum()} / {len(core_mask)} | æœ€ä½³å‚æ•°: {search_result.best_params}"
    )
    return pseudo_path


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    # è§„èŒƒåŒ–è·¯å¾„
    args.feature_cache_dir = os.path.abspath(args.feature_cache_dir)
    args.ckpt_path = os.path.abspath(args.ckpt_path)

    if not os.path.isfile(args.ckpt_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æŒ‡å®š ckpt æ–‡ä»¶: {args.ckpt_path}")

    # æ¡ä»¶æ€§æ‰§è¡Œç‰¹å¾æå–
    if args.skip_feature_extraction:
        print("â­ï¸  è·³è¿‡ç‰¹å¾æå–é˜¶æ®µï¼ˆä½¿ç”¨å·²æœ‰ç¼“å­˜ï¼‰")
        # éªŒè¯ç¼“å­˜æ˜¯å¦å­˜åœ¨
        from utils.data.feature_loader import FeatureLoader
        loader = FeatureLoader(cache_base_dir=args.feature_cache_dir)
        cache_path = loader.get_cache_path(args.superclass_name, use_l2=True)
        if not os.path.isfile(cache_path):
            raise FileNotFoundError(
                f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}\n"
                f"æç¤º: è¯·å…ˆè¿è¡Œç‰¹å¾æå–ï¼Œæˆ–ç§»é™¤ --skip_feature_extraction å‚æ•°"
            )
        print(f"âœ… æ‰¾åˆ°ç‰¹å¾ç¼“å­˜: {cache_path}")
    else:
        run_cache_features(args)

    pseudo_path = run_offline_clustering(args)

    print("\nâœ… ç¦»çº¿ SSDDBC ç®¡çº¿æ‰§è¡Œå®Œæˆï¼ˆæç‰¹å¾ + SSDDBC ç½‘æ ¼æœç´¢ï¼‰ã€‚")
    print(f"ğŸ“ ä¼ªæ ‡ç­¾æ–‡ä»¶: {pseudo_path}")


if __name__ == "__main__":
    main()
