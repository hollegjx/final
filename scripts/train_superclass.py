#!/usr/bin/env python3
"""
è¶…ç±»è®­ç»ƒè„šæœ¬ - åŸºäºGCDé¡¹ç›®çš„contrastive_training.py
æ”¯æŒ15ä¸ªè¶…ç±»çš„ç‹¬ç«‹è®­ç»ƒå’Œæ•°æ®åˆ’åˆ†
"""

import argparse
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from torch.utils.data import DataLoader
import numpy as np
from sklearn.cluster import KMeans
import torch
from torch.optim import SGD, lr_scheduler
from project_utils.cluster_utils import mixed_eval, AverageMeter
from models import vision_transformer as vits

from project_utils.general_utils import init_experiment, get_mean_lr, str2bool, get_dino_head_weights

from data.augmentations import get_transform
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import get_single_superclass_datasets, get_superclass_splits, SUPERCLASS_NAMES
from data.data_utils import MergedDataset
from copy import deepcopy

from tqdm import tqdm

from project_utils.cluster_and_log_utils import log_accs_from_preds
from config import exp_root, dino_pretrain_path

# ä»åŸå§‹è®­ç»ƒè„šæœ¬å¯¼å…¥å¿…è¦çš„ç±»å’Œå‡½æ•°
from methods.contrastive_training.contrastive_training import (
    SupConLoss, ContrastiveLearningViewGenerator, info_nce_logits
)

# å¯¼å…¥è®­ç»ƒå·¥å…·
from utils.training_utils import TrainingSession
from utils.checkpoint_utils import save_training_state, load_training_state
from utils.pseudo_labels import load_pseudo_label_cache

# å¯¼å…¥è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
from project_utils.superclass_model_saver import create_superclass_model_saver
from ssddbc.grid_search.api import run_clustering_search_on_features

# TODO: Debug
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


def get_gamma(epoch: int, warmup_epochs: int = 50, total_epochs: int = 200) -> float:
    """
    ä¼ªæ ‡ç­¾æƒé‡è°ƒåº¦å‡½æ•° Î³(epoch)ï¼Œå¯¹åº” TODO.md ä¸­çš„çº¿æ€§è°ƒåº¦ç­–ç•¥ã€‚

    - epoch < warmup_epochs: Î³ = 0.0
    - warmup_epochs <= epoch <= total_epochs: Î³ ä» 0.0 çº¿æ€§ä¸Šå‡åˆ° 1.0
    - epoch > total_epochs: Î³ å›ºå®šä¸º 1.0
    """
    if epoch < warmup_epochs:
        return 0.0
    remaining_epochs = max(total_epochs - warmup_epochs, 1)
    progress = (epoch - warmup_epochs) / remaining_epochs
    progress = max(0.0, min(1.0, progress))
    return float(progress)


def train_superclass(projection_head, model, train_loader, test_loader, unlabelled_train_loader, args,
                     pseudo_cache=None, model_saver=None, progress_parent=None):
    """
    è¶…ç±»è®­ç»ƒå‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„trainå‡½æ•°ä¿®æ”¹

    Returns:
        tuple: (model, projection_head, best_all_acc_test)
    """
    optimizer = SGD(list(projection_head.parameters()) + list(model.parameters()), lr=args.lr, momentum=args.momentum,
                    weight_decay=args.weight_decay)

    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=args.epochs,
            eta_min=args.lr * 1e-3,
        )

    sup_con_crit = SupConLoss()
    # ä½¿ç”¨all_acc_testè¿½è¸ªæœ€ä½³è¡¨ç°ï¼Œä¾›æ¨¡å‹ä¿å­˜å’Œè¿”å›å€¼ä½¿ç”¨
    best_test_acc_lab = 0

    is_grid_search = getattr(args, 'is_grid_search', False)

    # åˆå§‹åŒ–è®­ç»ƒä¼šè¯ç®¡ç†å™¨
    training_session = TrainingSession(args, enable_early_stopping=True, patience=20, quiet=is_grid_search)
    model_info = {
        'name': args.base_model,
        'feat_dim': getattr(args, 'feat_dim', 'Unknown')
    }
    training_session.start_training(model_info)
    setattr(args, "pseudo_cache", pseudo_cache)

    epoch_progress = None
    if is_grid_search and progress_parent is not None:
        epoch_progress = tqdm(
            total=args.epochs,
            desc=f"[{args.superclass_name}] è®­ç»ƒ",
            position=1,
            leave=False,
            dynamic_ncols=True
        )

    # åˆå§‹åŒ–è¶…ç±»æ¨¡å‹ä¿å­˜å™¨ï¼ˆä¼ å…¥argsä»¥æ”¯æŒè¶…å‚æ•°è®°å½•ï¼‰
    if model_saver is None:
        model_saver = create_superclass_model_saver(args.superclass_name, args=args)
    if not is_grid_search:
        print(f"ğŸ—‚ï¸  è¶…ç±»æ¨¡å‹ä¿å­˜å™¨å·²åˆå§‹åŒ–ï¼Œä¿å­˜ç›®å½•: {model_saver.save_dir}")

    # æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if getattr(args, "resume_from_ckpt", None):
        if not is_grid_search:
            print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume_from_ckpt}")
        loaded_epoch, extra_state = load_training_state(
            args.resume_from_ckpt,
            model=model,
            projection_head=projection_head,
            optimizer=optimizer,
            scheduler=exp_lr_scheduler,
            training_session=training_session,
        )
        start_epoch = loaded_epoch + 1
        if not is_grid_search:
            print(f"   æ¢å¤åˆ°ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ")

    if pseudo_cache and getattr(args, "writer", None):
        args.writer.add_scalar('Pseudo/file_core_ratio', pseudo_cache.core_ratio, start_epoch)
        args.writer.add_scalar('Pseudo/file_num_core', pseudo_cache.num_core, start_epoch)
        train_stats = getattr(pseudo_cache, "training_stats", None)
        if train_stats:
            args.writer.add_scalar('Pseudo/train_core_ratio', train_stats.get('core_ratio', 0.0), start_epoch)
            args.writer.add_scalar('Pseudo/train_core_count', train_stats.get('core_count', 0), start_epoch)

    for epoch in range(start_epoch, args.epochs):
        if epoch_progress:
            epoch_progress.set_description(f"[{args.superclass_name}] Epoch {epoch+1}/{args.epochs}")

        # å¼€å§‹æ–°è½®æ¬¡
        training_session.start_epoch(epoch)

        loss_record = AverageMeter()
        contrastive_loss_record = AverageMeter()
        sup_con_loss_record = AverageMeter()
        pseudo_loss_record = AverageMeter()
        pseudo_core_record = AverageMeter()
        train_acc_record = AverageMeter()

        # å®šä¹‰è½®æ¬¡çº§åˆ«çš„æƒé‡å˜é‡ï¼Œé¿å…æ‰¹æ¬¡å†…é‡å¤å®šä¹‰
        epoch_contrastive_weight = 1 - args.sup_con_weight
        epoch_sup_con_weight = args.sup_con_weight

        projection_head.train()
        model.train()

        if not is_grid_search:
            train_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')
        else:
            train_iter = train_loader

        for batch_idx, batch in enumerate(train_iter):

            images, class_labels, uq_idxs, mask_lab = batch
            mask_lab = mask_lab[:, 0]

            class_labels, mask_lab = class_labels.to(args.device), mask_lab.to(args.device).bool()
            images = torch.cat(images, dim=0).to(args.device)

            # Extract features with base model
            features = model(images)

            # Pass features through projection head
            features = projection_head(features)

            # L2-normalize features
            features = torch.nn.functional.normalize(features, dim=-1)

            # Choose which instances to run the contrastive loss on
            if args.contrast_unlabel_only:
                # Contrastive loss only on unlabelled instances
                f1, f2 = [f[~mask_lab] for f in features.chunk(2)]
                con_feats = torch.cat([f1, f2], dim=0)
            else:
                # Contrastive loss for all examples
                con_feats = features

            contrastive_logits, contrastive_labels = info_nce_logits(features=con_feats, args=args)
            contrastive_loss = torch.nn.CrossEntropyLoss()(contrastive_logits, contrastive_labels)

            # Supervised contrastive loss
            f1, f2 = [f[mask_lab] for f in features.chunk(2)]
            if f1.size(0) > 0:  # åªæœ‰å½“æœ‰æ ‡è®°æ ·æœ¬æ—¶æ‰è®¡ç®—ç›‘ç£å¯¹æ¯”æŸå¤±
                sup_con_feats = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
                sup_con_labels = class_labels[mask_lab]
                sup_con_loss = sup_con_crit(sup_con_feats, labels=sup_con_labels)
            else:
                sup_con_loss = torch.tensor(0.0).to(args.device)

            # ä¼ªæ ‡ç­¾å¯¹æ¯”æŸå¤±
            pseudo_loss = torch.tensor(0.0, device=args.device)
            pseudo_core_count = 0
            if pseudo_cache is not None:
                if isinstance(uq_idxs, torch.Tensor):
                    batch_indices_np = uq_idxs.detach().cpu().numpy().astype(np.int64)
                else:
                    batch_indices_np = np.asarray(uq_idxs, dtype=np.int64)
                pseudo_labels_np = pseudo_cache.lookup_labels(batch_indices_np)
                pseudo_core_mask_np = pseudo_cache.lookup_core_mask(batch_indices_np)
                if pseudo_core_mask_np.any():
                    pseudo_labels_tensor = torch.from_numpy(
                        pseudo_labels_np[pseudo_core_mask_np]
                    ).long().to(args.device)
                    core_mask_tensor = torch.from_numpy(pseudo_core_mask_np).to(args.device)
                    f1_pseudo, f2_pseudo = [f[core_mask_tensor] for f in features.chunk(2)]
                    if f1_pseudo.size(0) > 0:
                        pseudo_feats = torch.cat([f1_pseudo.unsqueeze(1), f2_pseudo.unsqueeze(1)], dim=1)
                        pseudo_loss = sup_con_crit(pseudo_feats, labels=pseudo_labels_tensor)
                        pseudo_core_count = int(pseudo_labels_tensor.size(0))

            gamma = get_gamma(epoch, warmup_epochs=50, total_epochs=args.epochs)

            # æ€»æŸå¤± = æ— ç›‘ç£å¯¹æ¯”æŸå¤± + ç›‘ç£å¯¹æ¯”æŸå¤± + Î³Â·ä¼ªæ ‡ç­¾æŸå¤±
            loss = epoch_contrastive_weight * contrastive_loss + epoch_sup_con_weight * sup_con_loss
            if pseudo_cache is not None and pseudo_core_count > 0:
                loss = loss + gamma * pseudo_loss

            # Train acc
            _, pred = contrastive_logits.max(1)
            acc = (pred == contrastive_labels).float().mean().item()
            train_acc_record.update(acc, pred.size(0))

            # è®°å½•å„ç§æŸå¤±
            loss_record.update(loss.item(), class_labels.size(0))
            contrastive_loss_record.update(contrastive_loss.item(), class_labels.size(0))
            sup_con_loss_record.update(sup_con_loss.item(), class_labels.size(0))
            if pseudo_cache is not None:
                pseudo_loss_record.update(pseudo_loss.item(), max(pseudo_core_count, 1))
                pseudo_core_record.update(pseudo_core_count, 1)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡å®æ—¶æŸå¤±
            if batch_idx % 50 == 0 and batch_idx > 0 and not is_grid_search:
                print(f"   Batch {batch_idx}: loss = {epoch_contrastive_weight:.2f}*{contrastive_loss.item():.4f} + {epoch_sup_con_weight:.2f}*{sup_con_loss.item():.4f} = {loss.item():.4f}")

        with torch.no_grad():
            if not is_grid_search:
                print('ğŸ” Testing on unlabelled examples in the training data...')
            all_acc, old_acc, new_acc = test_kmeans_superclass(model, unlabelled_train_loader,
                                                        epoch=epoch, save_name='Train ACC Unlabelled',
                                                        args=args)

            if not is_grid_search:
                print('ğŸ” Testing on disjoint test set...')
            all_acc_test, old_acc_test, new_acc_test = test_kmeans_superclass(model, test_loader,
                                                                   epoch=epoch, save_name='Test ACC',
                                                                   args=args)

        # ----------------
        # è¾“å‡ºè¯¦ç»†æŸå¤±åˆ†è§£
        # ----------------
        lr_value = get_mean_lr(optimizer)

        if not is_grid_search:
            print(f"\nğŸ“Š Epoch {epoch+1}/{args.epochs} æŸå¤±åˆ†è§£:")
            print(f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*å¯¹æ¯”æŸå¤± + {epoch_sup_con_weight:.2f}*ç›‘ç£å¯¹æ¯”æŸå¤± + Î³*ä¼ªæ ‡ç­¾æŸå¤±")
            print(
                f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*{contrastive_loss_record.avg:.4f} "
                f"+ {epoch_sup_con_weight:.2f}*{sup_con_loss_record.avg:.4f} "
                f"+ {gamma:.2f}*{pseudo_loss_record.avg:.4f} = {loss_record.avg:.4f}"
            )
            print(f"   å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrastive_loss_record.avg:.4f}")
            print(f"   ç›‘ç£å¯¹æ¯”æŸå¤±: {sup_con_loss_record.avg:.4f}")
            if pseudo_cache is not None:
                print(
                    f"   ä¼ªæ ‡ç­¾æŸå¤±: {pseudo_loss_record.avg:.4f} "
                    f"(æ¯æ‰¹æ ¸å¿ƒç‚¹â‰ˆ{pseudo_core_record.avg:.1f})"
                )
            print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_acc_record.avg:.4f}")
            print(f"   å­¦ä¹ ç‡: {lr_value:.6f}")
        elif epoch_progress:
            epoch_progress.update(1)
            epoch_progress.set_postfix({
                'Loss': f"{loss_record.avg:.4f}",
                'TrainAcc': f"{train_acc_record.avg:.4f}",
                'TestAll': f"{all_acc_test:.4f}",
                'Best': f"{best_test_acc_lab:.4f}"
            })

        # ----------------
        # LOG & HOOKS
        # ----------------
        # 1) è®°å½•å½“å‰æŸå¤±ä¸å­¦ä¹ ç‡
        args.writer.add_scalar('Loss/Total', loss_record.avg, epoch)
        args.writer.add_scalar('Loss/Contrastive', contrastive_loss_record.avg, epoch)
        args.writer.add_scalar('Loss/SupCon', sup_con_loss_record.avg, epoch)
        args.writer.add_scalar('Train Acc Labelled Data', train_acc_record.avg, epoch)
        args.writer.add_scalar('LR', get_mean_lr(optimizer), epoch)

        # 2) è®¡ç®—å¹¶è®°å½•å½“å‰ epoch çš„ Î³ï¼ˆä¼ªæ ‡ç­¾æŸå¤±æƒé‡ï¼Œå ä½ï¼‰
        gamma = get_gamma(epoch, warmup_epochs=50, total_epochs=args.epochs)
        args.writer.add_scalar('Pseudo/gamma', gamma, epoch)
        if pseudo_cache is not None:
            args.writer.add_scalar('Loss/Pseudo', pseudo_loss_record.avg, epoch)
            args.writer.add_scalar('Pseudo/core_per_batch', pseudo_core_record.avg, epoch)
        if not is_grid_search:
            print(f"   ä¼ªæ ‡ç­¾æƒé‡ Î³(epoch={epoch}) = {gamma:.4f}")

        # 3) ä¼ªæ ‡ç­¾åˆ·æ–°é’©å­ï¼ˆé˜¶æ®µ2ï¼šå®é™…è¿è¡Œå†…å­˜çº§èšç±»æœç´¢ï¼Œä½†å°šæœªåŠ å…¥æŸå¤±ï¼‰
        #    è¿™é‡Œæ¯éš”10è½®ï¼ˆä»ç¬¬50è½®å¼€å§‹ï¼‰åœ¨å½“å‰æ¨¡å‹ç‰¹å¾ä¸Šæ‰§è¡Œä¸€æ¬¡ç½‘æ ¼æœç´¢ï¼Œ
        #    ä»…ç”¨äºæ—¥å¿—ä¸è¯„ä¼°ï¼Œä¸ä¿®æ”¹è®­ç»ƒè¿‡ç¨‹æˆ–æŸå¤±å‡½æ•°ã€‚
        if epoch >= 50 and (epoch - 50) % 10 == 0:
            if not is_grid_search:
                print("   [PSEUDO] åˆ·æ–°ä¼ªæ ‡ç­¾ï¼ˆä»…èšç±»æœç´¢å’Œæ—¥å¿—ï¼Œä¸å‚ä¸å½“å‰è½®æŸå¤±è®¡ç®—ï¼‰...")

            # æ”¶é›†å½“å‰æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„ç‰¹å¾ä¸æ ‡ç­¾ï¼ˆæ— æ¢¯åº¦ï¼‰
            all_feats = []
            all_targets = []
            all_known = []
            all_labeled = []

            # æš‚å­˜å½“å‰æ¨¡å¼
            prev_model_training = model.training
            prev_head_training = projection_head.training

            model.eval()
            projection_head.eval()

            with torch.no_grad():
                for batch in train_loader:
                    images, class_labels, uq_idxs, mask_lab = batch
                    # mask_lab: [B, 1] -> [B]
                    mask_lab = mask_lab[:, 0].bool()

                    class_labels = class_labels.to(args.device)
                    # ContrastiveLearningViewGenerator äº§ç”Ÿçš„ä¸¤è§†å›¾åœ¨ images åˆ—è¡¨ä¸­
                    images_cat = torch.cat(images, dim=0).to(args.device)  # (2B, C, H, W)

                    feats = model(images_cat)
                    feats = projection_head(feats)
                    feats = torch.nn.functional.normalize(feats, dim=-1)

                    # ä»…ä½¿ç”¨ç¬¬ä¸€è§†å›¾çš„ç‰¹å¾è¿›è¡Œèšç±»æœç´¢ï¼Œä¿è¯ä¸ class_labels/mask_lab ä¸€ä¸€å¯¹åº”
                    try:
                        view1_feats, _ = feats.chunk(2, dim=0)  # (B, feat_dim)
                    except RuntimeError:
                        # æå°‘æƒ…å†µä¸‹ batch å°ºå¯¸ä¸åŒ¹é…æ—¶è·³è¿‡è¯¥ batch
                        continue

                    all_feats.append(view1_feats.cpu().numpy())
                    all_targets.append(class_labels.cpu().numpy())
                    # åœ¨å½“å‰å®ç°ä¸­ï¼Œå°†æœ‰æ ‡ç­¾æ ·æœ¬è§†ä¸ºå·²çŸ¥ç±»
                    all_known.append(mask_lab.cpu().numpy())
                    all_labeled.append(mask_lab.cpu().numpy())

            # æ¢å¤è®­ç»ƒ/è¯„ä¼°æ¨¡å¼
            model.train(prev_model_training)
            projection_head.train(prev_head_training)

            features_np = np.concatenate(all_feats, axis=0)
            targets_np = np.concatenate(all_targets, axis=0)
            known_np = np.concatenate(all_known, axis=0)
            labeled_np = np.concatenate(all_labeled, axis=0)

            # é€‰æ‹©è¾ƒå°çš„æœç´¢ç½‘æ ¼ï¼Œé¿å…åœ¨è®­ç»ƒä¸­å¼•å…¥è¿‡å¤§å¼€é”€
            k_range = range(3, 8)               # k = 3,4,5,6,7
            density_range = range(40, 80, 10)   # dp = 40,50,60,70

            pseudo_result = run_clustering_search_on_features(
                features=features_np,
                targets=targets_np,
                known_mask=known_np,
                labeled_mask=labeled_np,
                k_range=k_range,
                density_range=density_range,
                random_state=0,
                silent=True,
            )

            core_ratio = (
                len(pseudo_result.core_points) / len(pseudo_result.labels)
                if len(pseudo_result.labels) > 0
                else 0.0
            )

            if not is_grid_search:
                print(
                    f"   [PSEUDO] èšç±»æœç´¢å®Œæˆ: best_params={pseudo_result.best_params}, "
                    f"All={pseudo_result.all_acc:.4f}, Old={pseudo_result.old_acc:.4f}, "
                    f"New={pseudo_result.new_acc:.4f}, "
                    f"core={len(pseudo_result.core_points)} ({core_ratio*100:.1f}%)"
                )

            # å°†ä¼ªæ ‡ç­¾æœç´¢çš„æŒ‡æ ‡å†™å…¥ TensorBoardï¼Œä¾¿äºåç»­åˆ†æ
            args.writer.add_scalar('Pseudo/all_acc', pseudo_result.all_acc, epoch)
            args.writer.add_scalar('Pseudo/old_acc', pseudo_result.old_acc, epoch)
            args.writer.add_scalar('Pseudo/new_acc', pseudo_result.new_acc, epoch)
            args.writer.add_scalar('Pseudo/core_ratio', core_ratio, epoch)

        # ä½¿ç”¨è®­ç»ƒä¼šè¯ç®¡ç†å™¨å¤„ç†è½®æ¬¡ç»“æŸ
        should_stop = training_session.end_epoch(
            epoch=epoch,
            train_acc=train_acc_record.avg,
            loss_avg=loss_record.avg,
            all_acc=all_acc,
            old_acc=old_acc,
            new_acc=new_acc,
            all_acc_test=all_acc_test,
            old_acc_test=old_acc_test,
            new_acc_test=new_acc_test
        )

        # Step schedule
        exp_lr_scheduler.step()

        # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨æ–°çš„è¶…ç±»æ¨¡å‹ä¿å­˜æœºåˆ¶
        # ä½¿ç”¨è¶…ç±»æ¨¡å‹ä¿å­˜å™¨ä¿å­˜æœ€ä½³æ¨¡å‹ (ä»…ä¿å­˜æœ€ä½³ï¼Œä¸å†ä¿å­˜æ¯è½®æ¨¡å‹)
        if all_acc_test > best_test_acc_lab:
            # ä½¿ç”¨æ–°çš„ä¿å­˜æœºåˆ¶ï¼Œä¼šè‡ªåŠ¨ç®¡ç†æ–‡ä»¶å‘½åã€åˆ é™¤æ—§æ¨¡å‹ã€ç”Ÿæˆè¶…å‚æ•°è®°å½•
            best_model_path, best_proj_path = model_saver.save_best_model(
                model=model,
                projection_head=projection_head,
                acc=all_acc_test,
                current_epoch=epoch + 1,  # ä¼ å…¥å½“å‰è½®æ•°ä»¥è®°å½•è®­ç»ƒè¿›åº¦
                metadata={
                    'epoch': epoch + 1,
                    'train_loss': loss_record.avg,
                    'all_acc_test': all_acc_test,
                    'old_acc_test': old_acc_test,
                    'new_acc_test': new_acc_test,
                    'train_acc': train_acc_record.avg
                }
            )
            best_test_acc_lab = all_acc_test

        # å¯é€‰ï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€æ£€æŸ¥ç‚¹ï¼Œä¾¿äºåç»­éªŒè¯æ–­ç‚¹ç»­è®­
        if getattr(args, "save_ckpt_every", 0) and (epoch + 1) % args.save_ckpt_every == 0:
            ckpt_dir = os.path.join(args.exp_root, "checkpoints", args.superclass_name)
            ckpt_name = f"ckpt_epoch_{epoch+1:03d}.pt"
            ckpt_path = os.path.join(ckpt_dir, ckpt_name)
            extra_state = {
                "superclass_name": getattr(args, "superclass_name", None),
            }
            if not is_grid_search:
                print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹: {ckpt_path}")
            save_training_state(
                ckpt_path,
                epoch=epoch,
                model=model,
                projection_head=projection_head,
                optimizer=optimizer,
                scheduler=exp_lr_scheduler,
                training_session=training_session,
                extra_state=extra_state,
            )

        # æ£€æŸ¥æ—©åœ
        if should_stop:
            if not is_grid_search:
                print(f"\nğŸ›‘ è¶…ç±» '{args.superclass_name}' æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
            training_session.finish_training(epoch, early_stopped=True)
            break

    else:
        # æ­£å¸¸å®Œæˆæ‰€æœ‰è½®æ¬¡
        training_session.finish_training(args.epochs - 1, early_stopped=False)

    if epoch_progress:
        epoch_progress.close()

    # è®­ç»ƒç»“æŸï¼Œæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    if not is_grid_search:
        print(f"\nğŸ“Š è¶…ç±» '{args.superclass_name}' è®­ç»ƒå®Œæˆ")
        best_model_info = model_saver.get_best_model_info()
        print(f"ğŸ† æœ€ä½³æ¨¡å‹ä¿¡æ¯:")
        print(f"   æœ€ä½³ACC: {best_model_info['best_acc']:.4f}")
        if best_model_info['model_path']:
            print(f"   æ¨¡å‹æ–‡ä»¶: {os.path.basename(best_model_info['model_path'])}")
            print(f"   æŠ•å½±å¤´: {os.path.basename(best_model_info['proj_path'])}")
            print(f"   ä¿å­˜ç›®å½•: {best_model_info['save_dir']}")
        else:
            print("   æœªä¿å­˜ä»»ä½•æœ€ä½³æ¨¡å‹ (å¯èƒ½æœªè¾¾åˆ°ä¿å­˜æ¡ä»¶)")

    return model, projection_head, best_test_acc_lab

def test_kmeans_superclass(model, test_loader, epoch, save_name, args):
    """
    è¶…ç±»K-meansæµ‹è¯•å‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„test_kmeansä¿®æ”¹
    """
    model.eval()

    all_feats = []
    targets = np.array([])
    mask = np.array([])

    is_grid_search = getattr(args, 'is_grid_search', False)

    if not is_grid_search:
        print('Collating features...')
        test_iter = tqdm(test_loader, desc='Collating features')
    else:
        test_iter = test_loader

    # First extract all features
    for batch_idx, (images, label, _) in enumerate(test_iter):

        images = images.to(args.device)

        # Pass features through base model only (no projection head for evaluation)
        feats = model(images)
        feats = torch.nn.functional.normalize(feats, dim=-1)

        all_feats.append(feats.cpu().numpy())
        targets = np.append(targets, label.cpu().numpy())
        mask = np.append(mask, np.array([True if x.item() in range(len(args.train_classes))
                                         else False for x in label]))

    # -----------------------
    # K-MEANS
    # -----------------------
    if not is_grid_search:
        print('Fitting K-Means...')
    all_feats = np.concatenate(all_feats)
    kmeans = KMeans(n_clusters=args.num_labeled_classes + args.num_unlabeled_classes, random_state=0, n_init=10).fit(all_feats)
    preds = kmeans.labels_
    if not is_grid_search:
        print('Done!')

    # -----------------------
    # EVALUATE
    # -----------------------
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥ç±»
    mask = np.array(mask, dtype=bool)  # ç¡®ä¿maskæ˜¯numpyæ•°ç»„
    has_unknown_classes = args.num_unlabeled_classes > 0 and (~mask).sum() > 0

    if has_unknown_classes:
        # æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šæ­£å¸¸è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        all_acc, old_acc, new_acc = log_accs_from_preds(y_true=targets, y_pred=preds, mask=mask,
                                                        T=epoch, eval_funcs=args.eval_funcs, save_name=save_name,
                                                        writer=args.writer)
    else:
        # æ²¡æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šåªè®¡ç®—å·²çŸ¥ç±»å‡†ç¡®ç‡
        from project_utils.cluster_utils import cluster_acc
        old_acc = cluster_acc(targets, preds)
        all_acc = old_acc  # å½“æ²¡æœ‰æœªçŸ¥ç±»æ—¶ï¼ŒAll ACC = Old ACC
        new_acc = 0.0      # æ²¡æœ‰æœªçŸ¥ç±»ï¼ŒNew ACCä¸º0

        if not is_grid_search:
            print(f"âš ï¸  æ³¨æ„: è¶…ç±» '{args.superclass_name}' ä¸­æ²¡æœ‰æœªçŸ¥ç±»æ ·æœ¬ï¼Œä»…è®¡ç®—Old ACC")

        # å†™å…¥TensorBoardæ—¥å¿—
        if args.writer is not None:
            args.writer.add_scalars(f'{save_name}_v1',
                                   {'Old': old_acc, 'New': new_acc, 'All': all_acc}, epoch)

    return all_acc, old_acc, new_acc



def train_all_superclasses(args):
    """
    è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»
    """
    print("ğŸŒŸ å¼€å§‹è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
    print("=" * 80)

    superclass_splits = get_superclass_splits()

    for superclass_name in SUPERCLASS_NAMES:
        print(f"\nğŸ¯ å‡†å¤‡è®­ç»ƒè¶…ç±»: {superclass_name}")

        # æ£€æŸ¥è¯¥è¶…ç±»æ˜¯å¦æœ‰è¶³å¤Ÿçš„å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»
        split_info = superclass_splits[superclass_name]
        if len(split_info['known_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰å·²çŸ¥ç±»")
            continue
        if len(split_info['unknown_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰æœªçŸ¥ç±»")
            continue

        # è®¾ç½®å½“å‰è¶…ç±»çš„å‚æ•°
        current_args = argparse.Namespace(**vars(args))
        current_args.superclass_name = superclass_name
        current_args.dataset_name = 'cifar100_superclass'

        # è·å–ç±»åˆ«åˆ’åˆ†
        current_args = get_class_splits(current_args)
        current_args.num_labeled_classes = len(current_args.train_classes)
        current_args.num_unlabeled_classes = len(current_args.unlabeled_classes)

        # åˆ›å»ºå®éªŒç›®å½•
        exp_name = f'superclass_{superclass_name}_{current_args.model_name}'
        current_args.exp_name = exp_name
        init_experiment(current_args, runner_name=['superclass_train'])

        try:
            # è®­ç»ƒå•ä¸ªè¶…ç±»
            train_single_superclass(current_args)
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¶…ç±» '{superclass_name}' æ—¶å‡ºé”™: {e}")
            continue

    print("\nğŸ‰ æ‰€æœ‰è¶…ç±»è®­ç»ƒå®Œæˆ!")



def train_single_superclass(args, model_saver=None, progress_parent=None):
    """
    è®­ç»ƒå•ä¸ªè¶…ç±»

    Args:
        args: è®­ç»ƒé…ç½®
        model_saver: å¯é€‰ï¼Œè¦†ç›–é»˜è®¤çš„è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
    """
    device = args.device
    is_grid_search = getattr(args, 'is_grid_search', False)
    pseudo_cache = None
    pseudo_path = getattr(args, "pseudo_labels_path", None)
    if pseudo_path:
        pseudo_path = os.path.abspath(pseudo_path)
        if not os.path.isfile(pseudo_path):
            raise FileNotFoundError(f"ä¼ªæ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {pseudo_path}")
        pseudo_cache = load_pseudo_label_cache(pseudo_path)
        file_superclass = pseudo_cache.packet.metadata.get("superclass")
        if file_superclass and file_superclass != args.superclass_name:
            raise ValueError(
                f"ä¼ªæ ‡ç­¾æ–‡ä»¶å±äºè¶…ç±» '{file_superclass}'ï¼Œä¸å½“å‰ '{args.superclass_name}' ä¸åŒ¹é…"
            )
        if not is_grid_search:
            print("ğŸ“¥ è½½å…¥ä¼ªæ ‡ç­¾:")
            print(f"   æ–‡ä»¶: {pseudo_path}")
            print(
                f"   æ ·æœ¬æ€»æ•°: {pseudo_cache.num_total}, æ ¸å¿ƒç‚¹: {pseudo_cache.num_core} "
                f"({pseudo_cache.core_ratio * 100:.1f}%)"
            )
            if pseudo_cache.packet.metadata:
                meta_ckpt = pseudo_cache.packet.metadata.get("ckpt_path")
                if meta_ckpt:
                    print(f"   æ¥æº ckpt: {meta_ckpt}")

    # ----------------------
    # BASE MODEL
    # ----------------------
    if args.base_model == 'vit_dino':

        args.interpolation = 3
        args.crop_pct = 0.875
        pretrain_path = dino_pretrain_path

        model = vits.__dict__['vit_base']()

        state_dict = torch.load(pretrain_path, map_location='cpu')
        model.load_state_dict(state_dict)

        if args.warmup_model_dir is not None:
            if not getattr(args, 'is_grid_search', False):
                print(f'Loading weights from {args.warmup_model_dir}')
            model.load_state_dict(torch.load(args.warmup_model_dir, map_location='cpu'))

        model.to(device)

        # NOTE: Hardcoded image size as we do not finetune the entire ViT model
        args.image_size = 224
        args.feat_dim = 768
        args.num_mlp_layers = 3
        args.mlp_out_dim = 65536

        # ----------------------
        # HOW MUCH OF BASE MODEL TO FINETUNE
        # ----------------------
        for m in model.parameters():
            m.requires_grad = False

        # Only finetune layers from block 'args.grad_from_block' onwards
        for name, m in model.named_parameters():
            if 'block' in name:
                block_num = int(name.split('.')[1])
                if block_num >= args.grad_from_block:
                    m.requires_grad = True

    else:
        raise NotImplementedError

    # --------------------
    # CONTRASTIVE TRANSFORM
    # --------------------
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)
    train_transform = ContrastiveLearningViewGenerator(base_transform=train_transform, n_views=args.n_views)

    # --------------------
    # DATASETS - ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†
    # --------------------
    # ç›´æ¥ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†è·å–å‡½æ•°
    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed,
        verbose=not is_grid_search
    )

    # Train split (labelled and unlabelled classes) for training
    train_dataset = MergedDataset(labelled_dataset=deepcopy(datasets['train_labelled']),
                                  unlabelled_dataset=deepcopy(datasets['train_unlabelled']))

    test_dataset = datasets['test']
    unlabelled_train_examples_test = deepcopy(datasets['train_unlabelled'])
    unlabelled_train_examples_test.transform = test_transform

    # --------------------
    # SAMPLER
    # Sampler which balances labelled and unlabelled examples in each batch
    # --------------------
    label_len = len(train_dataset.labelled_dataset)
    unlabelled_len = len(train_dataset.unlabelled_dataset)
    sample_weights = [1 if i < label_len else label_len / unlabelled_len for i in range(len(train_dataset))]
    sample_weights = torch.DoubleTensor(sample_weights)
    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(train_dataset))

    # --------------------
    # DATALOADERS
    # --------------------
    train_loader = DataLoader(train_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False,
                              sampler=sampler, drop_last=True)
    test_loader_unlabelled = DataLoader(unlabelled_train_examples_test, num_workers=args.num_workers,
                                        batch_size=args.batch_size, shuffle=False)
    test_loader_labelled = DataLoader(test_dataset, num_workers=args.num_workers,
                                      batch_size=args.batch_size, shuffle=False)

    if pseudo_cache:
        labelled_uq = getattr(train_dataset.labelled_dataset, 'uq_idxs', None)
        unlabelled_uq = getattr(train_dataset.unlabelled_dataset, 'uq_idxs', None)
        if labelled_uq is None or unlabelled_uq is None:
            print("âš ï¸  å½“å‰æ•°æ®é›†ä¸åŒ…å« uq_idxs ä¿¡æ¯ï¼Œæ— æ³•åŒ¹é…ä¼ªæ ‡ç­¾ç´¢å¼•")
        else:
            train_indices = np.concatenate([labelled_uq, unlabelled_uq]).astype(np.int64)
            subset_stats = pseudo_cache.subset_stats(train_indices)
            if subset_stats["missing"] > 0:
                missing_sample = train_indices[pseudo_cache.missing_mask(train_indices)][0]
                raise ValueError(
                    f"ä¼ªæ ‡ç­¾æ–‡ä»¶ç¼ºå°‘ {subset_stats['missing']} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚ç¤ºä¾‹ç´¢å¼•: {missing_sample}"
                )
            pseudo_cache.training_stats = subset_stats
            if not is_grid_search:
                print(
                    f"ğŸ’¡ è®­ç»ƒæ ·æœ¬ä¼ªæ ‡ç­¾è¦†ç›–: {subset_stats['covered']}/{subset_stats['total']} "
                    f"({subset_stats['core_ratio']*100:.1f}% æ ¸å¿ƒç‚¹æ¯”ä¾‹)"
                )

    # ----------------------
    # PROJECTION HEAD
    # ----------------------
    projection_head = vits.__dict__['DINOHead'](in_dim=args.feat_dim,
                               out_dim=args.mlp_out_dim, nlayers=args.num_mlp_layers)
    projection_head.to(device)

    # ----------------------
    # TRAIN
    # ----------------------
    return train_superclass(
        projection_head,
        model,
        train_loader,
        test_loader_labelled,
        test_loader_unlabelled,
        args,
        pseudo_cache=pseudo_cache,
        model_saver=model_saver,
        progress_parent=progress_parent
    )


def build_superclass_train_parser(add_help=True):
    parser = argparse.ArgumentParser(
            description='Train GCD on CIFAR-100 superclasses',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            add_help=add_help)

    # =====================================================================
    # æ•°æ®é›†å’Œè®­ç»ƒåŸºç¡€å‚æ•°
    # =====================================================================
    parser.add_argument('--batch_size', default=128, type=int,
                        help='è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤å€¼: 128ã€‚è¾ƒå¤§çš„batch_sizeå¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œä½†éœ€è¦æ›´å¤šGPUå†…å­˜')
    parser.add_argument('--num_workers', default=8, type=int,
                        help='æ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°ã€‚é»˜è®¤å€¼: 8ã€‚æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼Œè¿‡å¤§å¯èƒ½å¯¼è‡´å†…å­˜å¼€é”€å¢åŠ ')
    parser.add_argument('--eval_funcs', nargs='+', default=['v1', 'v2'],
                        help='ä½¿ç”¨çš„è¯„ä¼°å‡½æ•°åˆ—è¡¨ã€‚é»˜è®¤å€¼: [v1, v2]ã€‚v1ä¸ºæ ‡å‡†èšç±»å‡†ç¡®ç‡ï¼Œv2ä¸ºåŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å‡†ç¡®ç‡')

    # =====================================================================
    # æ¨¡å‹æ¶æ„å‚æ•°
    # =====================================================================
    parser.add_argument('--warmup_model_dir', type=str, default=None,
                        help='é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœæä¾›ï¼Œå°†ä»è¯¥è·¯å¾„åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--model_name', type=str, default='vit_dino',
                        help='æ¨¡å‹åç§°æ ‡è¯†ã€‚é»˜è®¤å€¼: vit_dinoã€‚æ ¼å¼ä¸º{æ¨¡å‹æ¶æ„}_{é¢„è®­ç»ƒæ–¹æ³•}ï¼Œç”¨äºå®éªŒå‘½åå’Œæ—¥å¿—è®°å½•')
    parser.add_argument('--dataset_name', type=str, default='cifar100_superclass',
                        help='æ•°æ®é›†åç§°ã€‚é»˜è®¤å€¼: cifar100_superclassã€‚å›ºå®šä½¿ç”¨CIFAR-100è¶…ç±»æ•°æ®é›†ï¼Œä¸å»ºè®®ä¿®æ”¹')
    parser.add_argument('--prop_train_labels', type=float, default=0.8,
                        help='å·²çŸ¥ç±»æ ·æœ¬ä¸­ç”¨äºè®­ç»ƒçš„æ¯”ä¾‹ã€‚é»˜è®¤å€¼: 0.8ã€‚å‰©ä½™0.2ç”¨ä½œéªŒè¯é›†ï¼Œç¬¦åˆGCDæ ‡å‡†åˆ’åˆ†')

    # =====================================================================
    # è¶…ç±»è®­ç»ƒæ¨¡å¼å‚æ•°
    # =====================================================================
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¦è®­ç»ƒçš„å•ä¸ªè¶…ç±»åç§°ã€‚é»˜è®¤å€¼: Noneã€‚ä¾‹å¦‚: aquatic_mammals, fish, flowersç­‰ã€‚'
                             'å¯ç”¨è¶…ç±»è¯·æŸ¥çœ‹CIFAR-100è¶…ç±»å®šä¹‰')
    parser.add_argument('--train_all_superclasses', action='store_true',
                        help='æ‰¹é‡è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»çš„æ ‡å¿—ã€‚é»˜è®¤å€¼: Falseã€‚å¯ç”¨åå°†ä¾æ¬¡è®­ç»ƒæ‰€æœ‰è¶…ç±»ï¼Œå¿½ç•¥--superclass_nameå‚æ•°')

    # =====================================================================
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡å‚æ•°
    # =====================================================================
    parser.add_argument('--grad_from_block', type=int, default=11,
                        help='ViTæ¨¡å‹ä»ç¬¬å‡ ä¸ªblockå¼€å§‹è®¡ç®—æ¢¯åº¦ã€‚é»˜è®¤å€¼: 11ã€‚ViT-Baseå…±12ä¸ªblocks (0-11)ï¼Œ'
                             'ä»…å¾®è°ƒæœ€åå‡ å±‚å¯ä»¥å‡å°‘è®¡ç®—é‡å¹¶é˜²æ­¢è¿‡æ‹Ÿåˆ')
    parser.add_argument('--lr', type=float, default=0.1,
                        help='åˆå§‹å­¦ä¹ ç‡ã€‚é»˜è®¤å€¼: 0.1ã€‚ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé€æ¸é™ä½è‡³lr*1e-3')
    parser.add_argument('--save_best_thresh', type=float, default=None,
                        help='ä¿å­˜æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡é˜ˆå€¼ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœè®¾ç½®ï¼Œä»…å½“å‡†ç¡®ç‡è¶…è¿‡è¯¥é˜ˆå€¼æ—¶æ‰ä¿å­˜æ¨¡å‹')
    parser.add_argument('--gamma', type=float, default=0.1,
                        help='å­¦ä¹ ç‡è¡°å‡å› å­ã€‚é»˜è®¤å€¼: 0.1ã€‚ç”¨äºStepLRè°ƒåº¦å™¨ï¼ˆå½“å‰ä½¿ç”¨CosineAnnealingï¼Œæ­¤å‚æ•°æš‚æœªä½¿ç”¨ï¼‰')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='SGDä¼˜åŒ–å™¨çš„åŠ¨é‡ç³»æ•°ã€‚é»˜è®¤å€¼: 0.9ã€‚æœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘æŒ¯è¡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ç³»æ•°(L2æ­£åˆ™åŒ–)ã€‚é»˜è®¤å€¼: 1e-4ã€‚é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ')
    parser.add_argument('--epochs', default=20, type=int,
                        help='è®­ç»ƒæ€»è½®æ•°ã€‚é»˜è®¤å€¼: 20ã€‚é…åˆæ—©åœæœºåˆ¶(patience=20)ï¼Œå®é™…è®­ç»ƒå¯èƒ½æå‰ç»“æŸ')

    # =====================================================================
    # è·¯å¾„å’Œå®éªŒé…ç½®å‚æ•°
    # =====================================================================
    parser.add_argument('--exp_root', type=str, default=exp_root,
                        help=f'å®éªŒè¾“å‡ºæ ¹ç›®å½•ã€‚é»˜è®¤å€¼: {exp_root}ã€‚æ‰€æœ‰æ¨¡å‹ã€æ—¥å¿—ã€TensorBoardæ–‡ä»¶å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸‹')
    parser.add_argument('--transform', type=str, default='imagenet',
                        help='æ•°æ®å¢å¼ºæ–¹æ¡ˆåç§°ã€‚é»˜è®¤å€¼: imagenetã€‚ä½¿ç”¨ImageNetæ ‡å‡†çš„æ•°æ®å¢å¼ºç­–ç•¥')
    parser.add_argument('--seed', default=1, type=int,
                        help='éšæœºç§å­ã€‚é»˜è®¤å€¼: 1ã€‚ç”¨äºä¿è¯å®éªŒå¯å¤ç°æ€§')
    parser.add_argument('--gpu', type=int, default=0,
                        help='ä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ã€‚é»˜è®¤å€¼: 0ã€‚å¦‚æœæœ‰å¤šå—GPUï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨å“ªä¸€å—(0, 1, 2...)')

    # =====================================================================
    # è®­ç»ƒæ£€æŸ¥ç‚¹å‚æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ä¸ç¦»çº¿SSDDBCé›†æˆï¼‰
    # =====================================================================
    parser.add_argument('--resume_from_ckpt', type=str, default=None,
                        help='å¯é€‰ï¼šä»æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒï¼ˆä¿å­˜äº†æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œè®­ç»ƒä¼šè¯çŠ¶æ€ï¼‰ã€‚')
    parser.add_argument('--save_ckpt_every', type=int, default=0,
                        help='å¯é€‰ï¼šæ¯éš”å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡å®Œæ•´è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆ0è¡¨ç¤ºä¸è‡ªåŠ¨ä¿å­˜ï¼‰ã€‚')
    parser.add_argument('--pseudo_labels_path', type=str, default=None,
                        help='å¯é€‰ï¼šç¦»çº¿ SSDDBC ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ–‡ä»¶(.npz)è·¯å¾„ã€‚æä¾›åè®­ç»ƒä¼šåŠ è½½ä¼ªæ ‡ç­¾è¿›è¡Œé˜¶æ®µ2/3å®éªŒã€‚')

    # =====================================================================
    # å¯¹æ¯”å­¦ä¹ æ¡†æ¶å‚æ•°
    # =====================================================================
    parser.add_argument('--base_model', type=str, default='vit_dino',
                        help='åŸºç¡€æ¨¡å‹æ¶æ„ã€‚é»˜è®¤å€¼: vit_dinoã€‚å½“å‰ä»…æ”¯æŒvit_dino (ViT-Base with DINOé¢„è®­ç»ƒ)')
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦ç³»æ•°ã€‚é»˜è®¤å€¼: 1.0ã€‚æ§åˆ¶softmaxçš„å¹³æ»‘åº¦ï¼Œè¾ƒå°çš„å€¼ä½¿åˆ†å¸ƒæ›´å°–é”')
    parser.add_argument('--sup_con_weight', type=float, default=0.5,
                        help='ç›‘ç£å¯¹æ¯”æŸå¤±çš„æƒé‡ã€‚é»˜è®¤å€¼: 0.5ã€‚æ€»æŸå¤± = (1-w)*å¯¹æ¯”æŸå¤± + w*ç›‘ç£å¯¹æ¯”æŸå¤±ã€‚'
                             'èŒƒå›´[0,1]ï¼Œ0è¡¨ç¤ºçº¯æ— ç›‘ç£ï¼Œ1è¡¨ç¤ºçº¯ç›‘ç£')
    parser.add_argument('--n_views', default=2, type=int,
                        help='å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡ã€‚é»˜è®¤å€¼: 2ã€‚æ¯ä¸ªæ ·æœ¬ç”Ÿæˆnä¸ªå¢å¼ºè§†å›¾ç”¨äºå¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--contrast_unlabel_only', type=str2bool, default=False,
                        help='æ˜¯å¦ä»…å¯¹æœªæ ‡è®°æ ·æœ¬è®¡ç®—å¯¹æ¯”æŸå¤±ã€‚é»˜è®¤å€¼: Falseã€‚Trueæ—¶å¯¹æ¯”æŸå¤±ä»…ç”¨äºæœªçŸ¥ç±»æ ·æœ¬')

    return parser


def main():
    parser = build_superclass_train_parser()
    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    print("ğŸš€ CIFAR-100è¶…ç±»GCDè®­ç»ƒè„šæœ¬")
    print("=" * 80)
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # å°†deviceæ·»åŠ åˆ°argsä¸­ï¼Œæ–¹ä¾¿å…¶ä»–å‡½æ•°ä½¿ç”¨
    args.device = device

    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    if args.train_all_superclasses:
        print("ğŸŒŸ æ¨¡å¼: è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        train_all_superclasses(args)
    elif args.superclass_name:
        print(f"ğŸ¯ æ¨¡å¼: è®­ç»ƒå•ä¸ªè¶…ç±» '{args.superclass_name}'")

        # éªŒè¯è¶…ç±»åç§°
        if args.superclass_name not in SUPERCLASS_NAMES:
            print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{args.superclass_name}'")
            print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
            sys.exit(1)

        # è·å–ç±»åˆ«åˆ’åˆ†
        args = get_class_splits(args)
        args.num_labeled_classes = len(args.train_classes)
        args.num_unlabeled_classes = len(args.unlabeled_classes)

        # åˆå§‹åŒ–å®éªŒ
        exp_name = f'superclass_{args.superclass_name}_{args.model_name}'
        args.exp_name = exp_name
        init_experiment(args, runner_name=['superclass_train'])
        print(f'Using evaluation function {args.eval_funcs[0]} to print results')

        # è®­ç»ƒ
        train_single_superclass(args)
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€:")
        print("   --superclass_name <åç§°>  : è®­ç»ƒå•ä¸ªè¶…ç±»")
        print("   --train_all_superclasses  : è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
        sys.exit(1)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
