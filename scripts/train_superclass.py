#!/usr/bin/env python3
"""
è¶…ç±»è®­ç»ƒè„šæœ¬ - åŸºäºGCDé¡¹ç›®çš„contrastive_training.py
æ”¯æŒ15ä¸ªè¶…ç±»çš„ç‹¬ç«‹è®­ç»ƒå’Œæ•°æ®åˆ’åˆ†
"""

import argparse
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from sklearn.cluster import KMeans
import torch
from torch.optim import SGD, lr_scheduler
from project_utils.cluster_utils import mixed_eval, AverageMeter
from models import vision_transformer as vits

from project_utils.general_utils import init_experiment, get_mean_lr, str2bool, get_dino_head_weights

from data.augmentations import get_transform
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import get_single_superclass_datasets, get_superclass_splits, SUPERCLASS_NAMES
from data.data_utils import MergedDataset
from copy import deepcopy

from tqdm import tqdm

from project_utils.cluster_and_log_utils import log_accs_from_preds
from config import exp_root, dino_pretrain_path, feature_cache_dir

# ä»åŸå§‹è®­ç»ƒè„šæœ¬å¯¼å…¥å¿…è¦çš„ç±»å’Œå‡½æ•°
from methods.contrastive_training.contrastive_training import (
    SupConLoss, ContrastiveLearningViewGenerator, info_nce_logits
)

# å¯¼å…¥è®­ç»ƒå·¥å…·
from utils.training_utils import TrainingSession
from utils.checkpoint_utils import save_training_state, load_training_state
from utils.pseudo_labels import load_pseudo_label_cache
from utils.best_model_tracker import BestModelTracker

# å¯¼å…¥è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
from project_utils.superclass_model_saver import create_superclass_model_saver
from scripts._feature_cache_runner import run_cache_features

# TODO: Debug
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


from typing import Optional


def get_gamma(epoch: int, warmup_epochs: int = 50, total_epochs: int = 200,
              update_interval: Optional[int] = None, ramp_intervals: int = 2) -> float:
    """
    ä¼ªæ ‡ç­¾æƒé‡è°ƒåº¦å‡½æ•° Î³(epoch)ã€‚

    - epoch < warmup_epochs: Î³ = 0.0
    - è‹¥æä¾› update_intervalï¼Œåˆ™åœ¨ warmup åçš„å‰ ramp_intervals ä¸ªä¼ªæ ‡ç­¾æ›´æ–°é—´éš”å†…çº¿æ€§å‡è‡³ 1.0ï¼Œ
      ä¹‹åä¿æŒ 1.0ï¼ˆé»˜è®¤åœ¨å‰ä¸¤ä¸ªé—´éš”å†…å®Œæˆå¢é•¿ï¼‰ã€‚
    - è‹¥æœªæä¾› update_intervalï¼Œåˆ™é€€å›åŸæœ‰çš„çº¿æ€§è°ƒåº¦ï¼šwarmupâ†’total_epochs æœŸé—´å‡è‡³ 1.0ã€‚
    """
    if epoch < warmup_epochs:
        return 0.0

    if update_interval is not None and update_interval > 0 and ramp_intervals > 0:
        ramp_total = max(ramp_intervals * update_interval, 1)
        progress = (epoch - warmup_epochs + 1) / ramp_total
        progress = max(0.0, min(1.0, progress))
        return float(progress)

    # å…¼å®¹è€çš„çº¿æ€§ç­–ç•¥
    remaining_epochs = max(total_epochs - warmup_epochs, 1)
    progress = (epoch - warmup_epochs) / remaining_epochs
    progress = max(0.0, min(1.0, progress))
    return float(progress)


def train_superclass(projection_head, model, train_loader, test_loader, unlabelled_train_loader, args,
                     pseudo_cache=None, model_saver=None, progress_parent=None, feature_cache_args=None):
    """
    è¶…ç±»è®­ç»ƒå‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„trainå‡½æ•°ä¿®æ”¹

    Returns:
        tuple: (model, projection_head, best_all_acc_test)
    """
    optimizer = SGD(list(projection_head.parameters()) + list(model.parameters()), lr=args.lr, momentum=args.momentum,
                    weight_decay=args.weight_decay)

    exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=args.epochs,
            eta_min=args.lr * 1e-3,
        )

    sup_con_crit = SupConLoss()

    # åˆå§‹åŒ–æœ€ä½³æ¨¡å‹è¿½è¸ªå™¨ï¼ˆä½¿ç”¨ JSON æ–‡ä»¶æŒä¹…åŒ–å…¨å±€æœ€ä½³ä¿¡æ¯ï¼‰
    best_model_tracker = BestModelTracker(args.exp_root)
    best_test_acc_lab = best_model_tracker.get_best_acc()

    is_grid_search = getattr(args, 'is_grid_search', False)

    if not is_grid_search and best_test_acc_lab > 0:
        print(f"ğŸ“Š ä» JSON æ¢å¤å…¨å±€æœ€ä½³ ACC: {best_test_acc_lab:.4f} (epoch {best_model_tracker.get_best_epoch()})")

    # åˆå§‹åŒ–è®­ç»ƒä¼šè¯ç®¡ç†å™¨
    training_session = TrainingSession(args, enable_early_stopping=False, patience=20, quiet=is_grid_search)
    model_info = {
        'name': args.base_model,
        'feat_dim': getattr(args, 'feat_dim', 'Unknown')
    }
    training_session.start_training(model_info)
    setattr(args, "pseudo_cache", pseudo_cache)
    pseudo_weight_mode = getattr(args, "pseudo_weight_mode", "none")
    valid_weight_modes = {"none", "density", "inverse_density"}
    if pseudo_weight_mode not in valid_weight_modes:
        raise ValueError(f"ä¸æ”¯æŒçš„ pseudo_weight_mode: {pseudo_weight_mode}")
    pseudo_for_labeled_mode = getattr(args, "pseudo_for_labeled_mode", "off")
    if pseudo_for_labeled_mode not in {"off", "all"}:
        raise ValueError(f"ä¸æ”¯æŒçš„ pseudo_for_labeled_mode: {pseudo_for_labeled_mode}")
    effective_weight_mode = pseudo_weight_mode
    if pseudo_cache is None:
        if pseudo_weight_mode != "none" and not is_grid_search:
            print("âš ï¸  æœªæä¾›ä¼ªæ ‡ç­¾æ–‡ä»¶ï¼Œå¿½ç•¥ pseudo_weight_mode è®¾ç½®")
        effective_weight_mode = "none"
    elif pseudo_weight_mode in {"density", "inverse_density"} and not pseudo_cache.has_density_weights:
        if not is_grid_search:
            print("âš ï¸  ä¼ªæ ‡ç­¾æ–‡ä»¶ç¼ºå°‘å¯†åº¦ä¿¡æ¯ï¼Œæ— æ³•æŒ‰å¯†åº¦ç›¸å…³åŠ æƒï¼Œé€€å›å‡åŒ€æƒé‡")
        effective_weight_mode = "none"
    args.pseudo_weight_mode_effective = effective_weight_mode
    if pseudo_cache is not None and not is_grid_search:
        if effective_weight_mode == "density":
            mode_desc = "æŒ‰å¯†åº¦åŠ æƒ"
        elif effective_weight_mode == "inverse_density":
            mode_desc = "åå‘å¯†åº¦åŠ æƒ"
        else:
            mode_desc = "å‡åŒ€æƒé‡"
        print(f"   ä¼ªæ ‡ç­¾æŸå¤±æƒé‡æ¨¡å¼: {mode_desc}")
        mode_scope = "å·²æ ‡æ³¨+æœªæ ‡æ³¨ä¸€èµ·" if pseudo_for_labeled_mode == "all" else "ä»…æœªæ ‡æ³¨"
        print(f"   ä¼ªæ ‡ç­¾æ ·æœ¬èŒƒå›´: {mode_scope}")

    epoch_progress = None
    if is_grid_search and progress_parent is not None:
        epoch_progress = tqdm(
            total=args.epochs,
            desc=f"[{args.superclass_name}] è®­ç»ƒ",
            position=1,
            leave=False,
            dynamic_ncols=True
        )

    # åˆå§‹åŒ–è¶…ç±»æ¨¡å‹ä¿å­˜å™¨ï¼ˆä¼ å…¥argsä»¥æ”¯æŒè¶…å‚æ•°è®°å½•ï¼‰
    if model_saver is None:
        model_saver = create_superclass_model_saver(args.superclass_name, args=args)
    if not is_grid_search:
        print(f"ğŸ—‚ï¸  è¶…ç±»æ¨¡å‹ä¿å­˜å™¨å·²åˆå§‹åŒ–ï¼Œä¿å­˜ç›®å½•: {model_saver.save_dir}")

    # æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    start_epoch = 0
    if getattr(args, "resume_from_ckpt", None):
        if not is_grid_search:
            print(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume_from_ckpt}")
        loaded_epoch, extra_state = load_training_state(
            args.resume_from_ckpt,
            model=model,
            projection_head=projection_head,
            optimizer=optimizer,
            scheduler=exp_lr_scheduler,
            training_session=training_session,
        )
        start_epoch = loaded_epoch + 1

        # éªŒè¯ T_max æ˜¯å¦ä¸€è‡´ï¼ˆé˜²æ­¢ç”¨æˆ·æ‰‹åŠ¨ä¿®æ”¹ --epochs å‚æ•°ï¼‰
        saved_T_max = extra_state.get("T_max")
        if saved_T_max is not None and saved_T_max != args.epochs:
            raise ValueError(
                f"âŒ Checkpoint ä¿å­˜æ—¶çš„ epochs={saved_T_max}ï¼Œ"
                f"ä½†å½“å‰ --epochs={args.epochs}ï¼ŒT_max ä¸ä¸€è‡´ï¼\n"
                f"æç¤ºï¼šè¯·ä½¿ç”¨ç›¸åŒçš„ --epochs å‚æ•°æ¢å¤è®­ç»ƒï¼Œæˆ–åˆ é™¤ --resume_from_ckpt å‚æ•°ä»å¤´è®­ç»ƒã€‚"
            )

        if not is_grid_search:
            print(f"   âœ… æ¢å¤åˆ°ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ")
            print(f"   âœ… Scheduler çŠ¶æ€å·²æ¢å¤ (T_max={args.epochs}, å­¦ä¹ ç‡è¿ç»­)")

    if pseudo_cache and getattr(args, "writer", None):
        args.writer.add_scalar('Pseudo/file_core_ratio', pseudo_cache.core_ratio, start_epoch)
        args.writer.add_scalar('Pseudo/file_num_core', pseudo_cache.num_core, start_epoch)
        train_stats = getattr(pseudo_cache, "training_stats", None)
        if train_stats:
            args.writer.add_scalar('Pseudo/train_core_ratio', train_stats.get('core_ratio', 0.0), start_epoch)
            args.writer.add_scalar('Pseudo/train_core_count', train_stats.get('core_count', 0), start_epoch)
        args.writer.add_scalar('Pseudo/weight_mode_flag', 1 if args.pseudo_weight_mode_effective == 'density' else 0, start_epoch)
        args.writer.add_text('Pseudo/weight_mode', args.pseudo_weight_mode_effective, start_epoch)

    for epoch in range(start_epoch, args.epochs):
        if epoch_progress:
            epoch_progress.set_description(f"[{args.superclass_name}] Epoch {epoch+1}/{args.epochs}")

        # å¼€å§‹æ–°è½®æ¬¡
        training_session.start_epoch(epoch)

        loss_record = AverageMeter()
        contrastive_loss_record = AverageMeter()
        sup_con_loss_record = AverageMeter()
        pseudo_loss_record = AverageMeter()
        pseudo_sample_record = AverageMeter()
        train_acc_record = AverageMeter()

        # å®šä¹‰è½®æ¬¡çº§åˆ«çš„æƒé‡å˜é‡ï¼Œé¿å…æ‰¹æ¬¡å†…é‡å¤å®šä¹‰
        epoch_contrastive_weight = 1 - args.sup_con_weight
        epoch_sup_con_weight = args.sup_con_weight

        projection_head.train()
        model.train()

        if not is_grid_search:
            train_iter = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs}')
        else:
            train_iter = train_loader

        for batch_idx, batch in enumerate(train_iter):

            images, class_labels, uq_idxs, mask_lab = batch
            mask_lab = mask_lab[:, 0]

            class_labels, mask_lab = class_labels.to(args.device), mask_lab.to(args.device).bool()
            mask_lab_np = mask_lab.detach().cpu().numpy()
            images = torch.cat(images, dim=0).to(args.device)

            # Extract features with base model
            features = model(images)

            # Pass features through projection head
            features = projection_head(features)

            # L2-normalize features
            features = torch.nn.functional.normalize(features, dim=-1)

            # Choose which instances to run the contrastive loss on
            if args.contrast_unlabel_only:
                # Contrastive loss only on unlabelled instances
                f1, f2 = [f[~mask_lab] for f in features.chunk(2)]
                con_feats = torch.cat([f1, f2], dim=0)
            else:
                # Contrastive loss for all examples
                con_feats = features

            contrastive_logits, contrastive_labels = info_nce_logits(features=con_feats, args=args)
            contrastive_loss = torch.nn.CrossEntropyLoss()(contrastive_logits, contrastive_labels)

            # Supervised contrastive loss
            f1, f2 = [f[mask_lab] for f in features.chunk(2)]
            if f1.size(0) > 0:  # åªæœ‰å½“æœ‰æ ‡è®°æ ·æœ¬æ—¶æ‰è®¡ç®—ç›‘ç£å¯¹æ¯”æŸå¤±
                sup_con_feats = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)
                sup_con_labels = class_labels[mask_lab]
                sup_con_loss = sup_con_crit(sup_con_feats, labels=sup_con_labels)
            else:
                sup_con_loss = torch.tensor(0.0).to(args.device)

            # ä¼ªæ ‡ç­¾å¯¹æ¯”æŸå¤±
            pseudo_loss = torch.tensor(0.0, device=args.device)
            pseudo_sample_count = 0
            if pseudo_cache is not None:
                if isinstance(uq_idxs, torch.Tensor):
                    batch_indices_np = uq_idxs.detach().cpu().numpy().astype(np.int64)
                else:
                    batch_indices_np = np.asarray(uq_idxs, dtype=np.int64)
                pseudo_labels_np = pseudo_cache.lookup_labels(batch_indices_np)
                valid_mask_np = pseudo_labels_np >= 0
                if pseudo_for_labeled_mode != "all":
                    # ä»…æœªæ ‡æ³¨æ ·æœ¬å‚ä¸ä¼ªæ ‡ç­¾æŸå¤±
                    unlabeled_mask_np = ~mask_lab_np
                    valid_mask_np = np.logical_and(valid_mask_np, unlabeled_mask_np)
                if valid_mask_np.any():
                    pseudo_labels_tensor = torch.from_numpy(
                        pseudo_labels_np[valid_mask_np]
                    ).long().to(args.device)
                    pseudo_mask_tensor = torch.from_numpy(valid_mask_np).to(args.device)
                    f1_pseudo, f2_pseudo = [f[pseudo_mask_tensor] for f in features.chunk(2)]
                    if f1_pseudo.size(0) > 0:
                        pseudo_feats = torch.cat([f1_pseudo.unsqueeze(1), f2_pseudo.unsqueeze(1)], dim=1)
                        if args.pseudo_weight_mode_effective == 'density':
                            weight_values = pseudo_cache.lookup_weights(batch_indices_np, mode="density")[valid_mask_np]
                        elif args.pseudo_weight_mode_effective == 'inverse_density':
                            weight_values = pseudo_cache.lookup_weights(batch_indices_np, mode="inverse_density")[valid_mask_np]
                        else:
                            weight_values = np.ones(pseudo_labels_tensor.size(0), dtype=np.float32)
                        pseudo_weights_tensor = torch.from_numpy(weight_values).to(args.device)
                        pseudo_loss = sup_con_crit(
                            pseudo_feats,
                            labels=pseudo_labels_tensor,
                            sample_weights=pseudo_weights_tensor,
                        )
                        pseudo_sample_count = int(pseudo_labels_tensor.size(0))

            gamma = get_gamma(
                epoch,
                warmup_epochs=args.warmup_epochs,
                total_epochs=args.epochs,
                update_interval=getattr(args, "update_interval", None),
                ramp_intervals=2,
            )

            # æ€»æŸå¤± = æ— ç›‘ç£å¯¹æ¯”æŸå¤± + ç›‘ç£å¯¹æ¯”æŸå¤± + Î³Â·pseudo_loss_weightÂ·ä¼ªæ ‡ç­¾æŸå¤±
            loss = epoch_contrastive_weight * contrastive_loss + epoch_sup_con_weight * sup_con_loss
            if pseudo_cache is not None and pseudo_sample_count > 0:
                pseudo_loss_weight = getattr(args, 'pseudo_loss_weight', 1.0)
                loss = loss + gamma * pseudo_loss_weight * pseudo_loss

            # Train acc
            _, pred = contrastive_logits.max(1)
            acc = (pred == contrastive_labels).float().mean().item()
            train_acc_record.update(acc, pred.size(0))

            # è®°å½•å„ç§æŸå¤±
            loss_record.update(loss.item(), class_labels.size(0))
            contrastive_loss_record.update(contrastive_loss.item(), class_labels.size(0))
            sup_con_loss_record.update(sup_con_loss.item(), class_labels.size(0))
            if pseudo_cache is not None:
                pseudo_loss_record.update(pseudo_loss.item(), max(pseudo_sample_count, 1))
                pseudo_sample_record.update(pseudo_sample_count, 1)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # æ¯50ä¸ªbatchè¾“å‡ºä¸€æ¬¡å®æ—¶æŸå¤±
            if batch_idx % 50 == 0 and batch_idx > 0 and not is_grid_search:
                print(f"   Batch {batch_idx}: loss = {epoch_contrastive_weight:.2f}*{contrastive_loss.item():.4f} + {epoch_sup_con_weight:.2f}*{sup_con_loss.item():.4f} = {loss.item():.4f}")

        with torch.no_grad():
            if not is_grid_search:
                print('ğŸ” Testing on unlabelled examples in the training data...')
            all_acc, old_acc, new_acc = test_kmeans_superclass(model, unlabelled_train_loader,
                                                        epoch=epoch, save_name='Train ACC Unlabelled',
                                                        args=args)

            if not is_grid_search:
                print('ğŸ” Testing on disjoint test set...')
            all_acc_test, old_acc_test, new_acc_test = test_kmeans_superclass(model, test_loader,
                                                                   epoch=epoch, save_name='Test ACC',
                                                                   args=args)

        # ----------------
        # è¾“å‡ºè¯¦ç»†æŸå¤±åˆ†è§£
        # ----------------
        lr_value = get_mean_lr(optimizer)

        if not is_grid_search:
            pseudo_loss_weight = getattr(args, 'pseudo_loss_weight', 1.0)
            print(f"\nğŸ“Š Epoch {epoch+1}/{args.epochs} æŸå¤±åˆ†è§£:")
            print(f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*å¯¹æ¯”æŸå¤± + {epoch_sup_con_weight:.2f}*ç›‘ç£å¯¹æ¯”æŸå¤± + Î³*Î»*ä¼ªæ ‡ç­¾æŸå¤±")
            print(
                f"   æ€»æŸå¤± = {epoch_contrastive_weight:.2f}*{contrastive_loss_record.avg:.4f} "
                f"+ {epoch_sup_con_weight:.2f}*{sup_con_loss_record.avg:.4f} "
                f"+ {gamma:.2f}*{pseudo_loss_weight:.2f}*{pseudo_loss_record.avg:.4f} = {loss_record.avg:.4f}"
            )
            print(f"   å¯¹æ¯”å­¦ä¹ æŸå¤±: {contrastive_loss_record.avg:.4f}")
            print(f"   ç›‘ç£å¯¹æ¯”æŸå¤±: {sup_con_loss_record.avg:.4f}")
            if pseudo_cache is not None:
                print(
                    f"   ä¼ªæ ‡ç­¾æŸå¤±: {pseudo_loss_record.avg:.4f} "
                    f"(æ¯æ‰¹ä¼ªæ ‡ç­¾æ ·æœ¬â‰ˆ{pseudo_sample_record.avg:.1f})"
                )
            print(f"   è®­ç»ƒå‡†ç¡®ç‡: {train_acc_record.avg:.4f}")
            print(f"   å­¦ä¹ ç‡: {lr_value:.6f}")
        elif epoch_progress:
            epoch_progress.update(1)
            epoch_progress.set_postfix({
                'Loss': f"{loss_record.avg:.4f}",
                'TrainAcc': f"{train_acc_record.avg:.4f}",
                'TestAll': f"{all_acc_test:.4f}",
                'Best': f"{best_test_acc_lab:.4f}"
            })

        # ----------------
        # LOG & HOOKS
        # ----------------
        # 1) è®°å½•å½“å‰æŸå¤±ä¸å­¦ä¹ ç‡
        args.writer.add_scalar('Loss/Total', loss_record.avg, epoch)
        args.writer.add_scalar('Loss/Contrastive', contrastive_loss_record.avg, epoch)
        args.writer.add_scalar('Loss/SupCon', sup_con_loss_record.avg, epoch)
        args.writer.add_scalar('Train Acc Labelled Data', train_acc_record.avg, epoch)
        args.writer.add_scalar('LR', get_mean_lr(optimizer), epoch)

        # 2) è®¡ç®—å¹¶è®°å½•å½“å‰ epoch çš„ Î³ï¼ˆä¼ªæ ‡ç­¾æŸå¤±æƒé‡ï¼‰
        gamma = get_gamma(
            epoch,
            warmup_epochs=args.warmup_epochs,
            total_epochs=args.epochs,
            update_interval=getattr(args, "update_interval", None),
            ramp_intervals=2,
        )
        pseudo_loss_weight = getattr(args, 'pseudo_loss_weight', 1.0)
        args.writer.add_scalar('Pseudo/gamma', gamma, epoch)
        args.writer.add_scalar('Pseudo/loss_weight', pseudo_loss_weight, epoch)
        args.writer.add_scalar('Pseudo/effective_weight', gamma * pseudo_loss_weight, epoch)
        args.writer.add_scalar('Pseudo/for_labeled_mode_flag', 1.0 if pseudo_for_labeled_mode == "all" else 0.0, epoch)
        if pseudo_cache is not None:
            args.writer.add_scalar('Loss/Pseudo', pseudo_loss_record.avg, epoch)
            args.writer.add_scalar('Pseudo/samples_per_batch', pseudo_sample_record.avg, epoch)
        if not is_grid_search:
            print(f"   ä¼ªæ ‡ç­¾æƒé‡ Î³(epoch={epoch}) = {gamma:.4f}, Î» = {pseudo_loss_weight:.2f}, æœ‰æ•ˆæƒé‡ = {gamma * pseudo_loss_weight:.4f}")

        # 3) ä¼ªæ ‡ç­¾åˆ·æ–°é’©å­ï¼ˆå·²ç”±ç¦»çº¿ Stage2 å–ä»£ï¼Œé¿å…åœ¨è®­ç»ƒå†…éƒ¨é‡å¤èšç±»ï¼‰

        # ä½¿ç”¨è®­ç»ƒä¼šè¯ç®¡ç†å™¨å¤„ç†è½®æ¬¡ç»“æŸ
        should_stop = training_session.end_epoch(
            epoch=epoch,
            train_acc=train_acc_record.avg,
            loss_avg=loss_record.avg,
            all_acc=all_acc,
            old_acc=old_acc,
            new_acc=new_acc,
            all_acc_test=all_acc_test,
            old_acc_test=old_acc_test,
            new_acc_test=new_acc_test
        )

        # Step schedule
        exp_lr_scheduler.step()

        # ä¿å­˜æ¨¡å‹ - ä½¿ç”¨æ–°çš„è¶…ç±»æ¨¡å‹ä¿å­˜æœºåˆ¶
        # ä½¿ç”¨è¶…ç±»æ¨¡å‹ä¿å­˜å™¨ä¿å­˜æœ€ä½³æ¨¡å‹ (ä»…ä¿å­˜æœ€ä½³ï¼Œä¸å†ä¿å­˜æ¯è½®æ¨¡å‹)
        if all_acc_test > best_test_acc_lab:
            # ä½¿ç”¨æ–°çš„ä¿å­˜æœºåˆ¶ï¼Œä¼šè‡ªåŠ¨ç®¡ç†æ–‡ä»¶å‘½åã€åˆ é™¤æ—§æ¨¡å‹ã€ç”Ÿæˆè¶…å‚æ•°è®°å½•
            best_model_path, best_proj_path = model_saver.save_best_model(
                model=model,
                projection_head=projection_head,
                acc=all_acc_test,
                current_epoch=epoch + 1,  # ä¼ å…¥å½“å‰è½®æ•°ä»¥è®°å½•è®­ç»ƒè¿›åº¦
                metadata={
                    'epoch': epoch + 1,
                    'train_loss': loss_record.avg,
                    'all_acc_test': all_acc_test,
                    'old_acc_test': old_acc_test,
                    'new_acc_test': new_acc_test,
                    'train_acc': train_acc_record.avg
                }
            )
            best_test_acc_lab = all_acc_test

            # æ›´æ–° JSON æ–‡ä»¶è®°å½•å…¨å±€æœ€ä½³ä¿¡æ¯
            stage_name = "stage3" if getattr(args, "resume_from_ckpt", None) else "stage1"
            best_model_tracker.update_if_better(
                new_acc=all_acc_test,
                epoch=epoch + 1,
                model_path=os.path.relpath(best_model_path, args.exp_root),
                proj_path=os.path.relpath(best_proj_path, args.exp_root),
                metadata={
                    'all_acc': all_acc_test,
                    'old_acc': old_acc_test,
                    'new_acc': new_acc_test,
                    'train_loss': loss_record.avg,
                    'train_acc': train_acc_record.avg
                },
                hyperparameters={
                    'lr': args.lr,
                    'sup_con_weight': args.sup_con_weight,
                    'grad_from_block': args.grad_from_block,
                    'total_epochs': args.epochs
                },
                stage=stage_name
            )

        # å¯é€‰ï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€æ£€æŸ¥ç‚¹ï¼Œä¾¿äºåç»­éªŒè¯æ–­ç‚¹ç»­è®­
        if getattr(args, "save_ckpt_every", 0) and (epoch + 1) % args.save_ckpt_every == 0:
            ckpt_dir = os.path.join(args.exp_root, "checkpoints", args.superclass_name)
            ckpt_name = f"ckpt_epoch_{epoch+1:03d}.pt"
            ckpt_path = os.path.join(ckpt_dir, ckpt_name)
            extra_state = {
                "superclass_name": getattr(args, "superclass_name", None),
                "T_max": args.epochs,  # è®°å½•å½“å‰çš„ T_maxï¼Œç”¨äºæ¢å¤æ—¶éªŒè¯
            }
            if not is_grid_search:
                print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹: {ckpt_path}")
            save_training_state(
                ckpt_path,
                epoch=epoch,
                model=model,
                projection_head=projection_head,
                optimizer=optimizer,
                scheduler=exp_lr_scheduler,
                training_session=training_session,
                extra_state=extra_state,
            )

        stop_at_epoch = getattr(args, "stop_at_epoch", None)
        reached_stop = stop_at_epoch is not None and (epoch + 1) >= stop_at_epoch

        if should_stop or reached_stop:
            if not is_grid_search:
                reason = "æ—©åœè§¦å‘" if should_stop else f"åˆ°è¾¾ stop_at_epoch={stop_at_epoch}"
                print(f"\nğŸ›‘ è¶…ç±» '{args.superclass_name}' {reason}ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
            training_session.finish_training(epoch, early_stopped=True)
            if reached_stop and feature_cache_args and getattr(args, "save_features_and_exit", False):
                _run_feature_cache_stage(args, feature_cache_args)
            break

    else:
        # æ­£å¸¸å®Œæˆæ‰€æœ‰è½®æ¬¡
        training_session.finish_training(args.epochs - 1, early_stopped=False)

    if epoch_progress:
        epoch_progress.close()

    # è®­ç»ƒç»“æŸï¼Œæ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
    if not is_grid_search:
        print(f"\nğŸ“Š è¶…ç±» '{args.superclass_name}' è®­ç»ƒå®Œæˆ")

        # ä½¿ç”¨æŒä¹…åŒ–çš„ BestModelTrackerï¼ˆä¸ pipeline ä¿æŒä¸€è‡´ï¼‰
        best_acc = best_model_tracker.get_best_acc()
        best_epoch = best_model_tracker.get_best_epoch()

        print(f"ğŸ† æœ€ä½³æ¨¡å‹ä¿¡æ¯:")
        if best_acc > 0:
            print(f"   æœ€ä½³ACC: {best_acc:.4f} (epoch {best_epoch})")
            # æ˜¾ç¤ºè¯¦ç»†çš„å‡†ç¡®ç‡åˆ†è§£
            tracker_data = best_model_tracker.load()  # ä½¿ç”¨ load() æ–¹æ³•è€Œä¸æ˜¯ data å±æ€§
            if tracker_data.get('metadata'):
                metadata = tracker_data['metadata']
                old_acc = metadata.get('old_acc', 0)
                new_acc = metadata.get('new_acc', 0)
                if old_acc > 0 or new_acc > 0:
                    print(f"   Old ACC: {old_acc:.4f}")
                    print(f"   New ACC: {new_acc:.4f}")
        else:
            print("   å°šæœªè®°å½•æœ€ä½³æ¨¡å‹")

    return model, projection_head, best_test_acc_lab

def _run_feature_cache_stage(args, feature_cache_args):
    ckpt_dir = os.path.join(args.exp_root, "checkpoints", args.superclass_name)
    latest_ckpt = None
    if os.path.isdir(ckpt_dir):
        candidates = sorted(
            [os.path.join(ckpt_dir, f) for f in os.listdir(ckpt_dir) if f.startswith("ckpt_epoch_")],
            reverse=True)
        if candidates:
            latest_ckpt = candidates[0]
    if not latest_ckpt:
        raise FileNotFoundError(f"åœ¨ {ckpt_dir} æ‰¾ä¸åˆ° ckpt_epoch_*.ptï¼Œæ— æ³•å¯¼å‡ºç‰¹å¾")

    run_cache_features(
        superclass_name=feature_cache_args["superclass_name"],
        model_path=latest_ckpt,
        cache_dir=feature_cache_args["cache_dir"],
        batch_size=feature_cache_args["batch_size"],
        num_workers=feature_cache_args["num_workers"],
        gpu=feature_cache_args["gpu"],
        prop_train_labels=feature_cache_args["prop_train_labels"],
        seed=feature_cache_args["seed"],
    )


def test_kmeans_superclass(model, test_loader, epoch, save_name, args):
    """
    è¶…ç±»K-meansæµ‹è¯•å‡½æ•°ï¼ŒåŸºäºåŸå§‹çš„test_kmeansä¿®æ”¹
    """
    model.eval()

    all_feats = []
    targets = np.array([])
    mask = np.array([])

    is_grid_search = getattr(args, 'is_grid_search', False)

    if not is_grid_search:
        print('Collating features...')
        test_iter = tqdm(test_loader, desc='Collating features')
    else:
        test_iter = test_loader

    # First extract all features
    for batch_idx, (images, label, _) in enumerate(test_iter):

        images = images.to(args.device)

        # Pass features through base model only (no projection head for evaluation)
        feats = model(images)
        feats = torch.nn.functional.normalize(feats, dim=-1)

        all_feats.append(feats.cpu().numpy())
        targets = np.append(targets, label.cpu().numpy())
        mask = np.append(mask, np.array([True if x.item() in range(len(args.train_classes))
                                         else False for x in label]))

    # -----------------------
    # K-MEANS
    # -----------------------
    if not is_grid_search:
        print('Fitting K-Means...')
    all_feats = np.concatenate(all_feats)
    kmeans = KMeans(n_clusters=args.num_labeled_classes + args.num_unlabeled_classes, random_state=0, n_init=10).fit(all_feats)
    preds = kmeans.labels_
    if not is_grid_search:
        print('Done!')

    # -----------------------
    # EVALUATE
    # -----------------------
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥ç±»
    mask = np.array(mask, dtype=bool)  # ç¡®ä¿maskæ˜¯numpyæ•°ç»„
    has_unknown_classes = args.num_unlabeled_classes > 0 and (~mask).sum() > 0

    if has_unknown_classes:
        # æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šæ­£å¸¸è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        all_acc, old_acc, new_acc = log_accs_from_preds(y_true=targets, y_pred=preds, mask=mask,
                                                        T=epoch, eval_funcs=args.eval_funcs, save_name=save_name,
                                                        writer=args.writer)
    else:
        # æ²¡æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šåªè®¡ç®—å·²çŸ¥ç±»å‡†ç¡®ç‡
        from project_utils.cluster_utils import cluster_acc
        old_acc = cluster_acc(targets, preds)
        all_acc = old_acc  # å½“æ²¡æœ‰æœªçŸ¥ç±»æ—¶ï¼ŒAll ACC = Old ACC
        new_acc = 0.0      # æ²¡æœ‰æœªçŸ¥ç±»ï¼ŒNew ACCä¸º0

        if not is_grid_search:
            print(f"âš ï¸  æ³¨æ„: è¶…ç±» '{args.superclass_name}' ä¸­æ²¡æœ‰æœªçŸ¥ç±»æ ·æœ¬ï¼Œä»…è®¡ç®—Old ACC")

        # å†™å…¥TensorBoardæ—¥å¿—
        if args.writer is not None:
            args.writer.add_scalars(f'{save_name}_v1',
                                   {'Old': old_acc, 'New': new_acc, 'All': all_acc}, epoch)

    return all_acc, old_acc, new_acc



def train_all_superclasses(args):
    """
    è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»
    """
    print("ğŸŒŸ å¼€å§‹è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
    print("=" * 80)

    superclass_splits = get_superclass_splits()

    for superclass_name in SUPERCLASS_NAMES:
        print(f"\nğŸ¯ å‡†å¤‡è®­ç»ƒè¶…ç±»: {superclass_name}")

        # æ£€æŸ¥è¯¥è¶…ç±»æ˜¯å¦æœ‰è¶³å¤Ÿçš„å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»
        split_info = superclass_splits[superclass_name]
        if len(split_info['known_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰å·²çŸ¥ç±»")
            continue
        if len(split_info['unknown_classes']) == 0:
            print(f"âš ï¸  è·³è¿‡è¶…ç±» '{superclass_name}': æ²¡æœ‰æœªçŸ¥ç±»")
            continue

        # è®¾ç½®å½“å‰è¶…ç±»çš„å‚æ•°
        current_args = argparse.Namespace(**vars(args))
        current_args.superclass_name = superclass_name
        current_args.dataset_name = 'cifar100_superclass'

        # è·å–ç±»åˆ«åˆ’åˆ†
        current_args = get_class_splits(current_args)
        current_args.num_labeled_classes = len(current_args.train_classes)
        current_args.num_unlabeled_classes = len(current_args.unlabeled_classes)

        # åˆ›å»ºå®éªŒç›®å½•
        exp_name = f'superclass_{superclass_name}_{current_args.model_name}'
        current_args.exp_name = exp_name
        init_experiment(current_args, runner_name=['superclass_train'])

        try:
            # è®­ç»ƒå•ä¸ªè¶…ç±»
            train_single_superclass(current_args)
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¶…ç±» '{superclass_name}' æ—¶å‡ºé”™: {e}")
            continue

    print("\nğŸ‰ æ‰€æœ‰è¶…ç±»è®­ç»ƒå®Œæˆ!")



def train_single_superclass(args, model_saver=None, progress_parent=None):
    """
    è®­ç»ƒå•ä¸ªè¶…ç±»

    Args:
        args: è®­ç»ƒé…ç½®
        model_saver: å¯é€‰ï¼Œè¦†ç›–é»˜è®¤çš„è¶…ç±»æ¨¡å‹ä¿å­˜å™¨
    """
    device = args.device
    is_grid_search = getattr(args, 'is_grid_search', False)
    pseudo_cache = None
    pseudo_path = getattr(args, "pseudo_labels_path", None)
    if pseudo_path:
        pseudo_path = os.path.abspath(pseudo_path)
        if not os.path.isfile(pseudo_path):
            raise FileNotFoundError(f"ä¼ªæ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {pseudo_path}")
        pseudo_cache = load_pseudo_label_cache(pseudo_path)
        file_superclass = pseudo_cache.packet.metadata.get("superclass")
        if file_superclass and file_superclass != args.superclass_name:
            raise ValueError(
                f"ä¼ªæ ‡ç­¾æ–‡ä»¶å±äºè¶…ç±» '{file_superclass}'ï¼Œä¸å½“å‰ '{args.superclass_name}' ä¸åŒ¹é…"
            )
        if not is_grid_search:
            print("ğŸ“¥ è½½å…¥ä¼ªæ ‡ç­¾:")
            print(f"   æ–‡ä»¶: {pseudo_path}")
            print(
                f"   æ ·æœ¬æ€»æ•°: {pseudo_cache.num_total}, æ ¸å¿ƒç‚¹: {pseudo_cache.num_core} "
                f"({pseudo_cache.core_ratio * 100:.1f}%)"
            )
            if pseudo_cache.packet.metadata:
                meta_ckpt = pseudo_cache.packet.metadata.get("ckpt_path")
                if meta_ckpt:
                    print(f"   æ¥æº ckpt: {meta_ckpt}")

    # ----------------------
    # BASE MODEL
    # ----------------------
    if args.base_model == 'vit_dino':

        args.interpolation = 3
        args.crop_pct = 0.875
        pretrain_path = dino_pretrain_path

        model = vits.__dict__['vit_base']()

        state_dict = torch.load(pretrain_path, map_location='cpu')
        model.load_state_dict(state_dict)

        if args.warmup_model_dir is not None:
            if not getattr(args, 'is_grid_search', False):
                print(f'Loading weights from {args.warmup_model_dir}')
            model.load_state_dict(torch.load(args.warmup_model_dir, map_location='cpu'))

        model.to(device)

        # NOTE: Hardcoded image size as we do not finetune the entire ViT model
        args.image_size = 224
        args.feat_dim = 768
        args.num_mlp_layers = 3
        args.mlp_out_dim = 65536

        # ----------------------
        # HOW MUCH OF BASE MODEL TO FINETUNE
        # ----------------------
        for m in model.parameters():
            m.requires_grad = False

        # Only finetune layers from block 'args.grad_from_block' onwards
        for name, m in model.named_parameters():
            if 'block' in name:
                block_num = int(name.split('.')[1])
                if block_num >= args.grad_from_block:
                    m.requires_grad = True

    else:
        raise NotImplementedError

    # --------------------
    # CONTRASTIVE TRANSFORM
    # --------------------
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)
    train_transform = ContrastiveLearningViewGenerator(base_transform=train_transform, n_views=args.n_views)

    # --------------------
    # DATASETS - ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†
    # --------------------
    # ç›´æ¥ä½¿ç”¨è¶…ç±»ç‰¹å®šçš„æ•°æ®é›†è·å–å‡½æ•°
    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed,
        verbose=not is_grid_search
    )

    # Train split (labelled and unlabelled classes) for training
    train_dataset = MergedDataset(labelled_dataset=deepcopy(datasets['train_labelled']),
                                  unlabelled_dataset=deepcopy(datasets['train_unlabelled']))

    test_dataset = datasets['test']
    unlabelled_train_examples_test = deepcopy(datasets['train_unlabelled'])
    unlabelled_train_examples_test.transform = test_transform

    # --------------------
    # SAMPLER
    # Sampler which balances labelled and unlabelled examples in each batch
    # --------------------
    label_len = len(train_dataset.labelled_dataset)
    unlabelled_len = len(train_dataset.unlabelled_dataset)
    sample_weights = [1 if i < label_len else label_len / unlabelled_len for i in range(len(train_dataset))]
    sample_weights = torch.DoubleTensor(sample_weights)
    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(train_dataset))

    # --------------------
    # DATALOADERS
    # --------------------
    train_loader = DataLoader(train_dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False,
                              sampler=sampler, drop_last=True)
    test_loader_unlabelled = DataLoader(unlabelled_train_examples_test, num_workers=args.num_workers,
                                        batch_size=args.batch_size, shuffle=False)
    test_loader_labelled = DataLoader(test_dataset, num_workers=args.num_workers,
                                      batch_size=args.batch_size, shuffle=False)

    if pseudo_cache:
        labelled_uq = getattr(train_dataset.labelled_dataset, 'uq_idxs', None)
        unlabelled_uq = getattr(train_dataset.unlabelled_dataset, 'uq_idxs', None)
        if labelled_uq is None or unlabelled_uq is None:
            print("âš ï¸  å½“å‰æ•°æ®é›†ä¸åŒ…å« uq_idxs ä¿¡æ¯ï¼Œæ— æ³•åŒ¹é…ä¼ªæ ‡ç­¾ç´¢å¼•")
        else:
            train_indices = np.concatenate([labelled_uq, unlabelled_uq]).astype(np.int64)
            subset_stats = pseudo_cache.subset_stats(train_indices)
            if subset_stats["missing"] > 0:
                missing_sample = train_indices[pseudo_cache.missing_mask(train_indices)][0]
                raise ValueError(
                    f"ä¼ªæ ‡ç­¾æ–‡ä»¶ç¼ºå°‘ {subset_stats['missing']} ä¸ªè®­ç»ƒæ ·æœ¬ã€‚ç¤ºä¾‹ç´¢å¼•: {missing_sample}"
                )
            pseudo_cache.training_stats = subset_stats
            if not is_grid_search:
                print(
                    f"ğŸ’¡ è®­ç»ƒæ ·æœ¬ä¼ªæ ‡ç­¾è¦†ç›–: {subset_stats['covered']}/{subset_stats['total']} "
                    f"({subset_stats['core_ratio']*100:.1f}% æ ¸å¿ƒç‚¹æ¯”ä¾‹)"
                )

    # ----------------------
    # PROJECTION HEAD
    # ----------------------
    projection_head = vits.__dict__['DINOHead'](in_dim=args.feat_dim,
                               out_dim=args.mlp_out_dim, nlayers=args.num_mlp_layers)
    projection_head.to(device)

    # ----------------------
    # TRAIN
    # ----------------------
    return train_superclass(
        projection_head,
        model,
        train_loader,
        test_loader_labelled,
        test_loader_unlabelled,
        args,
        pseudo_cache=pseudo_cache,
        model_saver=model_saver,
        progress_parent=progress_parent,
        feature_cache_args={
            "superclass_name": args.superclass_name,
            "cache_dir": args.feature_cache_dir if args.feature_cache_dir else feature_cache_dir,
            "batch_size": args.batch_size,
            "num_workers": args.num_workers,
            "gpu": args.gpu,
            "prop_train_labels": args.prop_train_labels,
            "seed": args.seed,
        } if getattr(args, "save_features_and_exit", False) else None,
    )


def build_superclass_train_parser(add_help=True):
    parser = argparse.ArgumentParser(
            description='Train GCD on CIFAR-100 superclasses',
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            add_help=add_help)

    # =====================================================================
    # æ•°æ®é›†å’Œè®­ç»ƒåŸºç¡€å‚æ•°
    # =====================================================================
    parser.add_argument('--batch_size', default=128, type=int,
                        help='è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤å€¼: 128ã€‚è¾ƒå¤§çš„batch_sizeå¯ä»¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œä½†éœ€è¦æ›´å¤šGPUå†…å­˜')
    parser.add_argument('--num_workers', default=8, type=int,
                        help='æ•°æ®åŠ è½½çš„å·¥ä½œçº¿ç¨‹æ•°ã€‚é»˜è®¤å€¼: 8ã€‚æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼Œè¿‡å¤§å¯èƒ½å¯¼è‡´å†…å­˜å¼€é”€å¢åŠ ')
    parser.add_argument('--eval_funcs', nargs='+', default=['v1', 'v2'],
                        help='ä½¿ç”¨çš„è¯„ä¼°å‡½æ•°åˆ—è¡¨ã€‚é»˜è®¤å€¼: [v1, v2]ã€‚v1ä¸ºæ ‡å‡†èšç±»å‡†ç¡®ç‡ï¼Œv2ä¸ºåŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å‡†ç¡®ç‡')

    # =====================================================================
    # æ¨¡å‹æ¶æ„å‚æ•°
    # =====================================================================
    parser.add_argument('--warmup_model_dir', type=str, default=None,
                        help='é¢„è®­ç»ƒæƒé‡æ–‡ä»¶è·¯å¾„ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœæä¾›ï¼Œå°†ä»è¯¥è·¯å¾„åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒ')
    parser.add_argument('--model_name', type=str, default='vit_dino',
                        help='æ¨¡å‹åç§°æ ‡è¯†ã€‚é»˜è®¤å€¼: vit_dinoã€‚æ ¼å¼ä¸º{æ¨¡å‹æ¶æ„}_{é¢„è®­ç»ƒæ–¹æ³•}ï¼Œç”¨äºå®éªŒå‘½åå’Œæ—¥å¿—è®°å½•')
    parser.add_argument('--dataset_name', type=str, default='cifar100_superclass',
                        help='æ•°æ®é›†åç§°ã€‚é»˜è®¤å€¼: cifar100_superclassã€‚å›ºå®šä½¿ç”¨CIFAR-100è¶…ç±»æ•°æ®é›†ï¼Œä¸å»ºè®®ä¿®æ”¹')
    parser.add_argument('--prop_train_labels', type=float, default=0.8,
                        help='å·²çŸ¥ç±»æ ·æœ¬ä¸­ç”¨äºè®­ç»ƒçš„æ¯”ä¾‹ã€‚é»˜è®¤å€¼: 0.8ã€‚å‰©ä½™0.2ç”¨ä½œéªŒè¯é›†ï¼Œç¬¦åˆGCDæ ‡å‡†åˆ’åˆ†')

    # =====================================================================
    # è¶…ç±»è®­ç»ƒæ¨¡å¼å‚æ•°
    # =====================================================================
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¦è®­ç»ƒçš„å•ä¸ªè¶…ç±»åç§°ã€‚é»˜è®¤å€¼: Noneã€‚ä¾‹å¦‚: aquatic_mammals, fish, flowersç­‰ã€‚'
                             'å¯ç”¨è¶…ç±»è¯·æŸ¥çœ‹CIFAR-100è¶…ç±»å®šä¹‰')
    parser.add_argument('--train_all_superclasses', action='store_true',
                        help='æ‰¹é‡è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»çš„æ ‡å¿—ã€‚é»˜è®¤å€¼: Falseã€‚å¯ç”¨åå°†ä¾æ¬¡è®­ç»ƒæ‰€æœ‰è¶…ç±»ï¼Œå¿½ç•¥--superclass_nameå‚æ•°')

    # =====================================================================
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡å‚æ•°
    # =====================================================================
    parser.add_argument('--grad_from_block', type=int, default=11,
                        help='ViTæ¨¡å‹ä»ç¬¬å‡ ä¸ªblockå¼€å§‹è®¡ç®—æ¢¯åº¦ã€‚é»˜è®¤å€¼: 11ã€‚ViT-Baseå…±12ä¸ªblocks (0-11)ï¼Œ'
                             'ä»…å¾®è°ƒæœ€åå‡ å±‚å¯ä»¥å‡å°‘è®¡ç®—é‡å¹¶é˜²æ­¢è¿‡æ‹Ÿåˆ')
    parser.add_argument('--lr', type=float, default=0.1,
                        help='åˆå§‹å­¦ä¹ ç‡ã€‚é»˜è®¤å€¼: 0.1ã€‚ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šé€æ¸é™ä½è‡³lr*1e-3')
    parser.add_argument('--save_best_thresh', type=float, default=None,
                        help='ä¿å­˜æœ€ä½³æ¨¡å‹çš„å‡†ç¡®ç‡é˜ˆå€¼ã€‚é»˜è®¤å€¼: Noneã€‚å¦‚æœè®¾ç½®ï¼Œä»…å½“å‡†ç¡®ç‡è¶…è¿‡è¯¥é˜ˆå€¼æ—¶æ‰ä¿å­˜æ¨¡å‹')
    parser.add_argument('--gamma', type=float, default=0.1,
                        help='å­¦ä¹ ç‡è¡°å‡å› å­ã€‚é»˜è®¤å€¼: 0.1ã€‚ç”¨äºStepLRè°ƒåº¦å™¨ï¼ˆå½“å‰ä½¿ç”¨CosineAnnealingï¼Œæ­¤å‚æ•°æš‚æœªä½¿ç”¨ï¼‰')
    parser.add_argument('--momentum', type=float, default=0.9,
                        help='SGDä¼˜åŒ–å™¨çš„åŠ¨é‡ç³»æ•°ã€‚é»˜è®¤å€¼: 0.9ã€‚æœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›å¹¶å‡å°‘æŒ¯è¡')
    parser.add_argument('--weight_decay', type=float, default=1e-4,
                        help='æƒé‡è¡°å‡ç³»æ•°(L2æ­£åˆ™åŒ–)ã€‚é»˜è®¤å€¼: 1e-4ã€‚é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆ')
    parser.add_argument('--epochs', default=20, type=int,
                        help='è®­ç»ƒæ€»è½®æ•°ã€‚é»˜è®¤å€¼: 20ã€‚é…åˆæ—©åœæœºåˆ¶(patience=20)ï¼Œå®é™…è®­ç»ƒå¯èƒ½æå‰ç»“æŸ')

    # =====================================================================
    # è·¯å¾„å’Œå®éªŒé…ç½®å‚æ•°
    # =====================================================================
    parser.add_argument('--exp_root', type=str, default=exp_root,
                        help=f'å®éªŒè¾“å‡ºæ ¹ç›®å½•ã€‚é»˜è®¤å€¼: {exp_root}ã€‚æ‰€æœ‰æ¨¡å‹ã€æ—¥å¿—ã€TensorBoardæ–‡ä»¶å°†ä¿å­˜åœ¨æ­¤ç›®å½•ä¸‹')
    parser.add_argument('--transform', type=str, default='imagenet',
                        help='æ•°æ®å¢å¼ºæ–¹æ¡ˆåç§°ã€‚é»˜è®¤å€¼: imagenetã€‚ä½¿ç”¨ImageNetæ ‡å‡†çš„æ•°æ®å¢å¼ºç­–ç•¥')
    parser.add_argument('--seed', default=1, type=int,
                        help='éšæœºç§å­ã€‚é»˜è®¤å€¼: 1ã€‚ç”¨äºä¿è¯å®éªŒå¯å¤ç°æ€§')
    parser.add_argument('--gpu', type=int, default=0,
                        help='ä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ã€‚é»˜è®¤å€¼: 0ã€‚å¦‚æœæœ‰å¤šå—GPUï¼Œå¯ä»¥æŒ‡å®šä½¿ç”¨å“ªä¸€å—(0, 1, 2...)')

    # =====================================================================
    # è®­ç»ƒæ£€æŸ¥ç‚¹å‚æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ä¸ç¦»çº¿SSDDBCé›†æˆï¼‰
    # =====================================================================
    parser.add_argument('--resume_from_ckpt', type=str, default=None,
                        help='å¯é€‰ï¼šä»æŒ‡å®šæ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒï¼ˆä¿å­˜äº†æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œè®­ç»ƒä¼šè¯çŠ¶æ€ï¼‰ã€‚')
    parser.add_argument('--save_ckpt_every', type=int, default=0,
                        help='å¯é€‰ï¼šæ¯éš”å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡å®Œæ•´è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆ0è¡¨ç¤ºä¸è‡ªåŠ¨ä¿å­˜ï¼‰ã€‚')
    parser.add_argument('--pseudo_labels_path', type=str, default=None,
                        help='å¯é€‰ï¼šç¦»çº¿ SSDDBC ç”Ÿæˆçš„ä¼ªæ ‡ç­¾æ–‡ä»¶(.npz)è·¯å¾„ã€‚æä¾›åè®­ç»ƒä¼šåŠ è½½ä¼ªæ ‡ç­¾è¿›è¡Œé˜¶æ®µ2/3å®éªŒã€‚')
    parser.add_argument('--pseudo_weight_mode', type=str, default='none',
                        choices=['none', 'density', 'inverse_density'],
                        help='ä¼ªæ ‡ç­¾æŸå¤±æƒé‡æ¨¡å¼ï¼šnone=å‡åŒ€ï¼Œdensity=é«˜å¯†åº¦é«˜æƒé‡ï¼Œinverse_density=ä½å¯†åº¦é«˜æƒé‡ï¼ˆæƒé‡è£å‰ªåˆ°0.2-0.8ï¼‰ã€‚')
    parser.add_argument('--pseudo_loss_weight', type=float, default=1.0,
                        help='ä¼ªæ ‡ç­¾æŸå¤±çš„æ•´ä½“æƒé‡ç³»æ•°ï¼Œæœ€ç»ˆæƒé‡ = Î³ * pseudo_loss_weightï¼ˆé»˜è®¤: 1.0ï¼‰')
    parser.add_argument('--pseudo_for_labeled_mode', type=str, default='off',
                        choices=['off', 'all'],
                        help="ä¼ªæ ‡ç­¾æŸå¤±çš„æ ·æœ¬èŒƒå›´ï¼šoff=ä»…æœªæ ‡æ³¨æ ·æœ¬ï¼Œall=å·²æ ‡æ³¨+æœªæ ‡æ³¨ä¸€èµ·ä½¿ç”¨")
    parser.add_argument('--warmup_epochs', type=int, default=50,
                        help='ä¼ªæ ‡ç­¾æƒé‡é¢„çƒ­è½®æ•°ï¼Œepoch < warmup_epochs æ—¶ Î³=0ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--reuse_log_dir', type=str, default=None,
                        help='å¯é€‰ï¼šå¤ç”¨æ—¢æœ‰ TensorBoard æ—¥å¿—ç›®å½•ï¼Œä¾¿äºåœ¨å¤šé˜¶æ®µè®­ç»ƒä¸­æ‹¼æ¥æ›²çº¿ã€‚')
    parser.add_argument('--stop_at_epoch', type=int, default=None,
                        help='å¯é€‰ï¼šåœ¨æŒ‡å®š epoch åæå‰åœæ­¢è®­ç»ƒï¼Œå¸¸ç”¨äºé˜¶æ®µ1é¢„çƒ­ã€‚')
    parser.add_argument('--save_features_and_exit', action='store_true',
                        help='å¯é€‰ï¼šåœ¨ stop_at_epoch åœæ­¢æ—¶è‡ªåŠ¨è§¦å‘ç‰¹å¾ç¼“å­˜è„šæœ¬ã€‚')
    parser.add_argument('--feature_cache_dir', type=str, default=None,
                        help='å¯é€‰ï¼šç‰¹å¾ç¼“å­˜ç›®å½•ã€‚æœªæŒ‡å®šæ—¶ä½¿ç”¨ config.py ä¸­çš„é»˜è®¤å€¼ã€‚')
    parser.add_argument('--update_interval', type=int, default=None,
                        help='ä¼ªæ ‡ç­¾æ›´æ–°é—´éš”ï¼ˆç”¨äº Î³ åœ¨å‰ä¸¤ä¸ªé—´éš”å†…å®Œæˆä¸Šå‡ï¼›æœªæŒ‡å®šåˆ™ä½¿ç”¨çº¿æ€§è°ƒåº¦ï¼‰')

    # =====================================================================
    # å¯¹æ¯”å­¦ä¹ æ¡†æ¶å‚æ•°
    # =====================================================================
    parser.add_argument('--base_model', type=str, default='vit_dino',
                        help='åŸºç¡€æ¨¡å‹æ¶æ„ã€‚é»˜è®¤å€¼: vit_dinoã€‚å½“å‰ä»…æ”¯æŒvit_dino (ViT-Base with DINOé¢„è®­ç»ƒ)')
    parser.add_argument('--temperature', type=float, default=1.0,
                        help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦ç³»æ•°ã€‚é»˜è®¤å€¼: 1.0ã€‚æ§åˆ¶softmaxçš„å¹³æ»‘åº¦ï¼Œè¾ƒå°çš„å€¼ä½¿åˆ†å¸ƒæ›´å°–é”')
    parser.add_argument('--sup_con_weight', type=float, default=0.5,
                        help='ç›‘ç£å¯¹æ¯”æŸå¤±çš„æƒé‡ã€‚é»˜è®¤å€¼: 0.5ã€‚æ€»æŸå¤± = (1-w)*å¯¹æ¯”æŸå¤± + w*ç›‘ç£å¯¹æ¯”æŸå¤±ã€‚'
                             'èŒƒå›´[0,1]ï¼Œ0è¡¨ç¤ºçº¯æ— ç›‘ç£ï¼Œ1è¡¨ç¤ºçº¯ç›‘ç£')
    parser.add_argument('--n_views', default=2, type=int,
                        help='å¯¹æ¯”å­¦ä¹ çš„è§†å›¾æ•°é‡ã€‚é»˜è®¤å€¼: 2ã€‚æ¯ä¸ªæ ·æœ¬ç”Ÿæˆnä¸ªå¢å¼ºè§†å›¾ç”¨äºå¯¹æ¯”å­¦ä¹ ')
    parser.add_argument('--contrast_unlabel_only', type=str2bool, default=False,
                        help='æ˜¯å¦ä»…å¯¹æœªæ ‡è®°æ ·æœ¬è®¡ç®—å¯¹æ¯”æŸå¤±ã€‚é»˜è®¤å€¼: Falseã€‚Trueæ—¶å¯¹æ¯”æŸå¤±ä»…ç”¨äºæœªçŸ¥ç±»æ ·æœ¬')

    return parser


def main():
    parser = build_superclass_train_parser()
    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    print("ğŸš€ CIFAR-100è¶…ç±»GCDè®­ç»ƒè„šæœ¬")
    print("=" * 80)
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # å°†deviceæ·»åŠ åˆ°argsä¸­ï¼Œæ–¹ä¾¿å…¶ä»–å‡½æ•°ä½¿ç”¨
    args.device = device

    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    if args.train_all_superclasses:
        print("ğŸŒŸ æ¨¡å¼: è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        train_all_superclasses(args)
    elif args.superclass_name:
        print(f"ğŸ¯ æ¨¡å¼: è®­ç»ƒå•ä¸ªè¶…ç±» '{args.superclass_name}'")

        # éªŒè¯è¶…ç±»åç§°
        if args.superclass_name not in SUPERCLASS_NAMES:
            print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{args.superclass_name}'")
            print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
            sys.exit(1)

        # è·å–ç±»åˆ«åˆ’åˆ†
        args = get_class_splits(args)
        args.num_labeled_classes = len(args.train_classes)
        args.num_unlabeled_classes = len(args.unlabeled_classes)

        # åˆå§‹åŒ–å®éªŒ / å¤ç”¨æ—¥å¿—
        exp_name = f'superclass_{args.superclass_name}_{args.model_name}'
        args.exp_name = exp_name
        reuse_log_dir = getattr(args, "reuse_log_dir", None)
        if reuse_log_dir:
            reuse_log_dir = os.path.abspath(reuse_log_dir)
            if not os.path.isdir(reuse_log_dir):
                raise FileNotFoundError(f"reuse_log_dir ä¸å­˜åœ¨: {reuse_log_dir}")
            args.log_dir = reuse_log_dir
            args.model_dir = os.path.join(args.log_dir, 'checkpoints')
            os.makedirs(args.model_dir, exist_ok=True)
            args.model_path = os.path.join(args.model_dir, 'model.pt')
            if getattr(args, "writer", None):
                try:
                    args.writer.close()
                except Exception:
                    pass
            args.writer = SummaryWriter(log_dir=args.log_dir)
            if not getattr(args, 'is_grid_search', False):
                print(f"ğŸ” å¤ç”¨ TensorBoard æ—¥å¿—ç›®å½•: {args.log_dir}")
        else:
            init_experiment(args, runner_name=['superclass_train'])
        print(f'Using evaluation function {args.eval_funcs[0]} to print results')

        # è®­ç»ƒ
        train_single_superclass(args)
    else:
        print("âŒ é”™è¯¯: è¯·æŒ‡å®šä»¥ä¸‹é€‰é¡¹ä¹‹ä¸€:")
        print("   --superclass_name <åç§°>  : è®­ç»ƒå•ä¸ªè¶…ç±»")
        print("   --train_all_superclasses  : è®­ç»ƒæ‰€æœ‰15ä¸ªè¶…ç±»")
        print(f"ğŸ“‹ å¯ç”¨çš„è¶…ç±»: {SUPERCLASS_NAMES}")
        sys.exit(1)

    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")


if __name__ == "__main__":
    main()
