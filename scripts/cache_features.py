#!/usr/bin/env python3
"""
è¶…ç±»ç‰¹å¾ç¼“å­˜è„šæœ¬
è´Ÿè´£åŠ è½½æŒ‡å®šè¶…ç±»çš„æœ€ä½³æ¨¡å‹ï¼Œæå–è®­ç»ƒ/æµ‹è¯•ç‰¹å¾å¹¶å†™å…¥æ ‡å‡†ç¼“å­˜ç›®å½•
å®Œå…¨å¤åˆ¶test_feature.pyçš„ç‰¹å¾æå–é€»è¾‘ä»¥ç¡®ä¿æ€§èƒ½ä¸€è‡´æ€§
"""

from __future__ import annotations

import argparse
import glob
import os
import pickle
import re
import sys
from copy import deepcopy
from typing import Dict, List, Optional, Set, Tuple

import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥sys.pathï¼Œæ–¹ä¾¿è„šæœ¬ç‹¬ç«‹è¿è¡Œ
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from collections import OrderedDict
from utils.data import FeatureLoader
from config import checkpoint_root as DEFAULT_CHECKPOINT_ROOT
from config import feature_cache_dir as DEFAULT_FEATURE_CACHE_DIR
from config import dino_pretrain_path
from data.augmentations import get_transform
from data.cifar100_superclass import CIFAR100_SUPERCLASSES, SUPERCLASS_NAMES, get_single_superclass_datasets
from data.data_utils import MergedDataset
from models import vision_transformer as vits
from project_utils.general_utils import str2bool


def set_deterministic_behavior():
    """è®¾ç½®ç¡®å®šæ€§è¡Œä¸ºï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyï¼‰"""
    torch.manual_seed(0)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(0)
        torch.cuda.manual_seed_all(0)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def parse_acc_from_filename(filename: str) -> Optional[float]:
    """
    æ ¹æ®æ–‡ä»¶åè§£æACCå€¼ï¼Œå…¼å®¹å¤šç§å‘½åæ ¼å¼
    """
    match = re.search(r'acc([0-9]+(?:\.[0-9]+)?)', filename)
    if match:
        try:
            return float(match.group(1))
        except ValueError:
            return None

    match = re.search(r'allacc_(\d+)', filename)
    if match:
        try:
            return float(match.group(1)) / 100.0
        except ValueError:
            return None

    return None


def find_best_superclass_model(superclass_name: str,
                               checkpoint_root: str = DEFAULT_CHECKPOINT_ROOT) -> Tuple[str, float]:
    """
    è‡ªåŠ¨æ‰«æè¶…ç±»æ¨¡å‹ç›®å½•ä»¥æ‰¾åˆ°ACCæœ€é«˜çš„æ¨¡å‹æƒé‡
    """
    model_dir = os.path.join(checkpoint_root, superclass_name)
    if not os.path.isdir(model_dir):
        raise FileNotFoundError(f"è¶…ç±»æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")

    candidates: Set[str] = set()
    for pattern in ('model_best_acc*.pt', 'allacc_*.pt', '*.pt'):
        pattern_paths = glob.glob(os.path.join(model_dir, pattern))
        candidates.update(pattern_paths)

    if not candidates:
        raise FileNotFoundError(f"åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶: {model_dir}")

    best_path: Optional[str] = None
    best_acc = -1.0

    for path in sorted(candidates):
        acc = parse_acc_from_filename(os.path.basename(path))
        if acc is None:
            continue
        if acc > best_acc:
            best_acc = acc
            best_path = path

    if best_path is None:
        raise ValueError(f"æ— æ³•ä»ä»¥ä¸‹æ–‡ä»¶è§£æACC: {', '.join(sorted(candidates))}")

    return best_path, best_acc


def _unwrap_model_weights(raw_state) -> OrderedDict:
    """
    å°†å¯èƒ½åŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€çš„æ£€æŸ¥ç‚¹è§£åŒ…ä¸ºçº¯æ¨¡å‹æƒé‡ã€‚
    """
    if isinstance(raw_state, dict):
        # æ¥è‡ª save_training_state çš„å®Œæ•´æ£€æŸ¥ç‚¹
        if "model" in raw_state and isinstance(raw_state["model"], (dict, OrderedDict)):
            print("   æ£€æµ‹åˆ°å®Œæ•´è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œè‡ªåŠ¨æå–å…¶ä¸­çš„æ¨¡å‹å‚æ•°ç”¨äºç‰¹å¾æå–")
            return raw_state["model"]
        # é€šç”¨æ¨¡å¼ï¼šstate_dict åŒ…è£¹
        if "state_dict" in raw_state and isinstance(raw_state["state_dict"], (dict, OrderedDict)):
            return raw_state["state_dict"]
    if isinstance(raw_state, OrderedDict):
        return raw_state
    raise RuntimeError(
        "æ— æ³•ä»ç»™å®šæ–‡ä»¶ä¸­è§£ææ¨¡å‹å‚æ•°ï¼Œè¯·ç¡®è®¤ä¼ å…¥çš„æ˜¯æ¨¡å‹æƒé‡æˆ– save_training_state äº§ç”Ÿçš„æ£€æŸ¥ç‚¹ã€‚"
    )


def load_superclass_model(model_path: str, device: torch.device, feat_dim: int = 768) -> torch.nn.Module:
    """
    åŠ è½½è®­ç»ƒå¥½çš„ViTåŸºç¡€æ¨¡å‹ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyçš„é€»è¾‘ï¼‰
    é‡‡ç”¨ä¸¤é˜¶æ®µåŠ è½½ï¼šå…ˆåŠ è½½DINOé¢„è®­ç»ƒåŸºåº§ï¼Œå†åŠ è½½GCDè®­ç»ƒæƒé‡
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
    print(f"   è®¾å¤‡: {device}")

    # åˆ›å»ºViTæ¨¡å‹
    model = vits.__dict__['vit_base']()

    # ã€å…³é”®ã€‘ç¬¬ä¸€é˜¶æ®µï¼šåŠ è½½DINOé¢„è®­ç»ƒæƒé‡ä½œä¸ºåŸºåº§
    if os.path.exists(dino_pretrain_path):
        print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡: {dino_pretrain_path}")
        dino_state_dict = torch.load(dino_pretrain_path, map_location='cpu')
        model.load_state_dict(dino_state_dict, strict=False)
    else:
        print(f"âš ï¸  DINOé¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨: {dino_pretrain_path}")

    # ã€å…³é”®ã€‘ç¬¬äºŒé˜¶æ®µï¼šåŠ è½½GCDè®­ç»ƒæƒé‡
    print(f"   åŠ è½½è®­ç»ƒæƒé‡: {model_path}")
    raw_state = torch.load(model_path, map_location='cpu')
    gcd_state_dict = _unwrap_model_weights(raw_state)
    model.load_state_dict(gcd_state_dict)

    model.to(device)

    # å…³é—­æ¢¯åº¦è®¡ç®—
    for param in model.parameters():
        param.requires_grad = False

    model.eval()

    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (å‚æ•°é‡: {total_params:,}, ç‰¹å¾ç»´åº¦: {feat_dim})")
    return model


def extract_features(data_loader, model, device, known_classes=None, dataset_type="unknown", use_l2=True):
    """
    æå–ç‰¹å¾ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyçš„ç‰¹å¾æå–é€»è¾‘ï¼‰

    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        model: ç‰¹å¾æå–æ¨¡å‹
        device: è®¾å¤‡
        known_classes: å·²çŸ¥ç±»åˆ«é›†åˆ
        dataset_type: æ•°æ®é›†ç±»å‹æ ‡è¯†ç¬¦
        use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ– (True=L2å½’ä¸€åŒ–, False=ä¸ä½¿ç”¨)

    Returns:
        tuple(np.ndarray, ...):
            features: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
            targets: çœŸå®æ ‡ç­¾
            known_mask: å·²çŸ¥ç±»åˆ«æ©ç  (True=å·²çŸ¥ç±», False=æœªçŸ¥ç±»)
            labeled_mask: æœ‰æ ‡ç­¾æ©ç  (True=æœ‰æ ‡ç­¾, False=æ— æ ‡ç­¾)
            indices: æ•°æ®é›†å…¨å±€ç´¢å¼•
    """
    l2_status = "L2å½’ä¸€åŒ–" if use_l2 else "æ— L2å½’ä¸€åŒ–"
    print(f"ğŸ”„ æå–{dataset_type}ç‰¹å¾ ({l2_status})...")

    model.eval()
    all_feats = []
    targets = np.array([])
    mask = np.array([])  # å·²çŸ¥ç±»åˆ«æ©ç 
    labeled_mask = np.array([])  # æœ‰æ ‡ç­¾æ©ç 
    all_indices = []

    with torch.no_grad():
        for batch_idx, batch_data in enumerate(tqdm(data_loader, desc=f"æå–{dataset_type}ç‰¹å¾")):
            # è§£åŒ…æ•°æ®ï¼ˆä¸test_feature.pyå®Œå…¨ä¸€è‡´ï¼‰
            if len(batch_data) == 4:
                images, labels, indices, labeled_or_not = batch_data
                labeled_batch = labeled_or_not.numpy().flatten().astype(bool)
            elif len(batch_data) == 3:
                images, labels, indices = batch_data
                # æµ‹è¯•é›†å…¨éƒ¨æ ‡è®°ä¸ºæ— æ ‡ç­¾
                labeled_batch = np.zeros(len(labels), dtype=bool)
            else:
                continue

            # æå–ç‰¹å¾
            images = images.to(device)
            feats = model(images)

            # æ ¹æ®use_l2å‚æ•°å†³å®šæ˜¯å¦è¿›è¡ŒL2å½’ä¸€åŒ–
            if use_l2:
                feats = torch.nn.functional.normalize(feats, dim=-1)

            # æ”¶é›†æ•°æ®
            all_feats.append(feats.cpu().numpy())
            targets = np.append(targets, labels.cpu().numpy())
            labeled_mask = np.append(labeled_mask, labeled_batch)

            # è®°å½•æ ·æœ¬ç´¢å¼•
            if isinstance(indices, torch.Tensor):
                batch_indices = indices.cpu().numpy()
            else:
                batch_indices = np.atleast_1d(np.asarray(indices))
            all_indices.append(batch_indices.astype(np.int64))

            # åˆ›å»ºå·²çŸ¥ç±»åˆ«æ©ç ï¼ˆæ ¹æ®known_classesåˆ—è¡¨ï¼‰
            if known_classes is not None:
                batch_mask = np.array([True if x.item() in known_classes else False for x in labels])
            else:
                # é»˜è®¤ï¼šå‰80ä¸ªç±»åˆ«æ˜¯å·²çŸ¥ç±»
                batch_mask = np.array([True if x.item() < 80 else False for x in labels])
            mask = np.append(mask, batch_mask)

            # æ¸…ç†GPUå†…å­˜
            del images, feats
            torch.cuda.empty_cache()

    # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    all_indices = np.concatenate(all_indices, axis=0).astype(np.int64)
    print(f"âœ… {dataset_type}ç‰¹å¾æå–å®Œæˆ: {all_feats.shape}")

    return all_feats, targets.astype(int), mask.astype(bool), labeled_mask.astype(bool), all_indices


def load_superclass_datasets(superclass_name: str,
                             batch_size: int,
                             num_workers: int,
                             prop_train_labels: float = 0.8,
                             seed: int = 0,
                             image_size: int = 224):
    """
    æ„å»ºä¸test_feature.pyä¸€è‡´çš„æ•°æ®åŠ è½½å™¨ï¼ˆå®Œå…¨å¤åˆ¶é€»è¾‘ï¼‰
    """
    # ã€å…³é”®ã€‘ä½¿ç”¨ä¸test_feature.pyå®Œå…¨ä¸€è‡´çš„å‚æ•°è®¾ç½®
    class Args:
        def __init__(self):
            self.dataset_name = 'cifar100_superclass'
            self.superclass_name = superclass_name
            self.prop_train_labels = prop_train_labels
            self.image_size = image_size
            self.num_workers = num_workers
            self.batch_size = batch_size
            self.base_model = 'vit_dino'
            self.feat_dim = 768
            self.interpolation = 3
            self.crop_pct = 0.875
            self.seed = seed

    args = Args()

    # ã€å…³é”®ã€‘ç¡¬ç¼–ç ä½¿ç”¨'imagenet' transformï¼Œä¸test_feature.pyä¿æŒä¸€è‡´
    train_transform, test_transform = get_transform('imagenet', image_size=args.image_size, args=args)

    datasets = get_single_superclass_datasets(
        superclass_name=superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed
    )

    # åˆ›å»ºMergedDatasetï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰
    merged_train = MergedDataset(
        labelled_dataset=deepcopy(datasets['train_labelled']),
        unlabelled_dataset=deepcopy(datasets['train_unlabelled'])
    )

    train_loader = DataLoader(
        merged_train,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        drop_last=False
    )
    test_loader = DataLoader(
        datasets['test'],
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        drop_last=False
    )

    # ã€å…³é”®ã€‘æ‰‹åŠ¨æ„å»ºlabel_mappingï¼Œä¸test_feature.pyä¿æŒä¸€è‡´
    superclass_classes = set(CIFAR100_SUPERCLASSES[superclass_name])
    superclass_known_classes_orig = set([cls for cls in superclass_classes if cls < 80])
    superclass_unknown_classes_orig = set([cls for cls in superclass_classes if cls >= 80])

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ï¼ˆä¸è¶…ç±»æ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
    all_classes_sorted = sorted(list(superclass_classes))
    label_mapping = {orig_cls: new_cls for new_cls, orig_cls in enumerate(all_classes_sorted)}

    # æ˜ å°„åçš„å·²çŸ¥/æœªçŸ¥ç±»åˆ«ID
    known_label_ids = set([label_mapping[cls] for cls in superclass_known_classes_orig])
    unknown_label_ids = set([label_mapping[cls] for cls in superclass_unknown_classes_orig])

    dataset_stats = {
        'train_labelled_len': len(datasets['train_labelled']),
        'train_unlabelled_len': len(datasets['train_unlabelled']),
        'train_total': len(datasets['train_labelled']) + len(datasets['train_unlabelled']),
        'test_len': len(datasets['test']),
        'known_label_ids': known_label_ids,
        'unknown_label_ids': unknown_label_ids,
        'label_mapping': label_mapping,
        'all_classes_sorted': all_classes_sorted,
        'known_classes_orig': sorted(list(superclass_known_classes_orig)),
        'unknown_classes_orig': sorted(list(superclass_unknown_classes_orig))
    }

    print(f"ğŸ“Š è¶…ç±»ä¿¡æ¯:")
    print(f"   åŸå§‹å·²çŸ¥ç±»: {sorted(list(superclass_known_classes_orig))} -> æ˜ å°„å: {sorted(list(known_label_ids))}")
    print(f"   åŸå§‹æœªçŸ¥ç±»: {sorted(list(superclass_unknown_classes_orig))} -> æ˜ å°„å: {sorted(list(unknown_label_ids))}")

    return train_loader, test_loader, dataset_stats


def extract_and_cache_features(model: torch.nn.Module,
                               train_loader: DataLoader,
                               test_loader: DataLoader,
                               superclass_name: str,
                               dataset_stats: Dict,
                               cache_loader: FeatureLoader,
                               use_l2: bool,
                               device: torch.device,
                               model_path: str):
    """
    é€šè¿‡extract_featuresæå–ç‰¹å¾å¹¶å†™å…¥æ ‡å‡†ç¼“å­˜ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyçš„é€»è¾‘ï¼‰
    """
    # æå–ç‰¹å¾ï¼ˆä½¿ç”¨å¤åˆ¶çš„extract_featureså‡½æ•°ï¼‰
    print("ğŸ“Š æå–è®­ç»ƒé›†ç‰¹å¾...")
    train_feats, train_targets, train_known_mask, train_labeled_mask, train_indices = extract_features(
        train_loader, model, device, dataset_stats['known_label_ids'], "è®­ç»ƒé›†", use_l2=use_l2
    )

    print("ğŸ“Š æå–æµ‹è¯•é›†ç‰¹å¾...")
    test_feats, test_targets, test_known_mask, test_labeled_mask, test_indices = extract_features(
        test_loader, model, device, dataset_stats['known_label_ids'], "æµ‹è¯•é›†", use_l2=use_l2
    )

    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰
    all_feats = np.concatenate([train_feats, test_feats], axis=0)
    all_targets = np.concatenate([train_targets, test_targets], axis=0)
    all_known_mask = np.concatenate([train_known_mask, test_known_mask], axis=0)
    all_labeled_mask = np.concatenate([train_labeled_mask, test_labeled_mask], axis=0)
    all_indices = np.concatenate([train_indices, test_indices], axis=0)

    print(f"ğŸ“Š åˆå¹¶åæ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_feats)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬: {np.sum(all_known_mask)}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬: {np.sum(~all_known_mask)}")
    print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {np.sum(all_labeled_mask)}")
    print(f"   æ— æ ‡ç­¾æ ·æœ¬: {np.sum(~all_labeled_mask)}")

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆä¸test_feature.pyçš„æ•°æ®ç»“æ„å®Œå…¨ä¸€è‡´ï¼‰
    feature_dict = {
        # å®Œæ•´æ•°æ®ï¼ˆè®­ç»ƒ+æµ‹è¯•ï¼‰
        'all_features': all_feats,
        'all_targets': all_targets,
        'all_known_mask': all_known_mask,
        'all_labeled_mask': all_labeled_mask,
        'all_indices': all_indices,

        # åˆ†ç¦»çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®
        'train_features': train_feats,
        'train_targets': train_targets,
        'train_known_mask': train_known_mask,
        'train_labeled_mask': train_labeled_mask,
        'train_indices': train_indices,
        'test_features': test_feats,
        'test_targets': test_targets,
        'test_known_mask': test_known_mask,
        'test_labeled_mask': test_labeled_mask,
        'test_indices': test_indices,

        # å…ƒä¿¡æ¯
        'superclass_name': superclass_name,
        'known_classes_mapped': dataset_stats['known_label_ids'],
        'unknown_classes_mapped': dataset_stats['unknown_label_ids'],
        'known_classes_orig': dataset_stats['known_classes_orig'],
        'unknown_classes_orig': dataset_stats['unknown_classes_orig'],
        'label_mapping': dataset_stats['label_mapping'],
        'all_classes_sorted': dataset_stats['all_classes_sorted'],

        # æ•°æ®é›†åˆ†å‰²ä¿¡æ¯
        'train_size': len(train_feats),
        'test_size': len(test_feats),
        'total_size': len(all_feats),

        # æå–å‚æ•°
        'model_path': model_path,
        'feat_dim': 768,
        'image_size': 224
    }

    # ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
    cache_file_path = cache_loader.get_cache_path(superclass_name, use_l2=use_l2)
    os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)

    with open(cache_file_path, 'wb') as handle:
        pickle.dump(feature_dict, handle, protocol=4)

    total_samples = len(feature_dict['all_features'])
    feat_dim = feature_dict['all_features'].shape[1]
    size_mb = os.path.getsize(cache_file_path) / (1024 ** 2)

    print(f"ğŸ’¾ å·²å†™å…¥ç¼“å­˜: {cache_file_path}")
    print(f"   æ ·æœ¬æ•°: {total_samples}, ç‰¹å¾ç»´åº¦: {feat_dim}, æ–‡ä»¶å¤§å°: {size_mb:.2f} MB")

    return cache_file_path, feature_dict


def cache_single_superclass(superclass_name: str,
                            model_path: Optional[str] = None,
                            auto_find_best: bool = True,
                            checkpoint_root: str = DEFAULT_CHECKPOINT_ROOT,
                            cache_dir: str = DEFAULT_FEATURE_CACHE_DIR,
                            batch_size: int = 64,
                            num_workers: int = 4,
                            gpu: int = 0,
                            use_l2: bool = True,
                            overwrite: bool = False,
                            prop_train_labels: float = 0.8,
                            seed: int = 0,
                            image_size: int = 224) -> Dict[str, object]:
    """
    ä¸ºå•ä¸ªè¶…ç±»æå–å¹¶ç¼“å­˜ç‰¹å¾ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyçš„é€»è¾‘ï¼‰
    """
    print("\n" + "=" * 80)
    print(f"ğŸ¯ å¼€å§‹å¤„ç†è¶…ç±»: {superclass_name}")
    print("=" * 80)

    # è®¾ç½®ç¡®å®šæ€§è¡Œä¸ºï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰
    set_deterministic_behavior()

    cache_loader = FeatureLoader(cache_base_dir=cache_dir)

    if cache_loader.check_cache_exists(superclass_name, use_l2=use_l2) and not overwrite:
        cache_path = cache_loader.get_cache_path(superclass_name, use_l2=use_l2)
        print(f"âš ï¸  ç¼“å­˜å·²å­˜åœ¨ä¸”æœªå¯ç”¨è¦†ç›–: {cache_path}")
        return {'status': 'skipped', 'superclass_name': superclass_name, 'reason': 'cache_exists'}

    if model_path is None:
        if not auto_find_best:
            raise ValueError("æœªæä¾›model_pathä¸”auto_find_bestè¢«ç¦ç”¨")
        model_path, best_acc = find_best_superclass_model(superclass_name, checkpoint_root)
        print(f"ğŸ” è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³æ¨¡å‹: {os.path.basename(model_path)} (ACC={best_acc:.4f})")
    else:
        best_acc = None
        model_path = os.path.abspath(model_path)

    device = torch.device(f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # ã€å…³é”®ã€‘ä½¿ç”¨æ–°çš„æ¨¡å‹åŠ è½½å‡½æ•°ï¼ˆä¸¤é˜¶æ®µåŠ è½½ï¼‰
    model = load_superclass_model(model_path, device, feat_dim=768)

    # ã€å…³é”®ã€‘ä½¿ç”¨æ–°çš„æ•°æ®é›†åŠ è½½å‡½æ•°ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyé€»è¾‘ï¼‰
    train_loader, test_loader, dataset_stats = load_superclass_datasets(
        superclass_name=superclass_name,
        batch_size=batch_size,
        num_workers=num_workers,
        prop_train_labels=prop_train_labels,
        seed=seed,
        image_size=image_size
    )

    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   è®­ç»ƒé›†(æœ‰æ ‡è®°): {dataset_stats['train_labelled_len']} | "
          f"è®­ç»ƒé›†(æ— æ ‡è®°): {dataset_stats['train_unlabelled_len']}")
    print(f"   è®­ç»ƒé›†æ€»è®¡: {dataset_stats['train_total']} | æµ‹è¯•é›†: {dataset_stats['test_len']}")
    print(f"   å·²çŸ¥ç±»æ•°: {len(dataset_stats['known_label_ids'])} | "
          f"æœªçŸ¥ç±»æ•°: {len(dataset_stats['unknown_label_ids'])}")

    # ã€å…³é”®ã€‘ä½¿ç”¨æ–°çš„ç‰¹å¾æå–å’Œç¼“å­˜å‡½æ•°ï¼ˆå®Œå…¨å¤åˆ¶test_feature.pyé€»è¾‘ï¼‰
    cache_file_path, feature_dict = extract_and_cache_features(
        model=model,
        train_loader=train_loader,
        test_loader=test_loader,
        superclass_name=superclass_name,
        dataset_stats=dataset_stats,
        cache_loader=cache_loader,
        use_l2=use_l2,
        device=device,
        model_path=model_path
    )

    print("ğŸ§ª æ­£åœ¨éªŒè¯ç¼“å­˜å¯è¯»æ€§...")
    loaded = cache_loader.load(superclass_name, use_l2=use_l2, silent=True)
    if loaded is None:
        raise RuntimeError("ç¼“å­˜éªŒè¯å¤±è´¥ï¼Œpickleæ–‡ä»¶å¯èƒ½æŸå")

    print(f"ğŸ‰ è¶…ç±» {superclass_name} ç¼“å­˜å®Œæˆ")
    return {
        'status': 'success',
        'superclass_name': superclass_name,
        'model_path': model_path,
        'cache_file_path': cache_file_path,
        'n_samples': len(feature_dict['all_features']),
        'feat_dim': feature_dict['all_features'].shape[1],
        'best_acc': best_acc
    }


def cache_all_superclasses(superclass_names: Optional[List[str]] = None, **kwargs):
    """
    æ‰¹é‡ä¸ºå¤šä¸ªè¶…ç±»ç”Ÿæˆç‰¹å¾ç¼“å­˜
    """
    names = superclass_names or SUPERCLASS_NAMES
    print(f"\nğŸŒŸ å³å°†å¤„ç† {len(names)} ä¸ªè¶…ç±»: {names}")

    results = []
    for idx, name in enumerate(names, start=1):
        print(f"\nâ¡ï¸  è¿›åº¦ [{idx}/{len(names)}] - {name}")
        try:
            result = cache_single_superclass(superclass_name=name, **kwargs)
        except Exception as exc:  # pylint: disable=broad-except
            print(f"âŒ è¶…ç±» {name} ç¼“å­˜å¤±è´¥: {exc}")
            result = {'status': 'failed', 'superclass_name': name, 'error': str(exc)}
        results.append(result)

    success = sum(1 for r in results if r.get('status') == 'success')
    skipped = sum(1 for r in results if r.get('status') == 'skipped')
    failed = sum(1 for r in results if r.get('status') == 'failed')

    print("\n" + "=" * 80)
    print("ğŸ“ˆ æ‰¹é‡ç¼“å­˜ç»Ÿè®¡:")
    print(f"   æˆåŠŸ: {success} | è·³è¿‡: {skipped} | å¤±è´¥: {failed}")
    print("=" * 80)

    return results


def build_parser() -> argparse.ArgumentParser:
    """
    æ„å»ºå‘½ä»¤è¡Œè§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description='è¶…ç±»ç‰¹å¾ç¼“å­˜è„šæœ¬',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument('--superclass_name', type=str, default=None,
                        help="å•ä¸ªè¶…ç±»åç§°ï¼Œå¦‚'trees'")
    parser.add_argument('--all_superclasses', action='store_true',
                        help='æ˜¯å¦å¤„ç†å…¨éƒ¨15ä¸ªè¶…ç±»')

    parser.add_argument('--model_path', type=str, default=None,
                        help='æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--auto_find_best', type=str2bool, default=True,
                        help='æ˜¯å¦è‡ªåŠ¨æœç´¢ACCæœ€é«˜çš„æ¨¡å‹')
    parser.add_argument('--checkpoint_root', type=str, default=DEFAULT_CHECKPOINT_ROOT,
                        help='è¶…ç±»æ¨¡å‹checkpointæ ¹ç›®å½•')

    parser.add_argument('--cache_dir', type=str,
                        default=DEFAULT_FEATURE_CACHE_DIR,
                        help='ç‰¹å¾ç¼“å­˜è¾“å‡ºæ ¹ç›®å½•')
    parser.add_argument('--use_l2', type=str2bool, default=True,
                        help='æ˜¯å¦å¯¹ç‰¹å¾è¿›è¡ŒL2å½’ä¸€åŒ–åå†ç¼“å­˜')
    parser.add_argument('--overwrite', action='store_true',
                        help='æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç¼“å­˜')

    parser.add_argument('--batch_size', type=int, default=64,
                        help='ç‰¹å¾æå–æ‰¹å¤§å°ï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='DataLoaderçš„å·¥ä½œçº¿ç¨‹æ•°ï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰')
    parser.add_argument('--gpu', type=int, default=0,
                        help='ä½¿ç”¨çš„GPUç¼–å·')
    parser.add_argument('--prop_train_labels', type=float, default=0.8,
                        help='è®­ç»ƒé›†ä¸­æœ‰æ ‡ç­¾æ ·æœ¬å æ¯”')
    parser.add_argument('--seed', type=int, default=0,
                        help='æ•°æ®åˆ’åˆ†éšæœºç§å­ï¼ˆä¸test_feature.pyä¸€è‡´ï¼š0ï¼‰')
    parser.add_argument('--image_size', type=int, default=224,
                        help='å›¾åƒå¤§å°ï¼ˆä¸test_feature.pyä¸€è‡´ï¼‰')

    return parser


def main():
    parser = build_parser()
    args = parser.parse_args()

    if not args.all_superclasses and args.superclass_name is None:
        parser.error("å¿…é¡»æŒ‡å®š --superclass_name æˆ–å¼€å¯ --all_superclasses")

    common_kwargs = dict(
        model_path=args.model_path,
        auto_find_best=args.auto_find_best,
        checkpoint_root=args.checkpoint_root,
        cache_dir=args.cache_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        gpu=args.gpu,
        use_l2=args.use_l2,
        overwrite=args.overwrite,
        prop_train_labels=args.prop_train_labels,
        seed=args.seed,
        image_size=args.image_size
    )

    if args.all_superclasses:
        cache_all_superclasses(**common_kwargs)
    else:
        cache_single_superclass(superclass_name=args.superclass_name, **common_kwargs)

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")


if __name__ == '__main__':
    main()
