#!/usr/bin/env python3
"""
è®­ç»ƒæµæ°´çº¿æµ‹è¯•è„šæœ¬
æµ‹è¯•æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ•°æ®æµå’Œæ ‡ç­¾æ˜ å°„
æ¨¡æ‹Ÿè®­ç»ƒçš„å‰å‡ ä¸ªæ­¥éª¤ï¼Œç¡®ä¿æ‰€æœ‰ç»„ä»¶æ­£å¸¸å·¥ä½œ
"""

import sys
import os
import argparse
import numpy as np
import torch
from torch.utils.data import DataLoader
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.cifar100_superclass import (
    get_single_superclass_datasets,
    CIFAR100_SUPERCLASSES,
    get_superclass_splits
)
from data.augmentations import get_transform
from data.data_utils import MergedDataset
from methods.contrastive_training.contrastive_training import ContrastiveLearningViewGenerator
from copy import deepcopy

def test_training_pipeline(superclass_name='trees', batch_size=8, num_workers=0):
    """
    æµ‹è¯•è®­ç»ƒæµæ°´çº¿

    Args:
        superclass_name: è¶…ç±»åç§°
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ æµ‹è¯•è®­ç»ƒæµæ°´çº¿: {superclass_name}")
    print(f"{'='*60}")

    # 1. è®¾ç½®æ¨¡æ‹Ÿå‚æ•°
    class MockArgs:
        def __init__(self):
            self.superclass_name = superclass_name
            self.prop_train_labels = 0.8
            self.seed = 1
            self.n_views = 2
            # æ·»åŠ get_transforméœ€è¦çš„å±æ€§
            self.interpolation = 3
            self.crop_pct = 0.875
            # æ·»åŠ å…¶ä»–å¯èƒ½éœ€è¦çš„å±æ€§
            self.resize_lower_bound = 0.08
            self.rand_aug_n = 2
            self.rand_aug_m = 10

    args = MockArgs()

    # 2. è·å–ç±»åˆ«åˆ’åˆ†
    try:
        superclass_splits = get_superclass_splits()
        split_info = superclass_splits[superclass_name]

        args.train_classes = split_info['known_classes']
        args.unlabeled_classes = split_info['unknown_classes']

        print(f"âœ… ç±»åˆ«åˆ’åˆ†è·å–æˆåŠŸ")
        print(f"   å·²çŸ¥ç±»åˆ«: {args.train_classes}")
        print(f"   æœªçŸ¥ç±»åˆ«: {args.unlabeled_classes}")

    except Exception as e:
        print(f"âŒ ç±»åˆ«åˆ’åˆ†è·å–å¤±è´¥: {e}")
        return False

    # 3. åˆ›å»ºæ•°æ®è½¬æ¢
    try:
        # åˆ›å»ºç®€åŒ–çš„è½¬æ¢ï¼Œé¿å…å¤æ‚çš„ä¾èµ–
        import torch
        from torchvision import transforms
        from PIL import Image

        base_train_transform = transforms.Compose([
            transforms.Resize((224, 224), interpolation=Image.BICUBIC),
            transforms.ToTensor(),
        ])

        test_transform = transforms.Compose([
            transforms.Resize((224, 224), interpolation=Image.BICUBIC),
            transforms.ToTensor(),
        ])

        # åˆ›å»ºå¯¹æ¯”å­¦ä¹ è½¬æ¢
        train_transform = ContrastiveLearningViewGenerator(
            base_transform=base_train_transform,
            n_views=args.n_views
        )

        print(f"âœ… æ•°æ®è½¬æ¢åˆ›å»ºæˆåŠŸ")
        print(f"   å¯¹æ¯”å­¦ä¹ è§†å›¾æ•°: {args.n_views}")

    except Exception as e:
        print(f"âŒ æ•°æ®è½¬æ¢åˆ›å»ºå¤±è´¥: {e}")
        return False

    # 4. è·å–æ•°æ®é›†
    try:
        datasets = get_single_superclass_datasets(
            superclass_name=superclass_name,
            train_transform=train_transform,
            test_transform=test_transform,
            prop_train_labels=args.prop_train_labels,
            split_train_val=False,
            seed=args.seed
        )

        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸ")
        for split_name, dataset in datasets.items():
            if dataset is not None:
                print(f"   {split_name}: {len(dataset)} æ ·æœ¬")

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return False

    # 5. åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    try:
        train_dataset = MergedDataset(
            labelled_dataset=deepcopy(datasets['train_labelled']),
            unlabelled_dataset=deepcopy(datasets['train_unlabelled'])
        )

        test_dataset = datasets['test']
        unlabelled_train_examples_test = deepcopy(datasets['train_unlabelled'])
        unlabelled_train_examples_test.transform = test_transform

        print(f"âœ… è®­ç»ƒæ•°æ®é›†ç»„è£…æˆåŠŸ")
        print(f"   è®­ç»ƒé›†æ€»æ ·æœ¬: {len(train_dataset)}")
        print(f"   æµ‹è¯•é›†æ ·æœ¬: {len(test_dataset)}")
        print(f"   æœªæ ‡è®°è®­ç»ƒæ ·æœ¬: {len(unlabelled_train_examples_test)}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒæ•°æ®é›†ç»„è£…å¤±è´¥: {e}")
        return False

    # 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨å¹¶æµ‹è¯•
    try:
        # åˆ›å»ºé‡‡æ ·å™¨ï¼ˆå¹³è¡¡æ ‡è®°å’Œæœªæ ‡è®°æ ·æœ¬ï¼‰
        label_len = len(datasets['train_labelled'])
        unlabelled_len = len(datasets['train_unlabelled'])
        sample_weights = [1 if i < label_len else label_len / unlabelled_len for i in range(len(train_dataset))]
        sample_weights = torch.DoubleTensor(sample_weights)
        sampler = torch.utils.data.WeightedRandomSampler(sample_weights, num_samples=len(train_dataset))

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            num_workers=num_workers,
            batch_size=batch_size,
            shuffle=False,
            sampler=sampler,
            drop_last=True
        )

        test_loader = DataLoader(
            test_dataset,
            num_workers=num_workers,
            batch_size=batch_size,
            shuffle=False
        )

        unlabelled_loader = DataLoader(
            unlabelled_train_examples_test,
            num_workers=num_workers,
            batch_size=batch_size,
            shuffle=False
        )

        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"   æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        print(f"   æœªæ ‡è®°æ‰¹æ¬¡æ•°: {len(unlabelled_loader)}")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return False

    # 7. æµ‹è¯•è®­ç»ƒæ•°æ®æ‰¹æ¬¡
    print(f"\nğŸ” è®­ç»ƒæ•°æ®æ‰¹æ¬¡æµ‹è¯•:")
    try:
        for batch_idx, batch in enumerate(train_loader):
            if batch_idx >= 2:  # åªæµ‹è¯•å‰2ä¸ªæ‰¹æ¬¡
                break

            images, class_labels, uq_idxs, mask_lab = batch
            mask_lab = mask_lab[:, 0]

            print(f"   æ‰¹æ¬¡ {batch_idx}:")
            print(f"     å›¾åƒå½¢çŠ¶: {[img.shape for img in images]}")  # å¤šè§†å›¾
            print(f"     ç±»åˆ«æ ‡ç­¾å½¢çŠ¶: {class_labels.shape}, èŒƒå›´: [{class_labels.min()}, {class_labels.max()}]")
            print(f"     æ ‡è®°æ©ç : {mask_lab.sum().item()}/{len(mask_lab)} ä¸ªæ ·æœ¬è¢«æ ‡è®°")

            # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
            label_counts = Counter(class_labels.numpy())
            print(f"     æ ‡ç­¾åˆ†å¸ƒ: {dict(sorted(label_counts.items()))}")

            # æ£€æŸ¥å¯¹æ¯”å­¦ä¹ è§†å›¾
            if isinstance(images, list) and len(images) == 2:
                print(f"     âœ… å¯¹æ¯”å­¦ä¹ åŒè§†å›¾æ­£ç¡®")
            else:
                print(f"     âŒ å¯¹æ¯”å­¦ä¹ è§†å›¾å¼‚å¸¸: {type(images)}")

        print(f"   âœ… è®­ç»ƒæ•°æ®æ‰¹æ¬¡æµ‹è¯•é€šè¿‡")

    except Exception as e:
        print(f"   âŒ è®­ç»ƒæ•°æ®æ‰¹æ¬¡æµ‹è¯•å¤±è´¥: {e}")
        return False

    # 8. æµ‹è¯•æµ‹è¯•æ•°æ®æ‰¹æ¬¡
    print(f"\nğŸ” æµ‹è¯•æ•°æ®æ‰¹æ¬¡æµ‹è¯•:")
    try:
        for batch_idx, (images, labels, uq_idxs) in enumerate(test_loader):
            if batch_idx >= 1:  # åªæµ‹è¯•1ä¸ªæ‰¹æ¬¡
                break

            print(f"   æ‰¹æ¬¡ {batch_idx}:")
            print(f"     å›¾åƒå½¢çŠ¶: {images.shape}")
            print(f"     æ ‡ç­¾å½¢çŠ¶: {labels.shape}, èŒƒå›´: [{labels.min()}, {labels.max()}]")

            # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
            label_counts = Counter(labels.numpy())
            print(f"     æ ‡ç­¾åˆ†å¸ƒ: {dict(sorted(label_counts.items()))}")

        print(f"   âœ… æµ‹è¯•æ•°æ®æ‰¹æ¬¡æµ‹è¯•é€šè¿‡")

    except Exception as e:
        print(f"   âŒ æµ‹è¯•æ•°æ®æ‰¹æ¬¡æµ‹è¯•å¤±è´¥: {e}")
        return False

    # 9. éªŒè¯æ ‡ç­¾æ˜ å°„ä¸€è‡´æ€§
    print(f"\nğŸ” æ ‡ç­¾æ˜ å°„ä¸€è‡´æ€§éªŒè¯:")
    try:
        # æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ‡ç­¾
        all_train_labels = set()
        all_test_labels = set()

        # ä»è®­ç»ƒæ•°æ®ä¸­é‡‡æ ·æ ‡ç­¾
        sample_count = 0
        for batch in train_loader:
            if sample_count >= 50:  # é™åˆ¶é‡‡æ ·æ•°é‡
                break
            _, class_labels, _, _ = batch
            all_train_labels.update(class_labels.numpy())
            sample_count += len(class_labels)

        # ä»æµ‹è¯•æ•°æ®ä¸­é‡‡æ ·æ ‡ç­¾
        sample_count = 0
        for batch in test_loader:
            if sample_count >= 50:  # é™åˆ¶é‡‡æ ·æ•°é‡
                break
            _, labels, _ = batch
            all_test_labels.update(labels.numpy())
            sample_count += len(labels)

        print(f"   è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: {sorted(all_train_labels)}")
        print(f"   æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´: {sorted(all_test_labels)}")

        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦è¿ç»­ä¸”ä»0å¼€å§‹
        expected_labels = set(range(len(args.train_classes) + len(args.unlabeled_classes)))

        if all_train_labels.issubset(expected_labels) and all_test_labels.issubset(expected_labels):
            print(f"   âœ… æ ‡ç­¾æ˜ å°„ä¸€è‡´ï¼Œç¬¦åˆé¢„æœŸèŒƒå›´ [0, {len(expected_labels)-1}]")
        else:
            print(f"   âŒ æ ‡ç­¾æ˜ å°„å¼‚å¸¸:")
            print(f"     æœŸæœ›æ ‡ç­¾: {sorted(expected_labels)}")
            print(f"     è®­ç»ƒé›†å¤šä½™æ ‡ç­¾: {all_train_labels - expected_labels}")
            print(f"     æµ‹è¯•é›†å¤šä½™æ ‡ç­¾: {all_test_labels - expected_labels}")

    except Exception as e:
        print(f"   âŒ æ ‡ç­¾æ˜ å°„éªŒè¯å¤±è´¥: {e}")
        return False

    print(f"\nğŸ‰ è®­ç»ƒæµæ°´çº¿æµ‹è¯•å®Œæˆ!")
    print(f"âœ… è¶…ç±» '{superclass_name}' çš„è®­ç»ƒæµæ°´çº¿å·¥ä½œæ­£å¸¸")
    return True

def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•è®­ç»ƒæµæ°´çº¿')
    parser.add_argument('--superclass', type=str, default='trees',
                        help='è¦æµ‹è¯•çš„è¶…ç±»åç§°ï¼Œé»˜è®¤ä¸ºtrees')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º8')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸º0')

    args = parser.parse_args()

    print("ğŸš€ è®­ç»ƒæµæ°´çº¿æµ‹è¯•å·¥å…·")
    print("=" * 60)

    success = test_training_pipeline(
        superclass_name=args.superclass,
        batch_size=args.batch_size,
        num_workers=args.num_workers
    )

    if success:
        print(f"\nğŸŠ æµ‹è¯•æˆåŠŸ! å¯ä»¥å¼€å§‹è®­ç»ƒè¶…ç±» '{args.superclass}'")
        print(f"ğŸ’¡ å»ºè®®çš„è®­ç»ƒå‘½ä»¤:")
        print(f"   python scripts/train_superclass.py --superclass_name {args.superclass} --epochs 20 --gpu 0")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥! è¯·æ£€æŸ¥æ•°æ®é›†é…ç½®")

if __name__ == "__main__":
    main()