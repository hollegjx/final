#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•å®ç°
åŸºäºéªŒè¯çš„æ•°æ®åŠ è½½æ–¹æ¡ˆï¼Œå®ç°5æ­¥è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•ï¼š
1. å¯†åº¦è®¡ç®—: ä½¿ç”¨è‡ªé€‚åº”å¸¦å®½çš„é«˜æ–¯æ ¸ä¼°è®¡æ¯ä¸ªæ ·æœ¬çš„å±€éƒ¨å¯†åº¦
2. é«˜å¯†åº¦ç‚¹è¯†åˆ«: é€‰æ‹©75åˆ†ä½æ•°ä»¥ä¸Šçš„ç‚¹ä½œä¸ºèšç±»ç§å­
3. èšç±»æ„å»º: ä»é«˜å¯†åº¦ç‚¹å¼€å§‹ï¼Œé€šè¿‡kè¿‘é‚»æ‰©å±•å½¢æˆèšç±»
4. å†²çªå¤„ç†: æ ¹æ®å·²çŸ¥æ ‡ç­¾ä¿¡æ¯è§£å†³èšç±»è¾¹ç•Œå†²çª
5. ç¨€ç–ç‚¹åˆ†é…: å°†å‰©ä½™ç‚¹åˆ†é…ç»™æœ€è¿‘èšç±»æˆ–å½¢æˆå•ç‚¹èšç±»
"""

import sys
import os
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
from sklearn.cluster import KMeans
from tqdm import tqdm
import argparse
from copy import deepcopy

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.get_datasets import get_datasets, get_class_splits
from data.augmentations import get_transform
from data.cifar100_superclass import CIFAR100_SUPERCLASSES, get_single_superclass_datasets
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool
from data.data_utils import MergedDataset
from project_utils.cluster_utils import cluster_acc


def load_model(args, device):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ¨¡ä»¿eval_original_gcd.pyï¼‰
    """
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹æ–‡ä»¶: {args.model_path}")
    print(f"   è®¾å¤‡: {device}")

    # æ„å»ºbase model
    if args.base_model == 'vit_dino':
        model = vits.__dict__['vit_base']()

        # åŠ è½½DINOé¢„è®­ç»ƒæƒé‡
        if os.path.exists(dino_pretrain_path):
            print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡...")
            dino_state_dict = torch.load(dino_pretrain_path, map_location='cpu')
            model.load_state_dict(dino_state_dict, strict=False)

        # åŠ è½½è®­ç»ƒæƒé‡
        print(f"   åŠ è½½è®­ç»ƒæƒé‡...")
        gcd_state_dict = torch.load(args.model_path, map_location='cpu')
        model.load_state_dict(gcd_state_dict)

        model.to(device)

        # å…³é—­æ¢¯åº¦è®¡ç®—
        for param in model.parameters():
            param.requires_grad = False

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (ç‰¹å¾ç»´åº¦: {args.feat_dim})")
        return model
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.base_model}")


def extract_features(data_loader, model, device, known_classes=None, use_train_and_test=True):
    """
    æå–ç‰¹å¾ï¼ˆæ¨¡ä»¿kmeansçš„ç‰¹å¾æå–æ–¹å¼ï¼‰

    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        model: ç‰¹å¾æå–æ¨¡å‹
        device: è®¾å¤‡
        use_train_and_test: æ˜¯å¦ä½¿ç”¨è®­ç»ƒé›†+æµ‹è¯•é›†ï¼ŒFalseåˆ™åªç”¨æµ‹è¯•é›†

    Returns:
        features: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
        targets: çœŸå®æ ‡ç­¾ (ä»…ç”¨äºè¯„ä¼°)
        mask: å·²çŸ¥ç±»åˆ«æ©ç  (True=å·²çŸ¥ç±», False=æœªçŸ¥ç±»)
        labeled_mask: æœ‰æ ‡ç­¾æ©ç  (True=æœ‰æ ‡ç­¾, False=æ— æ ‡ç­¾)
    """
    print(f"ğŸ”„ æå–ç‰¹å¾...")

    model.eval()
    all_feats = []
    targets = np.array([])
    mask = np.array([])  # å·²çŸ¥ç±»åˆ«æ©ç 
    labeled_mask = np.array([])  # æœ‰æ ‡ç­¾æ©ç 

    with torch.no_grad():
        for batch_idx, batch_data in enumerate(tqdm(data_loader, desc="æå–ç‰¹å¾")):
            # è§£åŒ…æ•°æ®
            if len(batch_data) == 4:
                images, labels, indices, labeled_or_not = batch_data
                labeled_batch = labeled_or_not.numpy().flatten().astype(bool)
            elif len(batch_data) == 3:
                images, labels, indices = batch_data
                # æµ‹è¯•é›†å…¨éƒ¨æ ‡è®°ä¸ºæ— æ ‡ç­¾
                labeled_batch = np.zeros(len(labels), dtype=bool)
            else:
                continue

            # æå–ç‰¹å¾
            images = images.to(device)
            feats = model(images)
            feats = torch.nn.functional.normalize(feats, dim=-1)

            # æ”¶é›†æ•°æ®
            all_feats.append(feats.cpu().numpy())
            targets = np.append(targets, labels.cpu().numpy())
            labeled_mask = np.append(labeled_mask, labeled_batch)

            # åˆ›å»ºå·²çŸ¥ç±»åˆ«æ©ç ï¼ˆæ ¹æ®known_classesåˆ—è¡¨ï¼‰
            if known_classes is not None:
                batch_mask = np.array([True if x.item() in known_classes else False for x in labels])
            else:
                # é»˜è®¤ï¼šå‰80ä¸ªç±»åˆ«æ˜¯å·²çŸ¥ç±»
                batch_mask = np.array([True if x.item() < 80 else False for x in labels])
            mask = np.append(mask, batch_mask)

            # æ¸…ç†GPUå†…å­˜
            del images, feats
            torch.cuda.empty_cache()

    # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    print(f"âœ… ç‰¹å¾æå–å®Œæˆ: {all_feats.shape}")

    return all_feats, targets.astype(int), mask.astype(bool), labeled_mask.astype(bool)


def compute_simple_density(X, k=10):
    """
    æ­¥éª¤1: ä½¿ç”¨ç®€å•kè¿‘é‚»å¹³å‡è·ç¦»å€’æ•°è®¡ç®—å¯†åº¦

    SS-DDBCåªéœ€è¦èƒ½å¤Ÿæ¯”è¾ƒå¯†åº¦å¤§å°ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•å³å¯ï¼š
    å¯†åº¦ = 1 / kè¿‘é‚»å¹³å‡è·ç¦»

    Args:
        X: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
        k: kè¿‘é‚»æ•°é‡

    Returns:
        densities: æ¯ä¸ªæ ·æœ¬çš„å¯†åº¦å€¼
        knn_distances: kè¿‘é‚»è·ç¦»çŸ©é˜µ
        neighbors: kè¿‘é‚»ç´¢å¼•çŸ©é˜µ
    """
    print(f"ğŸ“Š è®¡ç®—ç®€åŒ–å¯†åº¦ (k={k})...")

    # è®¡ç®—kè¿‘é‚»
    nbrs = NearestNeighbors(n_neighbors=k+1, metric='euclidean').fit(X)
    knn_distances, neighbors = nbrs.kneighbors(X)

    # å»é™¤è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªé‚»å±…æ˜¯è‡ªå·±ï¼‰
    knn_distances = knn_distances[:, 1:]
    neighbors = neighbors[:, 1:]

    # ç®€å•å¯†åº¦è®¡ç®—ï¼škè¿‘é‚»å¹³å‡è·ç¦»çš„å€’æ•°
    avg_distances = np.mean(knn_distances, axis=1)
    densities = 1.0 / (avg_distances + 1e-8)  # é¿å…é™¤é›¶

    print(f"   å¯†åº¦ç»Ÿè®¡: min={densities.min():.3f}, max={densities.max():.3f}, mean={densities.mean():.3f}")

    return densities, knn_distances, neighbors


def identify_high_density_points(densities, percentile=75):
    """
    æ­¥éª¤2: é€‰æ‹©75åˆ†ä½æ•°ä»¥ä¸Šçš„ç‚¹ä½œä¸ºèšç±»ç§å­

    Args:
        densities: å¯†åº¦å€¼æ•°ç»„
        percentile: ç™¾åˆ†ä½æ•°é˜ˆå€¼

    Returns:
        high_density_mask: é«˜å¯†åº¦ç‚¹æ©ç 
    """
    density_threshold = np.percentile(densities, percentile)
    high_density_mask = densities >= density_threshold

    print(f"ğŸ“ è¯†åˆ«é«˜å¯†åº¦ç‚¹:")
    print(f"   å¯†åº¦é˜ˆå€¼: {density_threshold:.3f} (ç¬¬{percentile}ç™¾åˆ†ä½æ•°)")
    print(f"   é«˜å¯†åº¦ç‚¹æ•°é‡: {np.sum(high_density_mask)} / {len(densities)}")

    return high_density_mask


def ssddbc_conflict_resolution(xi_idx, xj_idx, xi_cluster_center, xj_cluster_center, X, densities):
    """
    SS-DDBCå†²çªè§£å†³ç®—æ³•

    å†²çªè§£å†³æµç¨‹:
    1. æ¯”è¾ƒxiå’Œxjçš„å¯†åº¦
    2. è®¡ç®—å†²çªç‚¹ä¸xiå’Œxjçš„è·ç¦»
    3. å†²çªç‚¹è·ç¦»xiè¾ƒè¿‘ä¸”xiå¯†åº¦è¾ƒå¤§ï¼Œåˆ™æŠŠå†²çªç‚¹é‡æ–°åˆ†é…ç»™xiï¼Œå¦åˆ™ä¸åšé‡æ–°åˆ†é…

    Args:
        xi_idx: å‚è€ƒç‚¹ç´¢å¼•
        xj_idx: å†²çªç‚¹ç´¢å¼•
        xi_cluster_center: xiæ‰€åœ¨èšç±»çš„ä¸­å¿ƒ
        xj_cluster_center: xjæ‰€åœ¨èšç±»çš„ä¸­å¿ƒ
        X: ç‰¹å¾çŸ©é˜µ
        densities: å¯†åº¦æ•°ç»„

    Returns:
        should_reassign: æ˜¯å¦åº”è¯¥é‡æ–°åˆ†é…å†²çªç‚¹
    """
    # 1. æ¯”è¾ƒxiå’Œxjçš„å¯†åº¦
    xi_density = densities[xi_idx]
    xj_density = densities[xj_idx]

    # 2. è®¡ç®—å†²çªç‚¹xjåˆ°ä¸¤ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»
    xj_pos = X[xj_idx]
    distance_to_xi_cluster = np.linalg.norm(xj_pos - xi_cluster_center)
    distance_to_xj_cluster = np.linalg.norm(xj_pos - xj_cluster_center)

    # 3. åˆ¤æ–­æ¡ä»¶ï¼šå†²çªç‚¹è·ç¦»xiè¾ƒè¿‘ä¸”xiå¯†åº¦è¾ƒå¤§
    closer_to_xi = distance_to_xi_cluster < distance_to_xj_cluster
    xi_density_higher = xi_density > xj_density

    if closer_to_xi and xi_density_higher:
        return True  # ä¸¤ä¸ªæ¡ä»¶éƒ½æ»¡è¶³ï¼Œé‡æ–°åˆ†é…xjç»™xi
    else:
        return False  # ä¸é‡æ–°åˆ†é…



def build_clusters_ssddbc(X, high_density_mask, neighbors, labeled_mask, targets, densities, known_mask, k=10):
    """
    å®Œå…¨æŒ‰ç…§SS-DDBCç®—æ³•æ„å»ºèšç±»
    ç®—æ³•æµç¨‹:
    For each high-density point xi:
        If xi is not assigned to any cluster, create a new cluster pi
        For each neighbor xj in k-neighbors of xi:
            If xj is not assigned to any cluster, add xj to pi
            Else If xj âˆˆ pj (si â‰  sj) and xj âˆˆ C (æœ‰æ ‡ç­¾å†²çª)
                perform Algorithm 3 for conflict resolution
            Else (si = sj æˆ–è€…æœ‰çš„ç°‡æ— æ ‡ç­¾)
                merge pi and pj

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        high_density_mask: é«˜å¯†åº¦ç‚¹æ©ç 
        neighbors: kè¿‘é‚»ç´¢å¼•çŸ©é˜µ
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        densities: å¯†åº¦æ•°ç»„
        known_mask: å·²çŸ¥ç±»æ©ç 
        k: æ‰©å±•æ—¶çš„è¿‘é‚»æ•°

    Returns:
        clusters: èšç±»åˆ—è¡¨ï¼Œæ¯ä¸ªèšç±»æ˜¯ä¸€ä¸ªæ ·æœ¬ç´¢å¼•é›†åˆ
        cluster_labels: æ¯ä¸ªæ ·æœ¬çš„èšç±»æ ‡ç­¾ (-1è¡¨ç¤ºæœªåˆ†é…)
    """
    print(f"SS-DDBCèšç±»æ„å»º...")

    n_samples = X.shape[0]
    cluster_labels = np.full(n_samples, -1, dtype=int)
    clusters = []
    current_cluster_id = 0

    # ä»æ¯ä¸ªé«˜å¯†åº¦ç‚¹å¼€å§‹æ„å»ºèšç±»
    high_density_indices = np.where(high_density_mask)[0]

    # ğŸ” è°ƒè¯•ï¼šåˆ†æé«˜å¯†åº¦ç‚¹çš„ç±»åˆ«åˆ†å¸ƒ
    print(f"\nğŸ” é«˜å¯†åº¦ç‚¹ç±»åˆ«åˆ†å¸ƒåˆ†æ:")
    known_high_density = 0
    unknown_high_density = 0
    for idx in high_density_indices:
        if known_mask[idx]:
            known_high_density += 1
        else:
            unknown_high_density += 1
    print(f"   å·²çŸ¥ç±»é«˜å¯†åº¦ç‚¹: {known_high_density}ä¸ª")
    print(f"   æœªçŸ¥ç±»é«˜å¯†åº¦ç‚¹: {unknown_high_density}ä¸ª")

    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    class_high_density_count = {}
    for idx in high_density_indices:
        true_label = targets[idx]
        if true_label not in class_high_density_count:
            class_high_density_count[true_label] = 0
        class_high_density_count[true_label] += 1
    print(f"   å„ç±»åˆ«é«˜å¯†åº¦ç‚¹æ•°: {class_high_density_count}")

    for xi_idx in high_density_indices:
        if cluster_labels[xi_idx] != -1:
            # ğŸ” è°ƒè¯•ï¼šè·Ÿè¸ªè¢«è·³è¿‡çš„é«˜å¯†åº¦ç‚¹
            xi_true_label = targets[xi_idx]
            xi_is_known = known_mask[xi_idx]
            assigned_cluster_id = cluster_labels[xi_idx]
            print(f"   â­ï¸ è·³è¿‡é«˜å¯†åº¦ç‚¹{xi_idx} (çœŸå®æ ‡ç­¾={xi_true_label}, å·²çŸ¥ç±»={xi_is_known}) - å·²åˆ†é…ç»™èšç±»{assigned_cluster_id}")
            continue  # å·²ç»è¢«åˆ†é…åˆ°å…¶ä»–èšç±»

        # If xi is not assigned to any cluster, create a new cluster pi
        cluster_pi = set([xi_idx])
        cluster_labels[xi_idx] = current_cluster_id
        queue = [xi_idx]

        # ğŸ” è°ƒè¯•ï¼šè·Ÿè¸ªèšç±»åˆ›å»º
        xi_true_label = targets[xi_idx]
        xi_is_known = known_mask[xi_idx]
        print(f"\nğŸ”¹ åˆ›å»ºèšç±»{current_cluster_id}: ç§å­ç‚¹{xi_idx} (çœŸå®æ ‡ç­¾={xi_true_label}, å·²çŸ¥ç±»={xi_is_known})")

        # BFSæ‰©å±•èšç±»
        while queue:
            current_idx = queue.pop(0)

            # åªæœ‰é«˜å¯†åº¦ç‚¹æ‰èƒ½æ‰©å±•å…¶ä»–ç‚¹
            if high_density_mask[current_idx]:
                # For each neighbor xj in k-neighbors of xi
                for xj_idx in neighbors[current_idx]:
                    xj_cluster_id = cluster_labels[xj_idx]

                    if xj_cluster_id == -1:
                        # If xj is not assigned to any cluster, add xj to pi
                        cluster_pi.add(xj_idx)
                        cluster_labels[xj_idx] = current_cluster_id
                        queue.append(xj_idx)

                    elif xj_cluster_id != current_cluster_id:
                        # xj âˆˆ pjï¼Œéœ€è¦åˆ¤æ–­æƒ…å†µ
                        xi_has_label = labeled_mask[current_idx]
                        xj_has_label = labeled_mask[xj_idx]

                        # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡ç­¾å†²çª (si â‰  sj) and xj âˆˆ C
                        has_conflict = False
                        if xi_has_label and xj_has_label:
                            xi_label = targets[current_idx]
                            xj_label = targets[xj_idx]
                            if xi_label != xj_label:
                                has_conflict = True

                        if has_conflict:
                            # Else If xj âˆˆ pj (si â‰  sj) and xj âˆˆ C
                            # perform Algorithm 3 for conflict resolution

                            # è®¡ç®—èšç±»ä¸­å¿ƒç”¨äºå†²çªè§£å†³
                            xi_cluster_center = np.mean(X[list(cluster_pi)], axis=0)
                            if xj_cluster_id < len(clusters) and len(clusters[xj_cluster_id]) > 0:
                                xj_cluster_center = np.mean(X[list(clusters[xj_cluster_id])], axis=0)

                                should_reassign = ssddbc_conflict_resolution(
                                    current_idx, xj_idx, xi_cluster_center, xj_cluster_center, X, densities
                                )

                                if should_reassign:
                                    # é‡æ–°åˆ†é…xjåˆ°å½“å‰èšç±»pi
                                    clusters[xj_cluster_id].discard(xj_idx)
                                    cluster_pi.add(xj_idx)
                                    cluster_labels[xj_idx] = current_cluster_id
                                    queue.append(xj_idx)
                                    print(f"   å†²çªè§£å†³: ç‚¹{xj_idx}ä»èšç±»{xj_cluster_id}é‡åˆ†é…åˆ°èšç±»{current_cluster_id}")
                            # å¦åˆ™ä¸åšé‡æ–°åˆ†é…

                        else:
                            # Else (si = sj æˆ–è€…æœ‰çš„ç°‡æ— æ ‡ç­¾) merge pi and pj
                            # æ— å†²çªï¼Œå¯ä»¥åˆå¹¶èšç±»
                            if xj_cluster_id < len(clusters) and len(clusters[xj_cluster_id]) > 0:
                                # ğŸ” è°ƒè¯•ï¼šè·Ÿè¸ªèšç±»åˆå¹¶
                                cluster_pj = clusters[xj_cluster_id]

                                # åˆ†æè¢«åˆå¹¶èšç±»çš„ç»„æˆ
                                merged_known_count = sum(1 for idx in cluster_pj if known_mask[idx])
                                merged_unknown_count = len(cluster_pj) - merged_known_count

                                # åˆ†æå½“å‰èšç±»çš„ç»„æˆ
                                current_known_count = sum(1 for idx in cluster_pi if known_mask[idx])
                                current_unknown_count = len(cluster_pi) - current_known_count

                                print(f"   ğŸ”„ åˆå¹¶èšç±»: èšç±»{xj_cluster_id}(å·²çŸ¥:{merged_known_count},æœªçŸ¥:{merged_unknown_count}) â†’ èšç±»{current_cluster_id}(å·²çŸ¥:{current_known_count},æœªçŸ¥:{current_unknown_count})")

                                # åˆå¹¶èšç±»pjåˆ°pi
                                for idx in cluster_pj:
                                    cluster_pi.add(idx)
                                    cluster_labels[idx] = current_cluster_id
                                    if high_density_mask[idx]:  # å¦‚æœæ˜¯é«˜å¯†åº¦ç‚¹ï¼ŒåŠ å…¥æ‰©å±•é˜Ÿåˆ—
                                        queue.append(idx)
                                clusters[xj_cluster_id] = set()  # æ¸…ç©ºè¢«åˆå¹¶çš„èšç±»

        clusters.append(cluster_pi)
        current_cluster_id += 1

    # ç§»é™¤ç©ºèšç±»å¹¶é‡æ–°ç¼–å·
    non_empty_clusters = [c for c in clusters if len(c) > 0]
    cluster_labels_new = np.full(n_samples, -1, dtype=int)

    for new_id, cluster in enumerate(non_empty_clusters):
        for idx in cluster:
            cluster_labels_new[idx] = new_id

    print(f"   SS-DDBCèšç±»æ•°é‡: {len(non_empty_clusters)}")
    print(f"   å·²åˆ†é…æ ·æœ¬: {np.sum(cluster_labels_new != -1)} / {n_samples}")

    return non_empty_clusters, cluster_labels_new


def analyze_ssddbc_clustering_result(clusters, cluster_labels, labeled_mask, targets, known_mask):
    """
    åˆ†æSS-DDBCèšç±»æ„å»ºæ­¥éª¤çš„ç»“æœ

    Args:
        clusters: èšç±»åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»æ©ç 
    """
    print(f"\nğŸ“Š SS-DDBCèšç±»æ„å»ºç»“æœåˆ†æ:")
    print("="*80)

    total_samples = len(cluster_labels)
    assigned_samples = np.sum(cluster_labels != -1)
    unassigned_samples = total_samples - assigned_samples

    print(f"æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   å·²åˆ†é…æ ·æœ¬: {assigned_samples}")
    print(f"   æœªåˆ†é…æ ·æœ¬: {unassigned_samples}")
    print(f"   èšç±»æ•°é‡: {len(clusters)}")

    print(f"\nå„èšç±»è¯¦ç»†åˆ†æ:")

    for cluster_id, cluster in enumerate(clusters):
        cluster_indices = list(cluster)
        cluster_size = len(cluster_indices)

        # ç»Ÿè®¡æœ‰æ ‡ç­¾/æ— æ ‡ç­¾æ ·æœ¬
        cluster_labeled_mask = labeled_mask[cluster_indices]
        labeled_count = np.sum(cluster_labeled_mask)
        unlabeled_count = cluster_size - labeled_count

        # ç»Ÿè®¡å·²çŸ¥ç±»/æœªçŸ¥ç±»æ ·æœ¬
        cluster_known_mask = known_mask[cluster_indices]
        known_count = np.sum(cluster_known_mask)
        unknown_count = cluster_size - known_count

        # åˆ†ææ ‡ç­¾åˆ†å¸ƒï¼ˆæœ‰æ ‡ç­¾æ ·æœ¬ï¼‰
        label_distribution = {}
        dominant_label = None
        label_purity = 0.0

        if labeled_count > 0:
            labeled_targets = targets[cluster_indices][cluster_labeled_mask]
            unique_labels, counts = np.unique(labeled_targets, return_counts=True)
            label_distribution = dict(zip(unique_labels, counts))
            dominant_label = unique_labels[np.argmax(counts)]
            label_purity = np.max(counts) / labeled_count

        # åˆ†æçœŸå®ç±»åˆ«åˆ†å¸ƒï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰
        all_targets = targets[cluster_indices]
        all_unique_labels, all_counts = np.unique(all_targets, return_counts=True)
        true_distribution = dict(zip(all_unique_labels, all_counts))

        # è¾“å‡ºèšç±»ä¿¡æ¯
        print(f"\nèšç±» #{cluster_id} (å¤§å°: {cluster_size}):")
        print(f"   æ ·æœ¬ç»„æˆ:")
        print(f"     æœ‰æ ‡ç­¾æ ·æœ¬: {labeled_count} ä¸ª")
        print(f"     æ— æ ‡ç­¾æ ·æœ¬: {unlabeled_count} ä¸ª")
        print(f"     å·²çŸ¥ç±»æ ·æœ¬: {known_count} ä¸ª")
        print(f"     æœªçŸ¥ç±»æ ·æœ¬: {unknown_count} ä¸ª")

        if labeled_count > 0:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ:")
            print(f"     ä¸»å¯¼æ ‡ç­¾: {dominant_label} (çº¯åº¦: {label_purity:.3f})")
            print(f"     è¯¦ç»†åˆ†å¸ƒ: {label_distribution}")

            # æ£€æŸ¥æ ‡ç­¾å†²çª
            if len(label_distribution) > 1:
                print(f"     âš ï¸  æ ‡ç­¾å†²çª: åŒ…å«{len(label_distribution)}ç§ä¸åŒæ ‡ç­¾")
        else:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: æ— æœ‰æ ‡ç­¾æ ·æœ¬ (æ½œåœ¨æœªçŸ¥ç±»)")

        print(f"   çœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬): {true_distribution}")

    # æœªåˆ†é…æ ·æœ¬åˆ†æ
    if unassigned_samples > 0:
        print(f"\næœªåˆ†é…æ ·æœ¬åˆ†æ ({unassigned_samples}ä¸ª):")
        unassigned_indices = np.where(cluster_labels == -1)[0]
        unassigned_labeled = np.sum(labeled_mask[unassigned_indices])
        unassigned_unlabeled = unassigned_samples - unassigned_labeled
        unassigned_known = np.sum(known_mask[unassigned_indices])
        unassigned_unknown = unassigned_samples - unassigned_known

        print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {unassigned_labeled} ä¸ª")
        print(f"   æ— æ ‡ç­¾æ ·æœ¬: {unassigned_unlabeled} ä¸ª")
        print(f"   å·²çŸ¥ç±»æ ·æœ¬: {unassigned_known} ä¸ª")
        print(f"   æœªçŸ¥ç±»æ ·æœ¬: {unassigned_unknown} ä¸ª")

        if unassigned_labeled > 0:
            unassigned_targets = targets[unassigned_indices][labeled_mask[unassigned_indices]]
            unassigned_unique, unassigned_counts = np.unique(unassigned_targets, return_counts=True)
            unassigned_distribution = dict(zip(unassigned_unique, unassigned_counts))
            print(f"   æœªåˆ†é…æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: {unassigned_distribution}")

    print("="*80)


def identify_unknown_clusters(clusters, labeled_mask):
    """
    è¯†åˆ«æ½œåœ¨æœªçŸ¥ç±»èšç±»

    æ ¹æ®SS-DDBC: "ç°‡çš„ç±»åˆ«å–å†³äºå…¶æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œä¸å«æœ‰æ ‡ç­¾æ ·æœ¬çš„å¯èƒ½æ˜¯æœªçŸ¥ç±»"

    Args:
        clusters: èšç±»åˆ—è¡¨
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 

    Returns:
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•åˆ—è¡¨
    """
    unknown_clusters = []

    for cluster_id, cluster in enumerate(clusters):
        cluster_indices = list(cluster)
        cluster_labeled_mask = labeled_mask[cluster_indices]

        # å¦‚æœèšç±»ä¸­æ²¡æœ‰æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œæ ‡è®°ä¸ºæ½œåœ¨æœªçŸ¥ç±»
        if not np.any(cluster_labeled_mask):
            unknown_clusters.append(cluster_id)

    return unknown_clusters


def test_kmeans_baseline(test_features, test_targets, test_known_mask, n_clusters, random_state=0):
    """
    K-meansåŸºçº¿å¯¹æ¯”æ–¹æ¡ˆ

    å®Œå…¨å‚è€ƒeval_original_gcd.pyçš„å®ç°ï¼Œä¿æŒä¸€è‡´æ€§

    Args:
        test_features: æµ‹è¯•é›†ç‰¹å¾ (n_test_samples, feat_dim) - åº”è¯¥æ˜¯L2å½’ä¸€åŒ–çš„
        test_targets: æµ‹è¯•é›†çœŸå®æ ‡ç­¾
        test_known_mask: æµ‹è¯•é›†å·²çŸ¥ç±»æ©ç 
        n_clusters: èšç±»æ•°é‡
        random_state: éšæœºç§å­

    Returns:
        kmeans_results: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
    """
    print(f"\nğŸ”„ è¿è¡ŒK-meansåŸºçº¿å¯¹æ¯” (ä¸eval_original_gcdä¿æŒä¸€è‡´)...")
    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_features)}")
    print(f"   èšç±»æ•°é‡: {n_clusters}")
    print(f"   éšæœºç§å­: {random_state}")

    # è¿è¡ŒK-meansèšç±» (ä¸åŸç‰ˆå®Œå…¨ä¸€è‡´)
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    kmeans_predictions = kmeans.fit_predict(test_features)

    # è®¡ç®—å„ç§ACCæŒ‡æ ‡ (åªåœ¨æµ‹è¯•é›†ä¸Š)
    # ä½¿ç”¨ä¸eval_original_gcd.pyç›¸åŒçš„ACCè®¡ç®—æ–¹æ³• (split_cluster_acc_v1)
    from project_utils.cluster_and_log_utils import split_cluster_acc_v1
    all_acc, old_acc, new_acc = split_cluster_acc_v1(test_targets, kmeans_predictions, test_known_mask)


    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    nmi = normalized_mutual_info_score(test_targets, kmeans_predictions)
    ari = adjusted_rand_score(test_targets, kmeans_predictions)

    print(f"âœ… K-meansç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")
    print(f"   NMI: {nmi:.4f}")
    print(f"   ARI: {ari:.4f}")

    return {
        'method': 'K-means',
        'n_clusters': n_clusters,
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'nmi': nmi,
        'ari': ari
    }


def build_prototypes(X, clusters, labeled_mask, targets):
    """
    åŸºäºpartial-clusteringç»“æœå»ºç«‹åŸå‹

    SS-DDBCæ­¥éª¤: (2) åŸºäºpartial-clusteringçš„ç»“æœå»ºç«‹åŸå‹

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        clusters: èšç±»åˆ—è¡¨
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾

    Returns:
        prototypes: æ¯ä¸ªèšç±»çš„åŸå‹ (èšç±»ä¸­å¿ƒ)
        prototype_labels: æ¯ä¸ªèšç±»çš„ä¸»å¯¼æ ‡ç­¾
    """
    print(f"   å»ºç«‹èšç±»åŸå‹...")

    prototypes = []
    prototype_labels = []

    for cluster_id, cluster in enumerate(clusters):
        cluster_indices = list(cluster)

        # è®¡ç®—èšç±»ä¸­å¿ƒä½œä¸ºåŸå‹
        prototype = np.mean(X[cluster_indices], axis=0)
        prototypes.append(prototype)

        # ç¡®å®šèšç±»çš„ä¸»å¯¼æ ‡ç­¾
        cluster_labeled_mask = labeled_mask[cluster_indices]
        if np.any(cluster_labeled_mask):
            # å¦‚æœæœ‰æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œä½¿ç”¨ä¸»å¯¼æ ‡ç­¾
            labeled_targets = targets[cluster_indices][cluster_labeled_mask]
            unique_labels, counts = np.unique(labeled_targets, return_counts=True)
            dominant_label = unique_labels[np.argmax(counts)]
            prototype_labels.append(dominant_label)
        else:
            # æ— æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œæ ‡è®°ä¸ºæœªçŸ¥ç±» (-1)
            prototype_labels.append(-1)

    print(f"   å»ºç«‹åŸå‹å®Œæˆ: {len(prototypes)}ä¸ªåŸå‹")
    return np.array(prototypes), np.array(prototype_labels)


def assign_sparse_points_ssddbc(X, clusters, cluster_labels, prototypes, neighbors, labeled_mask, targets, lambda_weight=0.7):
    """
    SS-DDBCç¨€ç–ç‚¹åˆ†é…ç®—æ³•

    SS-DDBCæ­¥éª¤: (3) ç»“åˆåŸå‹è·ç¦»å’Œè¿‘é‚»æƒ…å†µï¼Œæ ‡æ³¨å‰©ä½™çš„ç¨€ç–ç‚¹

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        clusters: èšç±»åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        prototypes: èšç±»åŸå‹
        neighbors: kè¿‘é‚»ç´¢å¼•çŸ©é˜µ
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        lambda_weight: åŸå‹è·ç¦»æƒé‡

    Returns:
        final_labels: æœ€ç»ˆèšç±»æ ‡ç­¾
    """
    print(f"ç¨€ç–ç‚¹åˆ†é… (ç»“åˆåŸå‹è·ç¦»å’Œè¿‘é‚»æƒ…å†µ)...")

    final_labels = cluster_labels.copy()
    unassigned_indices = np.where(cluster_labels == -1)[0]

    if len(unassigned_indices) == 0:
        print(f"   æ— éœ€åˆ†é…ç¨€ç–ç‚¹")
        return final_labels

    print(f"   åˆ†é…{len(unassigned_indices)}ä¸ªç¨€ç–ç‚¹")

    # åˆ†é…æœªåˆ†é…çš„ç‚¹
    for point_idx in unassigned_indices:
        point_features = X[point_idx]

        # 1. è®¡ç®—åˆ°å„ä¸ªåŸå‹çš„è·ç¦»
        distances_to_prototypes = []
        for prototype in prototypes:
            distance = np.linalg.norm(point_features - prototype)
            distances_to_prototypes.append(distance)

        # 2. åˆ†ækè¿‘é‚»çš„èšç±»åˆ†å¸ƒ
        point_neighbors = neighbors[point_idx]
        neighbor_clusters = cluster_labels[point_neighbors]
        neighbor_clusters = neighbor_clusters[neighbor_clusters != -1]  # æ’é™¤æœªåˆ†é…çš„é‚»å±…

        neighbor_confidence = np.zeros(len(prototypes))
        if len(neighbor_clusters) > 0:
            unique_clusters, counts = np.unique(neighbor_clusters, return_counts=True)
            for cluster_id, count in zip(unique_clusters, counts):
                if cluster_id < len(prototypes):
                    neighbor_confidence[cluster_id] = count / len(neighbor_clusters)

        # 3. ç»“åˆåŸå‹è·ç¦»å’Œè¿‘é‚»æƒ…å†µ
        if len(neighbor_clusters) > 0:
            # è·ç¦»ç½®ä¿¡åº¦ (è·ç¦»è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜)
            max_distance = np.max(distances_to_prototypes)
            if max_distance > 0:
                distance_confidence = 1 - (np.array(distances_to_prototypes) / max_distance)
            else:
                distance_confidence = np.ones(len(prototypes))

            # ç»¼åˆç½®ä¿¡åº¦: lambda * è·ç¦»ç½®ä¿¡åº¦ + (1-lambda) * é‚»å±…ç½®ä¿¡åº¦
            combined_confidence = lambda_weight * distance_confidence + (1 - lambda_weight) * neighbor_confidence
            best_cluster = np.argmax(combined_confidence)
        else:
            # æ²¡æœ‰å·²åˆ†é…çš„é‚»å±…ï¼Œä»…åŸºäºè·ç¦»
            best_cluster = np.argmin(distances_to_prototypes)

        # åˆ†é…åˆ°æœ€ä½³èšç±»
        final_labels[point_idx] = best_cluster

    print(f"   ç¨€ç–ç‚¹åˆ†é…å®Œæˆ")
    return final_labels


def resolve_conflicts_ssddbc(X, clusters, cluster_labels, labeled_mask, targets, known_mask, densities):
    """
    æ­¥éª¤4: SS-DDBCé£æ ¼çš„å†²çªå¤„ç†ç®—æ³•

    å†²çªè§£å†³æµç¨‹:
    1. æ£€æµ‹èšç±»é—´çš„å†²çªç‚¹
    2. æ¯”è¾ƒæ¶‰åŠçš„æ ¸å¿ƒç‚¹å¯†åº¦
    3. æ ¹æ®å¯†åº¦+è·ç¦»é‡æ–°åˆ†é…å†²çªç‚¹
    4. æ ‡è®°æ— æ ‡ç­¾æ ·æœ¬çš„ç°‡ä¸ºæ½œåœ¨æœªçŸ¥ç±»

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        clusters: èšç±»åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç 
        densities: æ¯ä¸ªç‚¹çš„å¯†åº¦å€¼

    Returns:
        refined_clusters: ä¼˜åŒ–åçš„èšç±»
        refined_labels: ä¼˜åŒ–åçš„èšç±»æ ‡ç­¾
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•
    """
    print(f"âš–ï¸ SS-DDBCå†²çªå¤„ç†...")

    refined_clusters = [cluster.copy() for cluster in clusters]
    refined_labels = cluster_labels.copy()
    conflict_count = 0

    # 1. æ£€æµ‹å¹¶è§£å†³å†²çª
    for i, cluster_i in enumerate(refined_clusters):
        if len(cluster_i) == 0:
            continue

        cluster_i_indices = list(cluster_i)

        # è·å–èšç±»içš„æœ‰æ ‡ç­¾æ ·æœ¬æ ‡ç­¾åˆ†å¸ƒ
        cluster_i_labeled = labeled_mask[cluster_i_indices]
        cluster_i_targets = targets[cluster_i_indices]
        cluster_i_labels = cluster_i_targets[cluster_i_labeled]

        if len(cluster_i_labels) == 0:
            continue  # æ— æ ‡ç­¾èšç±»ï¼Œç¨åå¤„ç†

        # æ£€æŸ¥èšç±»å†…æ˜¯å¦æœ‰æ ‡ç­¾å†²çª
        unique_labels_i, counts_i = np.unique(cluster_i_labels, return_counts=True)

        if len(unique_labels_i) > 1:
            # èšç±»å†…éƒ¨æ ‡ç­¾å†²çªï¼Œéœ€è¦è§£å†³
            main_label = unique_labels_i[np.argmax(counts_i)]
            conflict_indices = []

            for idx, point_idx in enumerate(cluster_i_indices):
                if cluster_i_labeled[idx] and cluster_i_targets[idx] != main_label:
                    conflict_indices.append(point_idx)

            # è§£å†³å†²çªç‚¹
            for conflict_idx in conflict_indices:
                conflict_count += 1

                # æ‰¾åˆ°å†²çªç‚¹åº”è¯¥å±äºçš„æ­£ç¡®èšç±»
                conflict_label = targets[conflict_idx]
                best_cluster = None
                min_distance = float('inf')

                # å¯»æ‰¾åŒ…å«ç›¸åŒæ ‡ç­¾çš„å…¶ä»–èšç±»
                for j, cluster_j in enumerate(refined_clusters):
                    if j == i or len(cluster_j) == 0:
                        continue

                    cluster_j_indices = list(cluster_j)
                    cluster_j_labeled = labeled_mask[cluster_j_indices]
                    cluster_j_targets = targets[cluster_j_indices]
                    cluster_j_labels = cluster_j_targets[cluster_j_labeled]

                    if conflict_label in cluster_j_labels:
                        # è®¡ç®—å†²çªç‚¹åˆ°èšç±»jä¸­å¿ƒçš„è·ç¦»
                        cluster_center = np.mean(X[cluster_j_indices], axis=0)
                        distance = np.linalg.norm(X[conflict_idx] - cluster_center)

                        if distance < min_distance:
                            min_distance = distance
                            best_cluster = j

                # é‡æ–°åˆ†é…å†²çªç‚¹
                if best_cluster is not None:
                    # æ¯”è¾ƒå¯†åº¦å†³å®šæ˜¯å¦é‡æ–°åˆ†é…
                    conflict_density = densities[conflict_idx]

                    # æ‰¾åˆ°ç›®æ ‡èšç±»ä¸­å¯†åº¦æœ€é«˜çš„ç‚¹
                    target_cluster_indices = list(refined_clusters[best_cluster])
                    target_densities = densities[target_cluster_indices]
                    max_target_density = np.max(target_densities)

                    # å¦‚æœç›®æ ‡èšç±»æœ‰æ›´é«˜å¯†åº¦çš„ç‚¹ï¼Œåˆ™é‡æ–°åˆ†é…
                    if max_target_density >= conflict_density:
                        refined_clusters[i].discard(conflict_idx)
                        refined_clusters[best_cluster].add(conflict_idx)
                        refined_labels[conflict_idx] = best_cluster
                        print(f"   å†²çªç‚¹{conflict_idx}ä»èšç±»{i}é‡æ–°åˆ†é…åˆ°èšç±»{best_cluster}")

    # 2. è¯†åˆ«æ½œåœ¨æœªçŸ¥ç±»èšç±»
    unknown_clusters = []
    for i, cluster in enumerate(refined_clusters):
        if len(cluster) == 0:
            continue

        cluster_indices = list(cluster)
        cluster_labeled = labeled_mask[cluster_indices]

        # å¦‚æœèšç±»ä¸­æ²¡æœ‰æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œæ ‡è®°ä¸ºæ½œåœ¨æœªçŸ¥ç±»
        if np.sum(cluster_labeled) == 0:
            unknown_clusters.append(i)
            print(f"   èšç±»{i}æ— æœ‰æ ‡ç­¾æ ·æœ¬ï¼Œæ ‡è®°ä¸ºæ½œåœ¨æœªçŸ¥ç±»")

    print(f"   å†²çªè§£å†³å®Œæˆ: å¤„ç†{conflict_count}ä¸ªå†²çªç‚¹")
    print(f"   è¯†åˆ«{len(unknown_clusters)}ä¸ªæ½œåœ¨æœªçŸ¥ç±»èšç±»")

    return refined_clusters, refined_labels, unknown_clusters


def assign_sparse_points(X, clusters, cluster_labels, lambda_weight=0.7):
    """
    æ­¥éª¤5: å°†å‰©ä½™ç‚¹åˆ†é…ç»™æœ€è¿‘èšç±»æˆ–å½¢æˆå•ç‚¹èšç±»

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        clusters: èšç±»åˆ—è¡¨
        cluster_labels: èšç±»æ ‡ç­¾
        lambda_weight: è·ç¦»æƒé‡

    Returns:
        final_labels: æœ€ç»ˆèšç±»æ ‡ç­¾
    """
    print(f"ğŸ¯ åˆ†é…ç¨€ç–ç‚¹...")

    final_labels = cluster_labels.copy()
    unassigned_mask = cluster_labels == -1
    unassigned_indices = np.where(unassigned_mask)[0]

    if len(unassigned_indices) == 0:
        print(f"   æ— éœ€åˆ†é…ç¨€ç–ç‚¹")
        return final_labels

    # è®¡ç®—èšç±»ä¸­å¿ƒ
    cluster_centers = []
    for cluster in clusters:
        if len(cluster) > 0:
            cluster_indices = list(cluster)
            center = np.mean(X[cluster_indices], axis=0)
            cluster_centers.append(center)
        else:
            cluster_centers.append(None)

    # åˆ†é…æœªåˆ†é…çš„ç‚¹
    for idx in unassigned_indices:
        point = X[idx]
        min_distance = float('inf')
        best_cluster = -1

        # è®¡ç®—åˆ°å„ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»
        for cluster_id, center in enumerate(cluster_centers):
            if center is not None:
                distance = np.linalg.norm(point - center)
                if distance < min_distance:
                    min_distance = distance
                    best_cluster = cluster_id

        # åˆ†é…åˆ°æœ€è¿‘çš„èšç±»
        final_labels[idx] = best_cluster

    print(f"   ç¨€ç–ç‚¹åˆ†é…å®Œæˆ: {len(unassigned_indices)}ä¸ªç‚¹")

    return final_labels


def adaptive_density_clustering(X, targets, known_mask, labeled_mask,
                               k=10, density_percentile=75, lambda_weight=0.7):
    """
    SS-DDBCé£æ ¼çš„è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        targets: çœŸå®æ ‡ç­¾ (ä»…ç”¨äºè¯„ä¼°)
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç 
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        k: kè¿‘é‚»å‚æ•°
        density_percentile: å¯†åº¦ç™¾åˆ†ä½æ•°é˜ˆå€¼
        lambda_weight: å†²çªè§£å†³æƒé‡

    Returns:
        predictions: èšç±»é¢„æµ‹ç»“æœ
        n_clusters: èšç±»æ•°é‡
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•
    """
    print("ğŸš€ å¼€å§‹SS-DDBCè‡ªé€‚åº”å¯†åº¦èšç±»...")

    # æ­¥éª¤1: ç®€åŒ–å¯†åº¦è®¡ç®—
    densities, knn_distances, neighbors = compute_simple_density(X, k)

    # æ­¥éª¤2: é«˜å¯†åº¦ç‚¹è¯†åˆ«
    high_density_mask = identify_high_density_points(densities, density_percentile)

    # æ­¥éª¤3: SS-DDBCèšç±»æ„å»º (é›†æˆå†²çªå¤„ç†)
    clusters, cluster_labels = build_clusters_ssddbc(
        X, high_density_mask, neighbors, labeled_mask, targets, densities, known_mask, k
    )

    # åˆ†æSS-DDBCèšç±»æ„å»ºç»“æœ
    analyze_ssddbc_clustering_result(clusters, cluster_labels, labeled_mask, targets, known_mask)

    # æ­¥éª¤4: åŸºäºpartial-clusteringç»“æœå»ºç«‹åŸå‹
    prototypes, prototype_labels = build_prototypes(X, clusters, labeled_mask, targets)

    # æ­¥éª¤5: ç»“åˆåŸå‹è·ç¦»å’Œè¿‘é‚»æƒ…å†µï¼Œæ ‡æ³¨å‰©ä½™çš„ç¨€ç–ç‚¹
    final_labels = assign_sparse_points_ssddbc(X, clusters, cluster_labels, prototypes, neighbors, labeled_mask, targets, lambda_weight)

    # æ­¥éª¤6: è¯†åˆ«æ½œåœ¨æœªçŸ¥ç±»èšç±»
    unknown_clusters = identify_unknown_clusters(clusters, labeled_mask)

    n_clusters = len(clusters)
    print(f"âœ… èšç±»å®Œæˆ: {n_clusters}ä¸ªèšç±»")

    if len(unknown_clusters) > 0:
        print(f"ğŸ” å‘ç°{len(unknown_clusters)}ä¸ªæ½œåœ¨æœªçŸ¥ç±»èšç±»: {unknown_clusters}")
    else:
        print("ğŸ” æœªå‘ç°æ½œåœ¨æœªçŸ¥ç±»èšç±»")

    return final_labels, n_clusters, unknown_clusters


def analyze_cluster_composition(predictions, targets, known_mask, labeled_mask, unknown_clusters):
    """
    åˆ†ææ¯ä¸ªèšç±»çš„å†…éƒ¨ç»„æˆæƒ…å†µ

    Args:
        predictions: èšç±»é¢„æµ‹ç»“æœ
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç 
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•åˆ—è¡¨
    """
    print(f"\nğŸ” èšç±»å†…éƒ¨ç»„æˆåˆ†æ:")
    print("="*80)

    unique_clusters = np.unique(predictions)

    for cluster_id in sorted(unique_clusters):
        cluster_mask = predictions == cluster_id
        cluster_indices = np.where(cluster_mask)[0]

        if len(cluster_indices) == 0:
            continue

        # åŸºæœ¬ä¿¡æ¯
        cluster_size = len(cluster_indices)
        is_unknown_cluster = cluster_id in unknown_clusters

        # æ ‡ç­¾ä¿¡æ¯
        cluster_targets = targets[cluster_indices]
        cluster_known_mask = known_mask[cluster_indices]
        cluster_labeled_mask = labeled_mask[cluster_indices]

        # ç»Ÿè®¡æœ‰æ ‡ç­¾æ ·æœ¬
        labeled_samples = cluster_indices[cluster_labeled_mask]
        unlabeled_samples = cluster_indices[~cluster_labeled_mask]

        # ç»Ÿè®¡å·²çŸ¥ç±»/æœªçŸ¥ç±»æ ·æœ¬
        known_samples = cluster_indices[cluster_known_mask]
        unknown_samples = cluster_indices[~cluster_known_mask]

        # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
        if len(labeled_samples) > 0:
            labeled_targets = cluster_targets[cluster_labeled_mask]
            unique_labels, label_counts = np.unique(labeled_targets, return_counts=True)
            label_distribution = dict(zip(unique_labels, label_counts))
            dominant_label = unique_labels[np.argmax(label_counts)]
            label_purity = np.max(label_counts) / len(labeled_samples)
        else:
            label_distribution = {}
            dominant_label = None
            label_purity = 0.0

        # è¾“å‡ºèšç±»ä¿¡æ¯
        cluster_type = "ğŸ” æ½œåœ¨æœªçŸ¥ç±»" if is_unknown_cluster else "ğŸ“Š å¸¸è§„èšç±»"
        print(f"\n{cluster_type} #{cluster_id} - å¤§å°: {cluster_size}")
        print(f"   æ ·æœ¬ç»„æˆ:")
        print(f"     æœ‰æ ‡ç­¾æ ·æœ¬: {len(labeled_samples)} ä¸ª")
        print(f"     æ— æ ‡ç­¾æ ·æœ¬: {len(unlabeled_samples)} ä¸ª")
        print(f"     å·²çŸ¥ç±»æ ·æœ¬: {len(known_samples)} ä¸ª")
        print(f"     æœªçŸ¥ç±»æ ·æœ¬: {len(unknown_samples)} ä¸ª")

        if len(labeled_samples) > 0:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ:")
            print(f"     ä¸»å¯¼æ ‡ç­¾: {dominant_label} (çº¯åº¦: {label_purity:.3f})")
            print(f"     è¯¦ç»†åˆ†å¸ƒ: {label_distribution}")

            # æ£€æŸ¥æ ‡ç­¾å†²çª
            if len(unique_labels) > 1:
                print(f"     âš ï¸  æ ‡ç­¾å†²çª: åŒ…å«{len(unique_labels)}ç§ä¸åŒæ ‡ç­¾")
        else:
            print(f"   æœ‰æ ‡ç­¾æ ·æœ¬åˆ†å¸ƒ: æ— æœ‰æ ‡ç­¾æ ·æœ¬")

        # åˆ†æçœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬æ— æ ‡ç­¾çš„)
        all_targets = cluster_targets  # æ‰€æœ‰èšç±»å†…æ ·æœ¬çš„çœŸå®æ ‡ç­¾
        all_unique_labels, all_label_counts = np.unique(all_targets, return_counts=True)
        all_label_distribution = dict(zip(all_unique_labels, all_label_counts))

        print(f"   çœŸå®ç±»åˆ«åˆ†å¸ƒ (æ‰€æœ‰æ ·æœ¬):")
        print(f"     è¯¦ç»†åˆ†å¸ƒ: {all_label_distribution}")

        # åˆ†æå·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»çš„çœŸå®åˆ†å¸ƒ
        if len(known_samples) > 0:
            known_targets = cluster_targets[cluster_known_mask]
            known_unique, known_counts = np.unique(known_targets, return_counts=True)
            known_class_distribution = dict(zip(known_unique, known_counts))
            print(f"   å·²çŸ¥ç±»æ ·æœ¬åˆ†å¸ƒ: {known_class_distribution}")

        if len(unknown_samples) > 0:
            unknown_targets = cluster_targets[~cluster_known_mask]
            unknown_unique, unknown_counts = np.unique(unknown_targets, return_counts=True)
            unknown_class_distribution = dict(zip(unknown_unique, unknown_counts))
            print(f"   æœªçŸ¥ç±»æ ·æœ¬åˆ†å¸ƒ: {unknown_class_distribution}")

        # åˆ†æèšç±»è´¨é‡
        if len(labeled_samples) > 1:
            # è®¡ç®—å†…éƒ¨ä¸€è‡´æ€§
            same_label_pairs = 0
            total_pairs = 0
            for i in range(len(labeled_samples)):
                for j in range(i+1, len(labeled_samples)):
                    if labeled_targets[i] == labeled_targets[j]:
                        same_label_pairs += 1
                    total_pairs += 1

            if total_pairs > 0:
                consistency = same_label_pairs / total_pairs
                print(f"   è´¨é‡è¯„ä¼°:")
                print(f"     å†…éƒ¨ä¸€è‡´æ€§: {consistency:.3f}")

                if consistency >= 0.9:
                    print(f"     è´¨é‡è¯„çº§: âœ… ä¼˜ç§€")
                elif consistency >= 0.7:
                    print(f"     è´¨é‡è¯„çº§: âœ… è‰¯å¥½")
                elif consistency >= 0.5:
                    print(f"     è´¨é‡è¯„çº§: âš ï¸  ä¸€èˆ¬")
                else:
                    print(f"     è´¨é‡è¯„çº§: âŒ è¾ƒå·®")

    # å…¨å±€ç»Ÿè®¡
    print(f"\nğŸ“Š å…¨å±€èšç±»ç»Ÿè®¡:")
    print(f"   æ€»èšç±»æ•°: {len(unique_clusters)}")
    print(f"   æ½œåœ¨æœªçŸ¥ç±»èšç±»æ•°: {len(unknown_clusters)}")
    print(f"   å¸¸è§„èšç±»æ•°: {len(unique_clusters) - len(unknown_clusters)}")

    # åˆ†æèšç±»å¤§å°åˆ†å¸ƒ
    cluster_sizes = []
    for cluster_id in unique_clusters:
        cluster_size = np.sum(predictions == cluster_id)
        cluster_sizes.append(cluster_size)

    print(f"   èšç±»å¤§å°ç»Ÿè®¡:")
    print(f"     å¹³å‡å¤§å°: {np.mean(cluster_sizes):.1f}")
    print(f"     æœ€å¤§èšç±»: {np.max(cluster_sizes)} ä¸ªæ ·æœ¬")
    print(f"     æœ€å°èšç±»: {np.min(cluster_sizes)} ä¸ªæ ·æœ¬")
    print(f"     å¤§å°æ ‡å‡†å·®: {np.std(cluster_sizes):.1f}")


def test_adaptive_clustering_on_superclass(superclass_name, model_path,
                                         use_train_and_test=True, k=10,
                                         density_percentile=75, lambda_weight=0.7):
    """
    åœ¨æŒ‡å®šè¶…ç±»ä¸Šæµ‹è¯•è‡ªé€‚åº”èšç±»ç®—æ³•
    """
    print(f"ğŸ§ª æµ‹è¯•è‡ªé€‚åº”èšç±» - è¶…ç±»: {superclass_name}")
    print("="*80)

    # è®¾ç½®å‚æ•°
    class Args:
        def __init__(self):
            self.dataset_name = 'cifar100_superclass'
            self.superclass_name = superclass_name
            self.prop_train_labels = 0.8
            self.image_size = 224
            self.num_workers = 4
            self.batch_size = 64
            self.base_model = 'vit_dino'
            self.feat_dim = 768
            self.model_path = model_path
            self.interpolation = 3
            self.crop_pct = 0.875
            self.seed = 0

    args = Args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ¨¡å‹
    model = load_model(args, device)

    # è·å–è¶…ç±»ä¿¡æ¯
    superclass_classes = set(CIFAR100_SUPERCLASSES[superclass_name])
    superclass_known_classes_orig = set([cls for cls in superclass_classes if cls < 80])
    superclass_unknown_classes_orig = set([cls for cls in superclass_classes if cls >= 80])

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ï¼ˆä¸è¶…ç±»æ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
    all_classes_sorted = sorted(list(superclass_classes))
    label_mapping = {orig_cls: new_cls for new_cls, orig_cls in enumerate(all_classes_sorted)}

    # æ˜ å°„åçš„å·²çŸ¥/æœªçŸ¥ç±»åˆ«ID
    known_classes_mapped = set([label_mapping[cls] for cls in superclass_known_classes_orig])
    unknown_classes_mapped = set([label_mapping[cls] for cls in superclass_unknown_classes_orig])

    print(f"ğŸ“Š è¶…ç±»ä¿¡æ¯:")
    print(f"   åŸå§‹å·²çŸ¥ç±»: {sorted(list(superclass_known_classes_orig))} -> æ˜ å°„å: {sorted(list(known_classes_mapped))}")
    print(f"   åŸå§‹æœªçŸ¥ç±»: {sorted(list(superclass_unknown_classes_orig))} -> æ˜ å°„å: {sorted(list(unknown_classes_mapped))}")

    # è·å–æ•°æ®
    train_transform, test_transform = get_transform('imagenet', image_size=args.image_size, args=args)
    datasets = get_single_superclass_datasets(
        superclass_name=superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed
    )

    if use_train_and_test:
        # ä½¿ç”¨è®­ç»ƒé›†+æµ‹è¯•é›†
        train_dataset = datasets['train_labelled']
        unlabelled_train_dataset = datasets['train_unlabelled']
        test_dataset = datasets['test']

        # åˆ›å»ºMergedDataset
        merged_train_dataset = MergedDataset(
            labelled_dataset=deepcopy(train_dataset),
            unlabelled_dataset=deepcopy(unlabelled_train_dataset)
        )

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(merged_train_dataset, batch_size=args.batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

        # æå–ç‰¹å¾
        print("ğŸ“Š æå–è®­ç»ƒé›†ç‰¹å¾...")
        train_feats, train_targets, train_known_mask, train_labeled_mask = extract_features(
            train_loader, model, device, known_classes_mapped
        )

        print("ğŸ“Š æå–æµ‹è¯•é›†ç‰¹å¾...")
        test_feats, test_targets, test_known_mask, test_labeled_mask = extract_features(
            test_loader, model, device, known_classes_mapped
        )

        # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        all_feats = np.concatenate([train_feats, test_feats], axis=0)
        all_targets = np.concatenate([train_targets, test_targets], axis=0)
        all_known_mask = np.concatenate([train_known_mask, test_known_mask], axis=0)
        all_labeled_mask = np.concatenate([train_labeled_mask, test_labeled_mask], axis=0)

    else:
        # åªä½¿ç”¨æµ‹è¯•é›†
        test_dataset = datasets['test']
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

        print("ğŸ“Š æå–æµ‹è¯•é›†ç‰¹å¾...")
        all_feats, all_targets, all_known_mask, all_labeled_mask = extract_features(
            test_loader, model, device, known_classes_mapped
        )

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_feats)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬: {np.sum(all_known_mask)}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬: {np.sum(~all_known_mask)}")
    print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {np.sum(all_labeled_mask)}")
    print(f"   æ— æ ‡ç­¾æ ·æœ¬: {np.sum(~all_labeled_mask)}")

    # ç‰¹å¾å·²ç»åœ¨extract_featuresä¸­è¿›è¡Œäº†L2å½’ä¸€åŒ–ï¼Œä¸eval_original_gcdä¿æŒä¸€è‡´
    # ä¸å†ä½¿ç”¨StandardScalerï¼Œç›´æ¥ä½¿ç”¨L2å½’ä¸€åŒ–çš„ç‰¹å¾

    # è¿è¡ŒSS-DDBCè‡ªé€‚åº”èšç±»
    predictions, n_clusters, unknown_clusters = adaptive_density_clustering(
        all_feats, all_targets, all_known_mask, all_labeled_mask,
        k=k, density_percentile=density_percentile, lambda_weight=lambda_weight
    )

    # ç¡®å®šæµ‹è¯•é›†èŒƒå›´ç”¨äºACCè®¡ç®—
    if use_train_and_test:
        # æµ‹è¯•é›†æ˜¯åé¢çš„éƒ¨åˆ†
        test_start_idx = len(train_feats)
        test_predictions = predictions[test_start_idx:]
        test_targets = all_targets[test_start_idx:]
        test_known_mask = all_known_mask[test_start_idx:]
        print(f"ğŸ“Š ACCè®¡ç®—èŒƒå›´: æµ‹è¯•é›† ({len(test_targets)}ä¸ªæ ·æœ¬, è®­ç»ƒé›†ä¸å‚ä¸è¯„ä¼°)")
    else:
        # å…¨éƒ¨éƒ½æ˜¯æµ‹è¯•é›†
        test_predictions = predictions
        test_targets = all_targets
        test_known_mask = all_known_mask
        print(f"ğŸ“Š ACCè®¡ç®—èŒƒå›´: ä»…æµ‹è¯•é›† ({len(test_targets)}ä¸ªæ ·æœ¬)")

    # ä½¿ç”¨ä¸eval_original_gcd.pyç›¸åŒçš„ACCè®¡ç®—æ–¹æ³• (split_cluster_acc_v1)
    from project_utils.cluster_and_log_utils import split_cluster_acc_v1
    all_acc, old_acc, new_acc = split_cluster_acc_v1(test_targets, test_predictions, test_known_mask)

    # è®¡ç®—å…¶ä»–æŒ‡æ ‡ï¼ˆä¹Ÿåªåœ¨æµ‹è¯•é›†ä¸Šï¼‰
    nmi = normalized_mutual_info_score(test_targets, test_predictions)
    ari = adjusted_rand_score(test_targets, test_predictions)

    print(f"ğŸ“ˆ èšç±»ç»“æœ:")
    print(f"   èšç±»æ•°é‡: {n_clusters}")
    print(f"   æ½œåœ¨æœªçŸ¥ç±»: {len(unknown_clusters)}ä¸ª")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")
    print(f"   NMI: {nmi:.4f}")
    print(f"   ARI: {ari:.4f}")

    # æ˜¾ç¤ºæ¯ä¸ªèšç±»çš„å†…éƒ¨æƒ…å†µ
    analyze_cluster_composition(predictions, all_targets, all_known_mask, all_labeled_mask, unknown_clusters)

    # æå–æµ‹è¯•é›†ç‰¹å¾ç”¨äºK-meanså¯¹æ¯” (ç°åœ¨ä½¿ç”¨ç›¸åŒçš„L2å½’ä¸€åŒ–ç‰¹å¾)
    if use_train_and_test:
        # æµ‹è¯•é›†æ˜¯åé¢çš„éƒ¨åˆ†
        test_start_idx = len(train_feats)
        test_features_for_kmeans = all_feats[test_start_idx:]
    else:
        # å…¨éƒ¨éƒ½æ˜¯æµ‹è¯•é›†
        test_features_for_kmeans = all_feats

    # è¿”å›ç»“æœï¼ŒåŒ…å«æµ‹è¯•é›†æ•°æ®ç”¨äºK-meanså¯¹æ¯”
    return {
        'method': 'SS-DDBC',
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'nmi': nmi,
        'ari': ari,
        'n_clusters': n_clusters,
        'unknown_clusters': unknown_clusters,
        'test_features': test_features_for_kmeans,
        'test_targets': test_targets,
        'test_known_mask': test_known_mask
    }


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•æµ‹è¯•')

    # å¿…è¦å‚æ•°
    parser.add_argument('--model_path', type=str,
                        default=None,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆæœªæä¾›æ—¶éœ€ç¡®ä¿ç¼“å­˜ç‰¹å¾å¯ç”¨ï¼‰')
    parser.add_argument('--superclass_name', type=str, default='trees',
                        help='æµ‹è¯•çš„è¶…ç±»åç§°')

    # ç®—æ³•å‚æ•°
    parser.add_argument('--use_train_and_test', type=str2bool, default=True,
                        help='æ˜¯å¦åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†')
    parser.add_argument('--k', type=int, default=10,
                        help='kè¿‘é‚»å‚æ•°')
    parser.add_argument('--density_percentile', type=int, default=75,
                        help='å¯†åº¦é˜ˆå€¼ç™¾åˆ†ä½æ•°')
    parser.add_argument('--lambda_weight', type=float, default=0.7,
                        help='å†²çªè§£å†³æƒé‡')
    parser.add_argument('--run_kmeans_baseline', type=str2bool, default=False,
                        help='æ˜¯å¦è¿è¡ŒK-meansåŸºçº¿å¯¹æ¯”')

    args = parser.parse_args()

    print("è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•æµ‹è¯•")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¶…ç±»åç§°: {args.superclass_name}")
    print(f"ä½¿ç”¨è®­ç»ƒ+æµ‹è¯•: {args.use_train_and_test}")
    print(f"ç®—æ³•å‚æ•°: k={args.k}, density_percentile={args.density_percentile}, lambda={args.lambda_weight}")
    print("="*80)

    try:
        # è¿è¡ŒSS-DDBCç®—æ³•
        ssddbc_results = test_adaptive_clustering_on_superclass(
            superclass_name=args.superclass_name,
            model_path=args.model_path,
            use_train_and_test=args.use_train_and_test,
            k=args.k,
            density_percentile=args.density_percentile,
            lambda_weight=args.lambda_weight
        )

        # å¦‚æœå¼€å¯K-meansåŸºçº¿å¯¹æ¯”
        if args.run_kmeans_baseline:
            print("\n" + "="*80)
            print("ğŸ”„ è¿è¡ŒK-meansåŸºçº¿å¯¹æ¯”...")
            print("âœ… ç°åœ¨ä½¿ç”¨ä¸eval_original_gcdå®Œå…¨ç›¸åŒçš„L2å½’ä¸€åŒ–ç‰¹å¾")

            # ä½¿ç”¨SS-DDBCæµ‹è¯•ä¸­å·²æå–çš„æµ‹è¯•é›†ç‰¹å¾ (ç›¸åŒçš„L2å½’ä¸€åŒ–)
            test_features = ssddbc_results['test_features']
            test_targets = ssddbc_results['test_targets']
            test_known_mask = ssddbc_results['test_known_mask']

            # ä½¿ç”¨çœŸå®çš„ç±»åˆ«æ•°ä½œä¸ºK-meansèšç±»æ•°ï¼ˆä¸eval_original_gcd.pyä¿æŒä¸€è‡´ï¼‰
            n_true_classes = len(np.unique(test_targets))
            print(f"ğŸ¯ K-meansèšç±»æ•°é‡: {n_true_classes} (çœŸå®ç±»åˆ«æ•°)")

            # è¿è¡ŒK-means (ä½¿ç”¨ç›¸åŒçš„L2å½’ä¸€åŒ–ç‰¹å¾)
            kmeans_results = test_kmeans_baseline(
                test_features,  # ç›¸åŒçš„L2å½’ä¸€åŒ–ç‰¹å¾
                test_targets,
                test_known_mask,
                n_clusters=n_true_classes,  # ä½¿ç”¨çœŸå®ç±»åˆ«æ•°
                random_state=0  # ä¸åŸç‰ˆä¸€è‡´çš„éšæœºç§å­
            )

            # å¯¹æ¯”ç»“æœ
            print(f"\nğŸ“Š ç®—æ³•å¯¹æ¯”ç»“æœ:")
            print("="*80)
            print(f"{'æŒ‡æ ‡':<15} {'SS-DDBC':<12} {'K-means':<12} {'å·®å¼‚':<12}")
            print("-"*80)
            print(f"{'All ACC':<15} {ssddbc_results['all_acc']:<12.4f} {kmeans_results['all_acc']:<12.4f} {ssddbc_results['all_acc']-kmeans_results['all_acc']:<+12.4f}")
            print(f"{'Old ACC':<15} {ssddbc_results['old_acc']:<12.4f} {kmeans_results['old_acc']:<12.4f} {ssddbc_results['old_acc']-kmeans_results['old_acc']:<+12.4f}")
            print(f"{'New ACC':<15} {ssddbc_results['new_acc']:<12.4f} {kmeans_results['new_acc']:<12.4f} {ssddbc_results['new_acc']-kmeans_results['new_acc']:<+12.4f}")
            print(f"{'NMI':<15} {ssddbc_results['nmi']:<12.4f} {kmeans_results['nmi']:<12.4f} {ssddbc_results['nmi']-kmeans_results['nmi']:<+12.4f}")
            print(f"{'ARI':<15} {ssddbc_results['ari']:<12.4f} {kmeans_results['ari']:<12.4f} {ssddbc_results['ari']-kmeans_results['ari']:<+12.4f}")
            print(f"{'èšç±»æ•°':<15} {ssddbc_results['n_clusters']:<12} {kmeans_results['n_clusters']:<12} {'=':<12}")
            print("="*80)

        print("\næµ‹è¯•å®Œæˆ!")

    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
