#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¶…ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬
ç”¨äºåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶é‡æ–°è¯„ä¼°æŒ‡å®šè¶…ç±»çš„æ€§èƒ½

=== è°ƒè¯•å†ç¨‹å’Œå…³é”®å‘ç° ===

æœ¬è„šæœ¬ç»å†äº†ä¸€ä¸ªé‡è¦çš„è°ƒè¯•è¿‡ç¨‹ï¼Œå‘ç°äº†å¯¼è‡´è¯„ä¼°ç»“æœä¸ä¸€è‡´çš„æ ¹æœ¬åŸå› ï¼š

é—®é¢˜èƒŒæ™¯ï¼š
- è®­ç»ƒæ—¶æ¨¡å‹è¾¾åˆ°0.8 All ACCï¼Œä½†é‡æ–°è¯„ä¼°åªæœ‰0.48 All ACC
- éœ€è¦æ‰¾å‡ºtrain_superclass.pyå’Œevaluate_superclass_model.pyä¹‹é—´çš„å·®å¼‚

è°ƒè¯•è¿‡ç¨‹ä¸­æ’æŸ¥çš„åŸå› ï¼š

1. ã€å·²æ’é™¤ã€‘æ¨¡å‹åŠ è½½é—®é¢˜
   - æ£€æŸ¥äº†state_dictåŠ è½½æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æ¨¡å‹ç»“æ„ä¸€è‡´æ€§

2. ã€å·²æ’é™¤ã€‘K-meanså®ç°å·®å¼‚
   - å¯¹æ¯”äº†K-meanså‚æ•°è®¾ç½® (n_clusters, random_state, n_init)
   - ç¡®è®¤èšç±»ç®—æ³•å®ç°ä¸€è‡´

3. ã€å·²æ’é™¤ã€‘maskæ„å»ºå·®å¼‚
   - å¯¹æ¯”äº†å·²çŸ¥ç±»/æœªçŸ¥ç±»maskçš„æ„å»ºé€»è¾‘
   - ç¡®è®¤ `x.item() in range(len(args.train_classes))` é€»è¾‘ä¸€è‡´

4. ã€å·²æ’é™¤ã€‘æ•°æ®åŠ è½½é¡ºåºé—®é¢˜
   - ç¡®è®¤äº†batch_sizeã€æ•°æ®é›†éšæœºç§å­ç­‰å‚æ•°ä¸€è‡´
   - æ’é™¤äº†æ•°æ®åˆ’åˆ†å·®å¼‚

5. ã€å…³é”®å‘ç°ã€‘å›¾åƒå°ºå¯¸å·®å¼‚ â­â­â­
   - train_superclass.py: args.image_size = 224
   - evaluate_superclass_model.py: args.image_size = 32 (é”™è¯¯!)

   è¿™ä¸ªå·®å¼‚å¯¼è‡´ï¼š
   - å®Œå…¨ä¸åŒçš„æ•°æ®é¢„å¤„ç†æµç¨‹
   - 224x224 vs 32x32 çš„è¾“å…¥å›¾åƒå°ºå¯¸
   - ViTæ¨¡å‹æ¥æ”¶åˆ°å®Œå…¨ä¸åŒçš„è¾“å…¥
   - ç‰¹å¾æå–ç»“æœå®Œå…¨ä¸åŒ¹é…

ä¿®å¤æªæ–½ï¼š
- å°†evaluate_superclass_model.pyçš„image_sizeæ”¹ä¸º224
- ç¡®ä¿ä¸train_superclass.pyçš„æ•°æ®é¢„å¤„ç†å®Œå…¨ä¸€è‡´

æ•™è®­æ€»ç»“ï¼š
- å›¾åƒå°ºå¯¸æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„å…³é”®è¶…å‚æ•°
- å³ä½¿æ¨¡å‹æƒé‡æ­£ç¡®ï¼Œè¾“å…¥å°ºå¯¸ä¸åŒ¹é…ä¹Ÿä¼šå¯¼è‡´å®Œå…¨é”™è¯¯çš„ç»“æœ
- åœ¨å¤ç°å®éªŒæ—¶ï¼Œå¿…é¡»ç¡®ä¿æ‰€æœ‰é¢„å¤„ç†å‚æ•°å®Œå…¨ä¸€è‡´

åŠŸèƒ½ï¼š
1. åŠ è½½æŒ‡å®šè¶…ç±»çš„å®Œæ•´è®­ç»ƒé›†ï¼ˆåŒ…å«æœ‰æ ‡ç­¾å’Œæ— æ ‡ç­¾æ ·æœ¬ï¼‰å’Œæµ‹è¯•é›†
2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä»…base modelï¼Œä¸K-meansè¯„ä¼°ä¿æŒä¸€è‡´ï¼‰
3. è¿›è¡Œå››ç§K-meansèšç±»è¯„ä¼°ï¼š
   - è®­ç»ƒæ—¶æ— æ ‡ç­¾è®­ç»ƒé›†ï¼ˆä¸train_superclass.pyä¸­çš„"Train ACC Unlabelled"å®Œå…¨ä¸€è‡´ï¼‰
   - çº¯æµ‹è¯•é›†ï¼ˆä¸train_superclass.pyä¸­çš„"Test ACC"å®Œå…¨ä¸€è‡´ï¼‰
   - å®Œæ•´è®­ç»ƒé›†ï¼ˆæœ‰æ ‡ç­¾+æ— æ ‡ç­¾ï¼‰
   - åˆå¹¶æ•°æ®é›†ï¼ˆå®Œæ•´è®­ç»ƒé›†+æµ‹è¯•é›†ï¼‰
4. è¾“å‡ºè¯¦ç»†çš„èšç±»åˆ†å¸ƒåˆ†æï¼ŒåŒ…å«åŒˆç‰™åˆ©ç®—æ³•åˆ†é…ç»“æœ
5. æä¾›å››ç§è¯„ä¼°ç»“æœçš„å¯¹æ¯”åˆ†æï¼Œé‡ç‚¹å…³æ³¨ä¸è®­ç»ƒæ—¶è¯„ä¼°çš„ä¸€è‡´æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
python test/evaluate_superclass_model.py \
    --superclass_name trees \
    --model_path /path/to/model.pt
"""

import sys
import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from sklearn.cluster import KMeans
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
from tqdm import tqdm
import argparse
from copy import deepcopy

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.get_datasets import get_class_splits
from data.augmentations import get_transform
from data.cifar100_superclass import get_single_superclass_datasets, SUPERCLASS_NAMES
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool
from project_utils.cluster_and_log_utils import split_cluster_acc_v1, split_cluster_acc_v2, log_accs_from_preds
from project_utils.cluster_utils import cluster_acc


def load_model(model_path, device, feat_dim=768):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹
    """
    print(f"ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...")
    print(f"   æ¨¡å‹æ–‡ä»¶: {model_path}")

    # æ„å»ºbase model
    model = vits.__dict__['vit_base']()

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if model_path and os.path.exists(model_path):
        print(f"   åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state_dict)
    else:
        print(f"   âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨DINOé¢„è®­ç»ƒæƒé‡")
        state_dict = torch.load(dino_pretrain_path, map_location='cpu')
        model.load_state_dict(state_dict)

    model.to(device)
    model.eval()

    # æ£€æŸ¥æ¨¡å‹å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ¨¡å‹å‚æ•°ç»Ÿè®¡: æ€»å‚æ•°={total_params:,}, å¯è®­ç»ƒ={trainable_params:,}")

    # æ£€æŸ¥å‰å‡ å±‚æƒé‡ä¿¡æ¯ï¼ˆç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½ï¼‰
    first_layer_weight = None
    for name, param in model.named_parameters():
        if 'weight' in name:
            first_layer_weight = param.data.flatten()[:10]
            print(f"   ç¬¬ä¸€å±‚æƒé‡æ ·æœ¬ ({name}): {first_layer_weight}")
            break

    print(f"   âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    return model



def extract_features(model, data_loader, device):
    """
    æå–ç‰¹å¾ï¼ˆä»…ä½¿ç”¨base modelï¼Œä¸K-meansè¯„ä¼°ä¿æŒä¸€è‡´ï¼‰
    """
    print(f"ğŸ” æå–ç‰¹å¾ï¼ˆä½¿ç”¨base modelï¼Œæ— æŠ•å½±å¤´ï¼‰...")

    all_feats = []
    all_targets = []
    all_indices = []

    model.eval()

    with torch.no_grad():
        for batch_idx, (images, targets_batch, indices) in enumerate(tqdm(data_loader, desc="æå–ç‰¹å¾")):
            images = images.to(device)

            # ä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹ç‰¹å¾æå–ï¼ˆä¸K-meansè¯„ä¼°ä¿æŒä¸€è‡´ï¼‰
            feats = model(images)

            # L2å½’ä¸€åŒ–
            feats = torch.nn.functional.normalize(feats, dim=-1)

            all_feats.append(feats.cpu().numpy())
            all_targets.append(targets_batch.numpy())
            all_indices.append(indices.numpy())

    # åˆå¹¶ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    all_indices = np.concatenate(all_indices, axis=0)

    print(f"   âœ… ç‰¹å¾æå–å®Œæˆ: {all_feats.shape}")
    return all_feats, all_targets, all_indices


def evaluate_clustering_like_training(features, targets, train_classes, num_labeled_classes, num_unlabeled_classes,
                                     evaluation_name="", args=None):
    """
    å®Œå…¨æŒ‰ç…§train_superclass.pyçš„test_kmeans_superclasså®ç°K-meansèšç±»è¯„ä¼°
    """
    print(f"\nğŸ”¬ {evaluation_name} K-meansèšç±»è¯„ä¼°ï¼ˆå®Œå…¨å¤åˆ¶train_superclass.pyé€»è¾‘ï¼‰...")

    # å®Œå…¨æŒ‰ç…§train_superclass.pyçš„æ–¹å¼æ„å»ºmask
    mask = np.array([True if x.item() in range(len(train_classes)) else False for x in targets])

    print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"   æ ·æœ¬æ•°é‡: {len(features)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬æ•°: {mask.sum()}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬æ•°: {(~mask).sum()}")

    # -----------------------
    # K-MEANS (å®Œå…¨æŒ‰ç…§train_superclass.py)
    # -----------------------
    print('Fitting K-Means...')
    kmeans = KMeans(n_clusters=num_labeled_classes + num_unlabeled_classes, random_state=0, n_init=10).fit(features)
    preds = kmeans.labels_
    print('Done!')

    # -----------------------
    # EVALUATE (å®Œå…¨æŒ‰ç…§train_superclass.py)
    # -----------------------
    # æ£€æŸ¥æ˜¯å¦æœ‰æœªçŸ¥ç±»
    mask = np.array(mask, dtype=bool)  # ç¡®ä¿maskæ˜¯numpyæ•°ç»„
    has_unknown_classes = num_unlabeled_classes > 0 and (~mask).sum() > 0

    if has_unknown_classes:
        # æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šæ­£å¸¸è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        all_acc, old_acc, new_acc = log_accs_from_preds(
            y_true=targets, y_pred=preds, mask=mask,
            T=0, eval_funcs=['v2'], save_name=evaluation_name,
            writer=None, print_output=False
        )
    else:
        # æ²¡æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šåªè®¡ç®—å·²çŸ¥ç±»å‡†ç¡®ç‡
        from project_utils.cluster_utils import cluster_acc
        old_acc = cluster_acc(targets, preds)
        all_acc = old_acc  # å½“æ²¡æœ‰æœªçŸ¥ç±»æ—¶ï¼ŒAll ACC = Old ACC
        new_acc = 0.0      # æ²¡æœ‰æœªçŸ¥ç±»ï¼ŒNew ACCä¸º0

        print(f"âš ï¸  æ³¨æ„: å½“å‰æ•°æ®ä¸­æ²¡æœ‰æœªçŸ¥ç±»æ ·æœ¬ï¼Œä»…è®¡ç®—Old ACC")

    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    nmi = normalized_mutual_info_score(targets, preds)
    ari = adjusted_rand_score(targets, preds)

    # è¾“å‡ºç»“æœ
    print(f"\nğŸ“Š {evaluation_name} èšç±»ç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")
    print(f"   NMI: {nmi:.4f}")
    print(f"   ARI: {ari:.4f}")

    return {
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'nmi': nmi,
        'ari': ari,
        'cluster_preds': preds
    }


def print_cluster_distribution_analysis(cluster_preds, targets, known_mask, superclass_name, dataset_name):
    """
    åˆ†æå¹¶æ‰“å°èšç±»æ ‡ç­¾åˆ†å¸ƒï¼ŒåŒ…å«åŒˆç‰™åˆ©ç®—æ³•åˆ†é…ç»“æœ
    """
    print(f"\nğŸ” {superclass_name} {dataset_name} èšç±»æ ‡ç­¾åˆ†å¸ƒåˆ†æ:")
    print("=" * 80)

    # è®¡ç®—åŒˆç‰™åˆ©ç®—æ³•åˆ†é…
    from scipy.optimize import linear_sum_assignment as linear_assignment

    # æ„å»ºæ··æ·†çŸ©é˜µ
    y_true = targets.astype(int)
    y_pred = cluster_preds.astype(int)
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=int)
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1

    # åŒˆç‰™åˆ©ç®—æ³•æ‰¾æœ€ä¼˜åˆ†é…
    ind = linear_assignment(w.max() - w)
    ind = np.vstack(ind).T

    # åˆ›å»ºèšç±»åˆ°çœŸå®æ ‡ç­¾çš„æ˜ å°„
    cluster_to_label_mapping = {cluster_id: true_label for cluster_id, true_label in ind}

    print(f"ğŸ¯ åŒˆç‰™åˆ©ç®—æ³•æœ€ä¼˜åˆ†é…:")
    print(f"   èšç±»ID -> çœŸå®æ ‡ç­¾: {cluster_to_label_mapping}")
    print()

    unique_clusters = np.unique(cluster_preds)

    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_samples = len(targets)
    total_known = known_mask.sum()
    total_unknown = (~known_mask).sum()

    print(f"ğŸ“Š æ•°æ®é›†æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬: {total_known}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬: {total_unknown}")
    print()

    for cluster_id in unique_clusters:
        # æ‰¾åˆ°å±äºå½“å‰èšç±»çš„æ ·æœ¬
        cluster_mask = cluster_preds == cluster_id
        cluster_targets = targets[cluster_mask]
        cluster_known_mask = known_mask[cluster_mask]

        # ç»Ÿè®¡å·²çŸ¥ç±»å’ŒæœªçŸ¥ç±»æ ·æœ¬æ•°
        known_count = cluster_known_mask.sum()
        unknown_count = (~cluster_known_mask).sum()
        total_count = len(cluster_targets)

        # è·å–åŒˆç‰™åˆ©åˆ†é…çš„æ ‡ç­¾
        assigned_label = cluster_to_label_mapping.get(cluster_id, "æœªåˆ†é…")

        print(f"èšç±» {cluster_id}: {total_count}ä¸ªæ ·æœ¬ (å·²çŸ¥ç±»: {known_count}, æœªçŸ¥ç±»: {unknown_count})")
        print(f"   åŒˆç‰™åˆ©åˆ†é… -> æ ‡ç­¾ {assigned_label}")

        # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
        unique_labels, counts = np.unique(cluster_targets, return_counts=True)
        label_dist = {int(label): int(count) for label, count in zip(unique_labels, counts)}
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {label_dist}")

        # è®¡ç®—èšç±»çº¯åº¦ï¼ˆæœ€å¤§ç±»åˆ«å æ¯”ï¼‰
        if total_count > 0:
            max_count = max(counts)
            purity = max_count / total_count
            dominant_label = unique_labels[np.argmax(counts)]
            print(f"   èšç±»çº¯åº¦: {purity:.3f} (ä¸»å¯¼æ ‡ç­¾: {dominant_label})")

            # åˆ†æåŒˆç‰™åˆ©åˆ†é…çš„æ­£ç¡®æ€§
            if assigned_label != "æœªåˆ†é…":
                assigned_count = label_dist.get(int(assigned_label), 0)
                assign_accuracy = assigned_count / total_count
                print(f"   åˆ†é…å‡†ç¡®æ€§: {assign_accuracy:.3f} (åˆ†é…æ ‡ç­¾{assigned_label}çš„æ ·æœ¬å æ¯”)")

        print()


def main():
    parser = argparse.ArgumentParser(description='è¶…ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬')

    # æ¨¡å‹å’Œæ•°æ®å‚æ•°
    parser.add_argument('--superclass_name', type=str, required=True,
                        help='è¶…ç±»åç§°', choices=SUPERCLASS_NAMES)
    parser.add_argument('--model_path', type=str, required=True,
                        help='åŸºç¡€æ¨¡å‹è·¯å¾„')

    # æ•°æ®é›†å‚æ•°ï¼ˆä¸train_superclass.pyä¿æŒä¸€è‡´ï¼‰
    parser.add_argument('--prop_train_labels', type=float, default=0.8,
                        help='è®­ç»ƒé›†æ ‡ç­¾æ¯”ä¾‹ï¼ˆä¸train_superclass.pyä¸€è‡´ï¼‰')
    parser.add_argument('--batch_size', type=int, default=128,
                        help='æ‰¹æ¬¡å¤§å°ï¼ˆä¸train_superclass.pyä¸€è‡´ï¼‰')
    parser.add_argument('--num_workers', type=int, default=8,
                        help='æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°')

    # è¯„ä¼°å‚æ•°ï¼ˆä¸train_superclass.pyä¿æŒä¸€è‡´ï¼‰
    parser.add_argument('--random_state', type=int, default=0,
                        help='K-meanséšæœºç§å­ï¼ˆä¸train_superclass.pyä¸€è‡´ï¼‰')
    parser.add_argument('--gpu', type=int, default=0,
                        help='GPUè®¾å¤‡ID')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--feat_dim', type=int, default=768,
                        help='ç‰¹å¾ç»´åº¦')

    args = parser.parse_args()

    print("ğŸš€ è¶…ç±»æ¨¡å‹è¯„ä¼°è„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“‚ è¶…ç±»åç§°: {args.superclass_name}")
    print(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {args.model_path}")
    print(f"ğŸ”§ ç‰¹å¾æå–: ä»…ä½¿ç”¨base modelï¼ˆä¸K-meansè¯„ä¼°ä¸€è‡´ï¼‰")

    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» è®¾å¤‡: {device}")

    # è®¾ç½®è¶…ç±»å‚æ•°
    args.dataset_name = 'cifar100_superclass'
    args.eval_funcs = ['v2']  # æ·»åŠ è¯„ä¼°å‡½æ•°å‚æ•°
    args.writer = None        # æ·»åŠ writerå‚æ•°
    args = get_class_splits(args)
    args.num_labeled_classes = len(args.train_classes)
    args.num_unlabeled_classes = len(args.unlabeled_classes)

    print(f"ğŸ“Š ç±»åˆ«ä¿¡æ¯:")
    print(f"   å·²çŸ¥ç±»æ•°é‡: {args.num_labeled_classes}")
    print(f"   æœªçŸ¥ç±»æ•°é‡: {args.num_unlabeled_classes}")
    print(f"   æ€»ç±»åˆ«æ•°: {args.num_labeled_classes + args.num_unlabeled_classes}")

    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“¦ åŠ è½½è¶…ç±»æ•°æ®é›†...")
    args.image_size = 224  # ä¸train_superclass.pyä¿æŒä¸€è‡´
    args.interpolation = 3
    args.crop_pct = 0.875
    args.transform = 'imagenet'  # ä¸train_superclass.pyä¿æŒä¸€è‡´
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)

    print(f"   ä½¿ç”¨å˜æ¢: {args.transform}")
    print(f"   å›¾åƒå°ºå¯¸: {args.image_size}")
    print(f"   æ’å€¼æ–¹å¼: {args.interpolation}")
    print(f"   è£å‰ªæ¯”ä¾‹: {args.crop_pct}")

    # æ•°æ®é›†å‚æ•°ï¼ˆä¸train_superclass.pyä¿æŒä¸€è‡´ï¼‰
    args.seed = 1  # ä¸train_superclass.pyé»˜è®¤å€¼ä¸€è‡´

    print(f"   æ•°æ®é›†éšæœºç§å­: {args.seed}")

    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=test_transform,  # è¯„ä¼°æ—¶éƒ½ä½¿ç”¨test_transform
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed  # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„seed
    )

    # æ„å»ºè¯„ä¼°æ•°æ®é›†
    test_dataset = datasets['test']
    train_labelled_dataset = datasets['train_labelled']
    train_unlabelled_dataset = datasets['train_unlabelled']

    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    print(f"   æœ‰æ ‡ç­¾è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_labelled_dataset)}")
    print(f"   æ— æ ‡ç­¾è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_unlabelled_dataset)}")

    # åˆå¹¶è®­ç»ƒé›†
    from torch.utils.data import ConcatDataset
    train_combined_dataset = ConcatDataset([train_labelled_dataset, train_unlabelled_dataset])
    print(f"   åˆå¹¶åè®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_combined_dataset)}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size,
                            shuffle=False, num_workers=args.num_workers)
    train_loader = DataLoader(train_combined_dataset, batch_size=args.batch_size,
                            shuffle=False, num_workers=args.num_workers)

    # åŠ è½½æ¨¡å‹ï¼ˆä»…base modelï¼‰
    model = load_model(args.model_path, device, args.feat_dim)

    # æå–è®­ç»ƒé›†ç‰¹å¾
    print(f"\n" + "=" * 80)
    print(f"ğŸ” æå–è®­ç»ƒé›†ç‰¹å¾")
    print("=" * 80)

    train_features, train_targets, train_indices = extract_features(
        model, train_loader, device)

    # æå–æµ‹è¯•é›†ç‰¹å¾
    print(f"\n" + "=" * 80)
    print(f"ğŸ” æå–æµ‹è¯•é›†ç‰¹å¾")
    print("=" * 80)

    test_features, test_targets, test_indices = extract_features(
        model, test_loader, device)

    # 1. è®­ç»ƒé›†K-meansèšç±»è¯„ä¼°ï¼ˆå®Œå…¨å¤åˆ¶train_superclass.pyé€»è¾‘ï¼‰
    print(f"\n" + "=" * 80)
    print(f"ğŸ§ª 1. è®­ç»ƒé›†K-meansèšç±»è¯„ä¼°")
    print("=" * 80)

    train_results = evaluate_clustering_like_training(
        train_features, train_targets, args.train_classes,
        args.num_labeled_classes, args.num_unlabeled_classes,
        "è®­ç»ƒé›†", args
    )

    train_known_mask = np.array([True if x.item() in range(len(args.train_classes)) else False for x in train_targets])
    print_cluster_distribution_analysis(
        train_results['cluster_preds'], train_targets, train_known_mask,
        args.superclass_name, "è®­ç»ƒé›†"
    )

    # 2. æµ‹è¯•é›†K-meansèšç±»è¯„ä¼°ï¼ˆå®Œå…¨å¤åˆ¶train_superclass.pyé€»è¾‘ï¼‰
    print(f"\n" + "=" * 80)
    print(f"ğŸ§ª 2. æµ‹è¯•é›†K-meansèšç±»è¯„ä¼°")
    print("=" * 80)

    test_results = evaluate_clustering_like_training(
        test_features, test_targets, args.train_classes,
        args.num_labeled_classes, args.num_unlabeled_classes,
        "æµ‹è¯•é›†", args
    )

    test_known_mask = np.array([True if x.item() in range(len(args.train_classes)) else False for x in test_targets])
    print_cluster_distribution_analysis(
        test_results['cluster_preds'], test_targets, test_known_mask,
        args.superclass_name, "æµ‹è¯•é›†"
    )

    # 3. åˆå¹¶æ•°æ®é›†K-meansèšç±»è¯„ä¼°ï¼ˆå®Œå…¨å¤åˆ¶train_superclass.pyé€»è¾‘ï¼‰
    print(f"\n" + "=" * 80)
    print(f"ğŸ§ª 3. åˆå¹¶æ•°æ®é›†K-meansèšç±»è¯„ä¼°")
    print("=" * 80)

    combined_features = np.concatenate([train_features, test_features], axis=0)
    combined_targets = np.concatenate([train_targets, test_targets], axis=0)

    combined_results = evaluate_clustering_like_training(
        combined_features, combined_targets, args.train_classes,
        args.num_labeled_classes, args.num_unlabeled_classes,
        "åˆå¹¶æ•°æ®é›†", args
    )

    combined_known_mask = np.array([True if x.item() in range(len(args.train_classes)) else False for x in combined_targets])
    print_cluster_distribution_analysis(
        combined_results['cluster_preds'], combined_targets, combined_known_mask,
        args.superclass_name, "åˆå¹¶æ•°æ®é›†"
    )

    # è¯„ä¼°ç»“æœæ€»ç»“å¯¹æ¯”
    print(f"\n" + "=" * 80)
    print(f"ğŸ“Š ä¸‰ç§K-meansè¯„ä¼°ç»“æœå¯¹æ¯”")
    print("=" * 80)
    print(f"è¶…ç±»: {args.superclass_name}")
    print(f"ç‰¹å¾æå–: ä»…ä½¿ç”¨base model")
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print()

    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    print(f"{'æŒ‡æ ‡':<12} {'è®­ç»ƒé›†':<12} {'æµ‹è¯•é›†':<12} {'åˆå¹¶æ•°æ®':<12}")
    print("-" * 50)
    print(f"{'All ACC':<12} {train_results['all_acc']:<12.4f} {test_results['all_acc']:<12.4f} {combined_results['all_acc']:<12.4f}")
    print(f"{'Old ACC':<12} {train_results['old_acc']:<12.4f} {test_results['old_acc']:<12.4f} {combined_results['old_acc']:<12.4f}")
    print(f"{'New ACC':<12} {train_results['new_acc']:<12.4f} {test_results['new_acc']:<12.4f} {combined_results['new_acc']:<12.4f}")
    print(f"{'NMI':<12} {train_results['nmi']:<12.4f} {test_results['nmi']:<12.4f} {combined_results['nmi']:<12.4f}")
    print(f"{'ARI':<12} {train_results['ari']:<12.4f} {test_results['ari']:<12.4f} {combined_results['ari']:<12.4f}")

    print()
    print(f"ğŸ” å…³é”®é—®é¢˜åˆ†æ - æµ‹è¯•é›†ACCä¸‹é™åŸå› :")
    print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_features)} | æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_features)}")

    # é‡ç‚¹åˆ†ææµ‹è¯•é›†æ€§èƒ½ä¸‹é™
    train_test_diff = test_results['all_acc'] - train_results['all_acc']
    print(f"   æµ‹è¯•é›† vs è®­ç»ƒé›† All ACCå·®å¼‚: {train_test_diff:+.4f}")

    if test_results['all_acc'] < 0.6:
        print(f"   ğŸš¨ æµ‹è¯•é›†ACC={test_results['all_acc']:.4f} æ˜æ˜¾åä½ï¼å¯èƒ½åŸå› :")
        print(f"      1. æ¨¡å‹æƒé‡åŠ è½½é—®é¢˜")
        print(f"      2. ç‰¹å¾æå–æ–¹å¼å·®å¼‚")
        print(f"      3. æ•°æ®é¢„å¤„ç†å·®å¼‚")
        print(f"      4. éšæœºç§å­ä¸ä¸€è‡´")

    print(f"\nâœ… ä¸‰ç§K-meansè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()