#!/usr/bin/env python3
"""
ç‰¹å¾æå–å¯¹æ¯”è„šæœ¬
å¯¹æ¯”train_superclass.pyå’Œevaluate_superclass_model.pyä¸¤å¥—ä»£ç çš„æµ‹è¯•é›†ç‰¹å¾æå–ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
python test/compare_feature_extraction.py \
    --superclass_name trees \
    --model_path /path/to/model.pt
"""

import sys
import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
from copy import deepcopy

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.get_datasets import get_class_splits
from data.augmentations import get_transform
from data.cifar100_superclass import get_single_superclass_datasets, SUPERCLASS_NAMES
from methods.contrastive_training.contrastive_training import SupConLoss, ContrastiveLearningViewGenerator
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool

def load_model(model_path, device, feat_dim=768):
    """åŠ è½½æ¨¡å‹ï¼ˆä¸¤å¥—ä»£ç å…±ç”¨ï¼‰"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
    model = vits.__dict__['vit_base']()

    if model_path and os.path.exists(model_path):
        state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state_dict)
    else:
        print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨DINOé¢„è®­ç»ƒæƒé‡")
        state_dict = torch.load(dino_pretrain_path, map_location='cpu')
        model.load_state_dict(state_dict)

    model.to(device)
    model.eval()
    return model

def extract_features_method1_train_superclass(model, superclass_name, device):
    """
    æ–¹æ³•1ï¼šå®Œå…¨å¤åˆ¶train_superclass.pyä¸­çš„æ•°æ®è¯»å–å’Œç‰¹å¾æå–æ–¹å¼
    """
    print(f"\n" + "="*80)
    print(f"ğŸ§ª æ–¹æ³•1ï¼štrain_superclass.pyçš„ç‰¹å¾æå–æ–¹å¼")
    print("="*80)

    # æ¨¡æ‹Ÿtrain_superclass.pyä¸­çš„å‚æ•°è®¾ç½®
    class Args:
        def __init__(self):
            self.superclass_name = superclass_name
            self.dataset_name = 'cifar100_superclass'
            self.transform = 'imagenet'
            self.image_size = 32
            self.interpolation = 3
            self.crop_pct = 0.875
            self.prop_train_labels = 0.8
            self.seed = 1
            self.batch_size = 128
            self.num_workers = 8
            self.device = device
            self.n_views = 2

    args = Args()

    # è·å–ç±»åˆ«åˆ’åˆ†
    args = get_class_splits(args)
    args.num_labeled_classes = len(args.train_classes)
    args.num_unlabeled_classes = len(args.unlabeled_classes)

    print(f"   train_classes: {args.train_classes}")
    print(f"   unlabeled_classes: {args.unlabeled_classes}")

    # æ•°æ®å˜æ¢ï¼ˆå®Œå…¨æŒ‰ç…§train_superclass.pyï¼‰
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)
    train_transform = ContrastiveLearningViewGenerator(base_transform=train_transform, n_views=args.n_views)

    # æ•°æ®é›†ï¼ˆå®Œå…¨æŒ‰ç…§train_superclass.pyï¼‰
    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed
    )

    # æµ‹è¯•é›†ï¼ˆå®Œå…¨æŒ‰ç…§train_superclass.pyï¼‰
    test_dataset = datasets['test']
    test_loader_labelled = DataLoader(test_dataset, num_workers=args.num_workers,
                                     batch_size=args.batch_size, shuffle=False)

    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    print(f"   batch_size: {args.batch_size}")

    # ç‰¹å¾æå–ï¼ˆå®Œå…¨æŒ‰ç…§train_superclass.pyä¸­test_kmeans_superclassçš„æ–¹å¼ï¼‰
    model.eval()
    all_feats = []
    targets = np.array([])
    mask = np.array([])

    print('Collating features...')
    with torch.no_grad():
        for batch_idx, (images, label, _) in enumerate(tqdm(test_loader_labelled)):
            images = images.to(args.device)

            # Pass features through base model only (no projection head for evaluation)
            feats = model(images)
            feats = torch.nn.functional.normalize(feats, dim=-1)

            all_feats.append(feats.cpu().numpy())
            targets = np.append(targets, label.cpu().numpy())
            mask = np.append(mask, np.array([True if x.item() in range(len(args.train_classes))
                                             else False for x in label]))

    # åˆå¹¶ç‰¹å¾
    all_feats = np.concatenate(all_feats)
    mask = np.array(mask, dtype=bool)

    print(f"   æå–çš„ç‰¹å¾å½¢çŠ¶: {all_feats.shape}")
    print(f"   æ ‡ç­¾æ•°é‡: {len(targets)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬æ•°: {mask.sum()}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬æ•°: {(~mask).sum()}")

    return all_feats, targets, mask


def extract_features_method2_evaluate_model(model, superclass_name, device):
    """
    æ–¹æ³•2ï¼šå®Œå…¨å¤åˆ¶evaluate_superclass_model.pyä¸­çš„æ•°æ®è¯»å–å’Œç‰¹å¾æå–æ–¹å¼
    """
    print(f"\n" + "="*80)
    print(f"ğŸ§ª æ–¹æ³•2ï¼ševaluate_superclass_model.pyçš„ç‰¹å¾æå–æ–¹å¼")
    print("="*80)

    # æ¨¡æ‹Ÿevaluate_superclass_model.pyä¸­çš„å‚æ•°è®¾ç½®
    class Args:
        def __init__(self):
            self.superclass_name = superclass_name
            self.dataset_name = 'cifar100_superclass'
            self.transform = 'imagenet'
            self.image_size = 32
            self.interpolation = 3
            self.crop_pct = 0.875
            self.prop_train_labels = 0.8
            self.seed = 1
            self.batch_size = 128  # ä¸æ–¹æ³•1ä¿æŒä¸€è‡´ï¼Œæ’é™¤batch_sizeå½±å“
            self.num_workers = 8
            self.device = device

    args = Args()

    # è·å–ç±»åˆ«åˆ’åˆ†
    args = get_class_splits(args)
    args.num_labeled_classes = len(args.train_classes)
    args.num_unlabeled_classes = len(args.unlabeled_classes)

    print(f"   train_classes: {args.train_classes}")
    print(f"   unlabeled_classes: {args.unlabeled_classes}")

    # æ•°æ®å˜æ¢ï¼ˆæŒ‰ç…§evaluate_superclass_model.pyï¼‰
    train_transform, test_transform = get_transform(args.transform, image_size=args.image_size, args=args)

    # æ•°æ®é›†ï¼ˆæŒ‰ç…§evaluate_superclass_model.pyï¼‰
    datasets = get_single_superclass_datasets(
        superclass_name=args.superclass_name,
        train_transform=test_transform,  # è¯„ä¼°æ—¶éƒ½ä½¿ç”¨test_transform
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed
    )

    # æµ‹è¯•é›†ï¼ˆæŒ‰ç…§evaluate_superclass_model.pyï¼‰
    test_dataset = datasets['test']
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size,
                            shuffle=False, num_workers=args.num_workers)

    print(f"   æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_dataset)}")
    print(f"   batch_size: {args.batch_size}")

    # ç‰¹å¾æå–ï¼ˆæŒ‰ç…§evaluate_superclass_model.pyçš„extract_featuresæ–¹å¼ï¼‰
    model.eval()
    all_feats = []
    all_targets = []
    all_indices = []

    print('æå–ç‰¹å¾ï¼ˆä½¿ç”¨base modelï¼Œæ— æŠ•å½±å¤´ï¼‰...')
    with torch.no_grad():
        for batch_idx, (images, targets_batch, indices) in enumerate(tqdm(test_loader, desc="æå–ç‰¹å¾")):
            images = images.to(device)

            # ä»…ä½¿ç”¨åŸºç¡€æ¨¡å‹ç‰¹å¾æå–ï¼ˆä¸K-meansè¯„ä¼°ä¿æŒä¸€è‡´ï¼‰
            feats = model(images)

            # L2å½’ä¸€åŒ–
            feats = torch.nn.functional.normalize(feats, dim=-1)

            all_feats.append(feats.cpu().numpy())
            all_targets.append(targets_batch.numpy())
            all_indices.append(indices.numpy())

    # åˆå¹¶ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    all_targets = np.concatenate(all_targets, axis=0)
    all_indices = np.concatenate(all_indices, axis=0)

    # æ„å»ºmaskï¼ˆæŒ‰ç…§evaluate_superclass_model.pyçš„æ–¹å¼ï¼‰
    mask = np.array([True if x.item() in range(len(args.train_classes)) else False for x in all_targets])

    print(f"   æå–çš„ç‰¹å¾å½¢çŠ¶: {all_feats.shape}")
    print(f"   æ ‡ç­¾æ•°é‡: {len(all_targets)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬æ•°: {mask.sum()}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬æ•°: {(~mask).sum()}")

    return all_feats, all_targets, mask


def compare_features(features1, targets1, mask1, features2, targets2, mask2):
    """æ¯”è¾ƒä¸¤å¥—ç‰¹å¾æå–çš„ç»“æœ"""
    print(f"\n" + "="*80)
    print(f"ğŸ” ç‰¹å¾å¯¹æ¯”åˆ†æ")
    print("="*80)

    # åŸºæœ¬ç»Ÿè®¡å¯¹æ¯”
    print(f"æ–¹æ³•1 - ç‰¹å¾å½¢çŠ¶: {features1.shape}, æ ‡ç­¾æ•°é‡: {len(targets1)}, å·²çŸ¥ç±»: {mask1.sum()}")
    print(f"æ–¹æ³•2 - ç‰¹å¾å½¢çŠ¶: {features2.shape}, æ ‡ç­¾æ•°é‡: {len(targets2)}, å·²çŸ¥ç±»: {mask2.sum()}")

    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦ä¸€è‡´
    if features1.shape != features2.shape:
        print(f"âŒ ç‰¹å¾å½¢çŠ¶ä¸ä¸€è‡´ï¼")
        return False

    if len(targets1) != len(targets2):
        print(f"âŒ æ ‡ç­¾æ•°é‡ä¸ä¸€è‡´ï¼")
        return False

    # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸€è‡´
    labels_match = np.array_equal(targets1, targets2)
    print(f"æ ‡ç­¾æ˜¯å¦ä¸€è‡´: {labels_match}")
    if not labels_match:
        print(f"âŒ æ ‡ç­¾ä¸ä¸€è‡´ï¼")
        print(f"æ–¹æ³•1å‰10ä¸ªæ ‡ç­¾: {targets1[:10]}")
        print(f"æ–¹æ³•2å‰10ä¸ªæ ‡ç­¾: {targets2[:10]}")
        return False

    # æ£€æŸ¥maskæ˜¯å¦ä¸€è‡´
    mask_match = np.array_equal(mask1, mask2)
    print(f"maskæ˜¯å¦ä¸€è‡´: {mask_match}")
    if not mask_match:
        print(f"âŒ maskä¸ä¸€è‡´ï¼")
        return False

    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å®Œå…¨ä¸€è‡´
    features_match = np.allclose(features1, features2, rtol=1e-6, atol=1e-8)
    print(f"ç‰¹å¾æ˜¯å¦ä¸€è‡´ï¼ˆå®¹å·®1e-6ï¼‰: {features_match}")

    if features_match:
        print(f"âœ… ä¸¤å¥—æ–¹æ³•çš„ç‰¹å¾æå–ç»“æœå®Œå…¨ä¸€è‡´ï¼")
    else:
        print(f"âŒ ç‰¹å¾ä¸ä¸€è‡´ï¼")

        # è¯¦ç»†åˆ†æå·®å¼‚
        diff = np.abs(features1 - features2)
        max_diff = np.max(diff)
        mean_diff = np.mean(diff)

        print(f"æœ€å¤§å·®å¼‚: {max_diff}")
        print(f"å¹³å‡å·®å¼‚: {mean_diff}")
        print(f"å·®å¼‚å¤§äº1e-6çš„å…ƒç´ æ•°é‡: {np.sum(diff > 1e-6)}")
        print(f"å·®å¼‚å¤§äº1e-4çš„å…ƒç´ æ•°é‡: {np.sum(diff > 1e-4)}")

        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„å·®å¼‚
        print(f"\nå‰5ä¸ªæ ·æœ¬çš„ç‰¹å¾å·®å¼‚:")
        for i in range(min(5, len(features1))):
            sample_diff = np.abs(features1[i] - features2[i])
            print(f"æ ·æœ¬{i}: æœ€å¤§å·®å¼‚={np.max(sample_diff):.8f}, å¹³å‡å·®å¼‚={np.mean(sample_diff):.8f}")

    return features_match


def main():
    parser = argparse.ArgumentParser(description='ç‰¹å¾æå–å¯¹æ¯”è„šæœ¬')

    parser.add_argument('--superclass_name', type=str, required=True,
                        help='è¶…ç±»åç§°', choices=SUPERCLASS_NAMES)
    parser.add_argument('--model_path', type=str, required=True,
                        help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--gpu', type=int, default=0,
                        help='GPUè®¾å¤‡ID')

    args = parser.parse_args()

    print("ğŸ” ç‰¹å¾æå–å¯¹æ¯”è„šæœ¬")
    print("="*80)
    print(f"è¶…ç±»: {args.superclass_name}")
    print(f"æ¨¡å‹: {args.model_path}")

    # è®¾ç½®è®¾å¤‡
    device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹
    model = load_model(args.model_path, device)

    # æ–¹æ³•1ï¼štrain_superclass.pyçš„æ–¹å¼
    features1, targets1, mask1 = extract_features_method1_train_superclass(
        model, args.superclass_name, device)

    # æ–¹æ³•2ï¼ševaluate_superclass_model.pyçš„æ–¹å¼
    features2, targets2, mask2 = extract_features_method2_evaluate_model(
        model, args.superclass_name, device)

    # å¯¹æ¯”ç»“æœ
    results_match = compare_features(features1, targets1, mask1,
                                   features2, targets2, mask2)

    if results_match:
        print(f"\nğŸ‰ ç»“è®ºï¼šä¸¤å¥—ä»£ç çš„ç‰¹å¾æå–ç»“æœå®Œå…¨ä¸€è‡´ï¼")
        print(f"   é—®é¢˜å¯èƒ½å‡ºç°åœ¨K-meansèšç±»æˆ–ACCè®¡ç®—éƒ¨åˆ†ã€‚")
    else:
        print(f"\nâš ï¸ ç»“è®ºï¼šä¸¤å¥—ä»£ç çš„ç‰¹å¾æå–ç»“æœä¸ä¸€è‡´ï¼")
        print(f"   è¿™å°±æ˜¯å¯¼è‡´ACCå·®å¼‚çš„æ ¹æœ¬åŸå› ã€‚")


if __name__ == "__main__":
    main()