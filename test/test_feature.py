#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç‰¹å¾æå–å’Œä¿å­˜å·¥å…·
å®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„ç‰¹å¾æå–é€»è¾‘ï¼Œæå–å¹¶ä¿å­˜ç‰¹å¾åˆ°æŒ‡å®šè·¯å¾„
ä¿å­˜çš„ç‰¹å¾å¯ä»¥æ–¹ä¾¿åç»­è®­ç»ƒæ–‡ä»¶è¯»å–ä½¿ç”¨
"""

import sys
import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import argparse
from copy import deepcopy
import pickle

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.augmentations import get_transform
from data.cifar100_superclass import CIFAR100_SUPERCLASSES, get_single_superclass_datasets
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool
from data.data_utils import MergedDataset


def set_deterministic_behavior():
    """è®¾ç½®ç¡®å®šæ€§è¡Œä¸º"""
    torch.manual_seed(0)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(0)
        torch.cuda.manual_seed_all(0)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def load_model(args, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„é€»è¾‘ï¼‰"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
    print(f"   æ¨¡å‹æ–‡ä»¶: {args.model_path}")
    print(f"   è®¾å¤‡: {device}")

    if args.base_model == 'vit_dino':
        model = vits.__dict__['vit_base']()

        # åŠ è½½DINOé¢„è®­ç»ƒæƒé‡
        if os.path.exists(dino_pretrain_path):
            print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡...")
            dino_state_dict = torch.load(dino_pretrain_path, map_location='cpu')
            model.load_state_dict(dino_state_dict, strict=False)

        # åŠ è½½è®­ç»ƒæƒé‡
        print(f"   åŠ è½½è®­ç»ƒæƒé‡...")
        gcd_state_dict = torch.load(args.model_path, map_location='cpu')
        model.load_state_dict(gcd_state_dict)

        model.to(device)

        # å…³é—­æ¢¯åº¦è®¡ç®—
        for param in model.parameters():
            param.requires_grad = False

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (ç‰¹å¾ç»´åº¦: {args.feat_dim})")
        return model
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.base_model}")


def extract_features(data_loader, model, device, known_classes=None, dataset_type="unknown", use_l2=True):
    """
    æå–ç‰¹å¾ï¼ˆå®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„ç‰¹å¾æå–é€»è¾‘ï¼‰

    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        model: ç‰¹å¾æå–æ¨¡å‹
        device: è®¾å¤‡
        known_classes: å·²çŸ¥ç±»åˆ«é›†åˆ
        dataset_type: æ•°æ®é›†ç±»å‹æ ‡è¯†ç¬¦
        use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ– (True=L2å½’ä¸€åŒ–, False=ä¸ä½¿ç”¨)

    Returns:
        features: ç‰¹å¾çŸ©é˜µ (n_samples, feat_dim)
        targets: çœŸå®æ ‡ç­¾
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç  (True=å·²çŸ¥ç±», False=æœªçŸ¥ç±»)
        labeled_mask: æœ‰æ ‡ç­¾æ©ç  (True=æœ‰æ ‡ç­¾, False=æ— æ ‡ç­¾)
    """
    l2_status = "L2å½’ä¸€åŒ–" if use_l2 else "æ— L2å½’ä¸€åŒ–"
    print(f"ğŸ”„ æå–{dataset_type}ç‰¹å¾ ({l2_status})...")

    model.eval()
    all_feats = []
    targets = np.array([])
    mask = np.array([])  # å·²çŸ¥ç±»åˆ«æ©ç 
    labeled_mask = np.array([])  # æœ‰æ ‡ç­¾æ©ç 

    with torch.no_grad():
        for batch_idx, batch_data in enumerate(tqdm(data_loader, desc=f"æå–{dataset_type}ç‰¹å¾")):
            # è§£åŒ…æ•°æ®ï¼ˆä¸test_adaptive_clustering.pyå®Œå…¨ä¸€è‡´ï¼‰
            if len(batch_data) == 4:
                images, labels, indices, labeled_or_not = batch_data
                labeled_batch = labeled_or_not.numpy().flatten().astype(bool)
            elif len(batch_data) == 3:
                images, labels, indices = batch_data
                # æµ‹è¯•é›†å…¨éƒ¨æ ‡è®°ä¸ºæ— æ ‡ç­¾
                labeled_batch = np.zeros(len(labels), dtype=bool)
            else:
                continue

            # æå–ç‰¹å¾
            images = images.to(device)
            feats = model(images)

            # æ ¹æ®use_l2å‚æ•°å†³å®šæ˜¯å¦è¿›è¡ŒL2å½’ä¸€åŒ–
            if use_l2:
                feats = torch.nn.functional.normalize(feats, dim=-1)

            # æ”¶é›†æ•°æ®
            all_feats.append(feats.cpu().numpy())
            targets = np.append(targets, labels.cpu().numpy())
            labeled_mask = np.append(labeled_mask, labeled_batch)

            # åˆ›å»ºå·²çŸ¥ç±»åˆ«æ©ç ï¼ˆæ ¹æ®known_classesåˆ—è¡¨ï¼‰
            if known_classes is not None:
                batch_mask = np.array([True if x.item() in known_classes else False for x in labels])
            else:
                # é»˜è®¤ï¼šå‰80ä¸ªç±»åˆ«æ˜¯å·²çŸ¥ç±»
                batch_mask = np.array([True if x.item() < 80 else False for x in labels])
            mask = np.append(mask, batch_mask)

            # æ¸…ç†GPUå†…å­˜
            del images, feats
            torch.cuda.empty_cache()

    # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
    all_feats = np.concatenate(all_feats, axis=0)
    print(f"âœ… {dataset_type}ç‰¹å¾æå–å®Œæˆ: {all_feats.shape}")

    return all_feats, targets.astype(int), mask.astype(bool), labeled_mask.astype(bool)


def extract_and_save_features(superclass_name, model_path, output_base_dir='/data/gjx/checkpoints/features', use_l2=True):
    """
    æå–å¹¶ä¿å­˜æŒ‡å®šè¶…ç±»çš„ç‰¹å¾

    Args:
        superclass_name: è¶…ç±»åç§°
        model_path: æ¨¡å‹è·¯å¾„
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ– (True=L2å½’ä¸€åŒ–, False=ä¸ä½¿ç”¨)
    """
    l2_status = "L2å½’ä¸€åŒ–" if use_l2 else "æ— L2å½’ä¸€åŒ–"
    print(f"ğŸš€ å¼€å§‹æå–å’Œä¿å­˜ç‰¹å¾ ({l2_status}) - è¶…ç±»: {superclass_name}")
    print("="*80)

    # è®¾ç½®å‚æ•°ï¼ˆå®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„å‚æ•°è®¾ç½®ï¼‰
    class Args:
        def __init__(self):
            self.dataset_name = 'cifar100_superclass'
            self.superclass_name = superclass_name
            self.prop_train_labels = 0.8
            self.image_size = 224
            self.num_workers = 4
            self.batch_size = 64
            self.base_model = 'vit_dino'
            self.feat_dim = 768
            self.model_path = model_path
            self.interpolation = 3
            self.crop_pct = 0.875
            self.seed = 0

    args = Args()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # è®¾ç½®ç¡®å®šæ€§è¡Œä¸º
    set_deterministic_behavior()

    # åŠ è½½æ¨¡å‹
    model = load_model(args, device)

    # è·å–è¶…ç±»ä¿¡æ¯ï¼ˆå®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„é€»è¾‘ï¼‰
    superclass_classes = set(CIFAR100_SUPERCLASSES[superclass_name])
    superclass_known_classes_orig = set([cls for cls in superclass_classes if cls < 80])
    superclass_unknown_classes_orig = set([cls for cls in superclass_classes if cls >= 80])

    # åˆ›å»ºæ ‡ç­¾æ˜ å°„ï¼ˆä¸è¶…ç±»æ•°æ®é›†ä¿æŒä¸€è‡´ï¼‰
    all_classes_sorted = sorted(list(superclass_classes))
    label_mapping = {orig_cls: new_cls for new_cls, orig_cls in enumerate(all_classes_sorted)}

    # æ˜ å°„åçš„å·²çŸ¥/æœªçŸ¥ç±»åˆ«ID
    known_classes_mapped = set([label_mapping[cls] for cls in superclass_known_classes_orig])
    unknown_classes_mapped = set([label_mapping[cls] for cls in superclass_unknown_classes_orig])

    print(f"ğŸ“Š è¶…ç±»ä¿¡æ¯:")
    print(f"   åŸå§‹å·²çŸ¥ç±»: {sorted(list(superclass_known_classes_orig))} -> æ˜ å°„å: {sorted(list(known_classes_mapped))}")
    print(f"   åŸå§‹æœªçŸ¥ç±»: {sorted(list(superclass_unknown_classes_orig))} -> æ˜ å°„å: {sorted(list(unknown_classes_mapped))}")

    # è·å–æ•°æ®ï¼ˆå®Œå…¨å¤åˆ¶test_adaptive_clustering.pyçš„æ•°æ®è·å–é€»è¾‘ï¼‰
    train_transform, test_transform = get_transform('imagenet', image_size=args.image_size, args=args)

    datasets = get_single_superclass_datasets(
        superclass_name=superclass_name,
        train_transform=train_transform,
        test_transform=test_transform,
        prop_train_labels=args.prop_train_labels,
        split_train_val=False,
        seed=args.seed
    )

    # å‡†å¤‡æ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆå¤åˆ¶train_and_testæ¨¡å¼ï¼‰
    train_dataset = datasets['train_labelled']
    unlabelled_train_dataset = datasets['train_unlabelled']
    test_dataset = datasets['test']

    # åˆ›å»ºMergedDataset
    merged_train_dataset = MergedDataset(
        labelled_dataset=deepcopy(train_dataset),
        unlabelled_dataset=deepcopy(unlabelled_train_dataset)
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(merged_train_dataset, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

    # æå–ç‰¹å¾
    print("ğŸ“Š æå–è®­ç»ƒé›†ç‰¹å¾...")
    train_feats, train_targets, train_known_mask, train_labeled_mask = extract_features(
        train_loader, model, device, known_classes_mapped, "è®­ç»ƒé›†", use_l2=use_l2
    )

    print("ğŸ“Š æå–æµ‹è¯•é›†ç‰¹å¾...")
    test_feats, test_targets, test_known_mask, test_labeled_mask = extract_features(
        test_loader, model, device, known_classes_mapped, "æµ‹è¯•é›†", use_l2=use_l2
    )

    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆä¸test_adaptive_clustering.pyä¸€è‡´ï¼‰
    all_feats = np.concatenate([train_feats, test_feats], axis=0)
    all_targets = np.concatenate([train_targets, test_targets], axis=0)
    all_known_mask = np.concatenate([train_known_mask, test_known_mask], axis=0)
    all_labeled_mask = np.concatenate([train_labeled_mask, test_labeled_mask], axis=0)

    print(f"ğŸ“Š åˆå¹¶åæ•°æ®ç»Ÿè®¡:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_feats)}")
    print(f"   å·²çŸ¥ç±»æ ·æœ¬: {np.sum(all_known_mask)}")
    print(f"   æœªçŸ¥ç±»æ ·æœ¬: {np.sum(~all_known_mask)}")
    print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {np.sum(all_labeled_mask)}")
    print(f"   æ— æ ‡ç­¾æ ·æœ¬: {np.sum(~all_labeled_mask)}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(output_base_dir, superclass_name)
    os.makedirs(output_dir, exist_ok=True)

    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    feature_data = {
        # å®Œæ•´æ•°æ®ï¼ˆè®­ç»ƒ+æµ‹è¯•ï¼‰
        'all_features': all_feats,
        'all_targets': all_targets,
        'all_known_mask': all_known_mask,
        'all_labeled_mask': all_labeled_mask,

        # åˆ†ç¦»çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®
        'train_features': train_feats,
        'train_targets': train_targets,
        'train_known_mask': train_known_mask,
        'train_labeled_mask': train_labeled_mask,
        'test_features': test_feats,
        'test_targets': test_targets,
        'test_known_mask': test_known_mask,
        'test_labeled_mask': test_labeled_mask,

        # å…ƒä¿¡æ¯
        'superclass_name': superclass_name,
        'known_classes_mapped': sorted(list(known_classes_mapped)),
        'unknown_classes_mapped': sorted(list(unknown_classes_mapped)),
        'known_classes_orig': sorted(list(superclass_known_classes_orig)),
        'unknown_classes_orig': sorted(list(superclass_unknown_classes_orig)),
        'label_mapping': label_mapping,
        'all_classes_sorted': all_classes_sorted,

        # æ•°æ®é›†åˆ†å‰²ä¿¡æ¯
        'train_size': len(train_feats),
        'test_size': len(test_feats),
        'total_size': len(all_feats),

        # æå–å‚æ•°
        'model_path': model_path,
        'feat_dim': args.feat_dim,
        'image_size': args.image_size,
        'prop_train_labels': args.prop_train_labels
    }

    # ä¿å­˜ç‰¹å¾æ•°æ®
    feature_file = os.path.join(output_dir, 'features.pkl')
    with open(feature_file, 'wb') as f:
        pickle.dump(feature_data, f)
    print(f"âœ… ç‰¹å¾æ•°æ®å·²ä¿å­˜åˆ°: {feature_file}")

    # ä¿å­˜æ–‡æœ¬æ‘˜è¦
    summary_file = os.path.join(output_dir, 'summary.txt')
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"è¶…ç±»ç‰¹å¾æå–æ‘˜è¦\n")
        f.write(f"================\n")
        f.write(f"è¶…ç±»åç§°: {superclass_name}\n")
        f.write(f"æ¨¡å‹è·¯å¾„: {model_path}\n")
        from datetime import datetime
        f.write(f"æå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write(f"æ•°æ®ç»Ÿè®¡:\n")
        f.write(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_feats)}\n")
        f.write(f"  æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_feats)}\n")
        f.write(f"  æ€»æ ·æœ¬æ•°: {len(all_feats)}\n")
        f.write(f"  ç‰¹å¾ç»´åº¦: {all_feats.shape[1]}\n\n")

        f.write(f"ç±»åˆ«ä¿¡æ¯:\n")
        f.write(f"  åŸå§‹å·²çŸ¥ç±»: {sorted(list(superclass_known_classes_orig))}\n")
        f.write(f"  åŸå§‹æœªçŸ¥ç±»: {sorted(list(superclass_unknown_classes_orig))}\n")
        f.write(f"  æ˜ å°„åå·²çŸ¥ç±»: {sorted(list(known_classes_mapped))}\n")
        f.write(f"  æ˜ å°„åæœªçŸ¥ç±»: {sorted(list(unknown_classes_mapped))}\n\n")

        f.write(f"æ ‡ç­¾ç»Ÿè®¡:\n")
        f.write(f"  å·²çŸ¥ç±»æ ·æœ¬: {np.sum(all_known_mask)} ({np.sum(all_known_mask)/len(all_feats)*100:.1f}%)\n")
        f.write(f"  æœªçŸ¥ç±»æ ·æœ¬: {np.sum(~all_known_mask)} ({np.sum(~all_known_mask)/len(all_feats)*100:.1f}%)\n")
        f.write(f"  æœ‰æ ‡ç­¾æ ·æœ¬: {np.sum(all_labeled_mask)} ({np.sum(all_labeled_mask)/len(all_feats)*100:.1f}%)\n")
        f.write(f"  æ— æ ‡ç­¾æ ·æœ¬: {np.sum(~all_labeled_mask)} ({np.sum(~all_labeled_mask)/len(all_feats)*100:.1f}%)\n")

    print(f"âœ… æ‘˜è¦ä¿¡æ¯å·²ä¿å­˜åˆ°: {summary_file}")

    print(f"\nğŸ¯ ç‰¹å¾æå–å®Œæˆ!")
    print(f"   è¶…ç±»: {superclass_name}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   ç‰¹å¾æ–‡ä»¶: features.pkl")
    print(f"   æ‘˜è¦æ–‡ä»¶: summary.txt")

    return feature_data


def load_saved_features(superclass_name, output_base_dir='/data/gjx/checkpoints/features'):
    """
    åŠ è½½å·²ä¿å­˜çš„ç‰¹å¾æ•°æ®

    Args:
        superclass_name: è¶…ç±»åç§°
        output_base_dir: ç‰¹å¾ä¿å­˜çš„åŸºç¡€ç›®å½•

    Returns:
        feature_data: ç‰¹å¾æ•°æ®å­—å…¸
    """
    feature_file = os.path.join(output_base_dir, superclass_name, 'features.pkl')

    if not os.path.exists(feature_file):
        raise FileNotFoundError(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {feature_file}")

    print(f"ğŸ“¥ åŠ è½½ç‰¹å¾æ•°æ®: {feature_file}")
    with open(feature_file, 'rb') as f:
        feature_data = pickle.load(f)

    print(f"âœ… ç‰¹å¾æ•°æ®åŠ è½½å®Œæˆ: {superclass_name}")
    print(f"   æ€»æ ·æœ¬æ•°: {feature_data['total_size']}")
    print(f"   ç‰¹å¾ç»´åº¦: {feature_data['all_features'].shape[1]}")

    return feature_data


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æå–å¹¶ä¿å­˜è¶…ç±»ç‰¹å¾ï¼ˆé€»è¾‘ä¸ test_adaptive_clustering.py ä¿æŒä¸€è‡´ï¼‰',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # å¿…è¦å‚æ•°
    parser.add_argument(
        '--model_path',
        type=str,
        default='/data1/jiangzhen/gjx/exp/newgpc/final/metric_learn_gcd/log/(14.09.2025_|_56.443)/checkpoints/model.pt',
        help='è®­ç»ƒå¥½çš„è¶…ç±»æ¨¡å‹æƒé‡è·¯å¾„ã€‚å¿…é¡»æ˜¯ä¸ test_adaptive_clustering.py å…¼å®¹çš„ ViT-DINO checkpointã€‚'
    )
    parser.add_argument(
        '--superclass_name',
        type=str,
        default='trees',
        help='ç›®æ ‡è¶…ç±»åç§°ï¼ˆCIFAR-100 è¶…ç±»ä¹‹ä¸€ï¼Œä¾‹å¦‚ treesã€flowers ç­‰ï¼‰ã€‚'
    )

    # å¯é€‰å‚æ•°
    parser.add_argument(
        '--load_only',
        type=str2bool,
        default=False,
        help='ä»…åŠ è½½å·²å­˜åœ¨çš„ features.pkl è€Œä¸é‡æ–°æå–ã€‚å½“ç¼“å­˜å·²å­˜åœ¨æ—¶å¯å¿«é€ŸæŸ¥çœ‹æ‘˜è¦ã€‚'
    )
    parser.add_argument(
        '--l2',
        type=str2bool,
        default=True,
        help='æ˜¯å¦åœ¨ä¿å­˜å‰å¯¹ç‰¹å¾åš L2 å½’ä¸€åŒ–ã€‚è¯¥å¼€å…³ä¼šå½±å“é»˜è®¤çš„è¾“å‡ºç›®å½•ï¼ˆfeatures vs features_nol2ï¼‰ã€‚'
    )
    parser.add_argument(
        '--output_base_dir',
        type=str,
        default=None,
        help='ç‰¹å¾ç¼“å­˜çš„æ ¹ç›®å½•ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨åˆ›å»º <root>/<superclass>/features.pklã€‚æœªæŒ‡å®šæ—¶æŒ‰ --l2 è‡ªåŠ¨è½åœ¨ /data/gjx/checkpoints/features[_nol2]ã€‚'
    )

    args = parser.parse_args()

    # æ ¹æ®l2å‚æ•°é€‰æ‹©è¾“å‡ºç›®å½•
    if args.output_base_dir:
        output_base_dir = args.output_base_dir
    else:
        output_base_dir = '/data/gjx/checkpoints/features' if args.l2 else '/data/gjx/checkpoints/features_nol2'

    print("ç‰¹å¾æå–å’Œä¿å­˜å·¥å…·")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¶…ç±»åç§°: {args.superclass_name}")
    print(f"ç‰¹å¾å½’ä¸€åŒ–: {'L2å½’ä¸€åŒ–' if args.l2 else 'æ— L2å½’ä¸€åŒ–'}")
    print(f"è¾“å‡ºç›®å½•: {output_base_dir}")
    print(f"ä»…åŠ è½½æ¨¡å¼: {args.load_only}")
    print("="*80)

    try:
        if args.load_only:
            # ä»…åŠ è½½æ¨¡å¼
            feature_data = load_saved_features(args.superclass_name, output_base_dir=output_base_dir)
            print(f"\nğŸ¯ ç‰¹å¾åŠ è½½å®Œæˆ!")
        else:
            # æå–å¹¶ä¿å­˜æ¨¡å¼
            feature_data = extract_and_save_features(
                superclass_name=args.superclass_name,
                model_path=args.model_path,
                output_base_dir=output_base_dir,
                use_l2=args.l2
            )

        # æ˜¾ç¤ºä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ ç‰¹å¾æ•°æ®ç»Ÿè®¡:")
        print(f"   ç‰¹å¾å½¢çŠ¶: {feature_data['all_features'].shape}")
        print(f"   å·²çŸ¥ç±»åˆ«: {len(feature_data['known_classes_mapped'])}ä¸ª")
        print(f"   æœªçŸ¥ç±»åˆ«: {len(feature_data['unknown_classes_mapped'])}ä¸ª")

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
