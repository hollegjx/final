#!/usr/bin/env python3
"""
ä½¿ç”¨éª¨å¹²ç½‘ç»œç‰¹å¾è¿›è¡Œè‡ªé€‚åº”å¯†åº¦èšç±»
é›†æˆåˆ°GCDè®­ç»ƒæ–‡ä»¶ä¸­çš„èšç±»æ–¹æ¡ˆ
"""

import torch
import numpy as np
import argparse
from tqdm import tqdm
from adaptive_density_clustering import AdaptiveDensityClustering, evaluate_clustering_results
# å¯é€‰ç‰¹å¾å¢å¼ºæ¨¡å—ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡ï¼‰
try:
    from feature_enhancement import FeatureEnhancer, compute_class_separability
    FEATURE_ENHANCEMENT_AVAILABLE = True
except ImportError:
    print("âš ï¸ ç‰¹å¾å¢å¼ºæ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç‰¹å¾å¢å¼ºåŠŸèƒ½")
    FEATURE_ENHANCEMENT_AVAILABLE = False
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import CIFAR100_SUPERCLASSES
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool
from project_utils.cluster_and_log_utils import log_accs_from_preds


def extract_backbone_features(model, dataloader, device):
    """
    æå–éª¨å¹²ç½‘ç»œç‰¹å¾

    Args:
        model: è®­ç»ƒå¥½çš„éª¨å¹²ç½‘ç»œ
        dataloader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡

    Returns:
        features: ç‰¹å¾çŸ©é˜µ [n_samples, feat_dim]
        labels: çœŸå®æ ‡ç­¾
        indices: æ ·æœ¬ç´¢å¼•
    """
    model.eval()
    all_features = []
    all_labels = []
    all_indices = []

    print("ğŸ”„ æå–éª¨å¹²ç½‘ç»œç‰¹å¾...")

    with torch.no_grad():
        for batch_idx, (images, labels, indices) in enumerate(tqdm(dataloader)):
            images = images.to(device)

            # æå–backboneç‰¹å¾ï¼ˆä¸ä½¿ç”¨projection headï¼‰
            features = model(images)
            features = torch.nn.functional.normalize(features, dim=-1)

            all_features.append(features.cpu().numpy())
            all_labels.extend(labels.numpy())
            all_indices.extend(indices.numpy())

    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    features = np.concatenate(all_features, axis=0)
    labels = np.array(all_labels)
    indices = np.array(all_indices)

    print(f"âœ… æå–å®Œæˆ! ç‰¹å¾å½¢çŠ¶: {features.shape}")

    return features, labels, indices


def create_known_unknown_mask(labels, train_classes):
    """
    åˆ›å»ºå·²çŸ¥/æœªçŸ¥ç±»åˆ«æ©ç 

    Args:
        labels: çœŸå®æ ‡ç­¾
        train_classes: è®­ç»ƒæ—¶çš„å·²çŸ¥ç±»åˆ«

    Returns:
        mask: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºå·²çŸ¥ç±»åˆ«
        known_labels: å·²çŸ¥æ ‡ç­¾æ•°ç»„ï¼Œ-1è¡¨ç¤ºæœªçŸ¥
    """
    mask = np.array([label in train_classes for label in labels])
    known_labels = np.where(mask, labels, -1)

    print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    print(f"   å·²çŸ¥ç±»åˆ«æ ·æœ¬: {mask.sum()}")
    print(f"   æœªçŸ¥ç±»åˆ«æ ·æœ¬: {(~mask).sum()}")
    print(f"   å·²çŸ¥ç±»åˆ«: {sorted(list(train_classes))}")

    return mask, known_labels


def adaptive_density_clustering_test(model, test_loader, args, device):
    """
    ä½¿ç”¨è‡ªé€‚åº”å¯†åº¦èšç±»è¿›è¡Œæµ‹è¯•

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        args: å‚æ•°é…ç½®
        device: è®¾å¤‡

    Returns:
        clustering_results: èšç±»ç»“æœå­—å…¸
    """
    print("\n" + "="*80)
    print("ğŸ§  è‡ªé€‚åº”å¯†åº¦èšç±»æµ‹è¯•")
    print("="*80)

    # æå–ç‰¹å¾
    features, true_labels, sample_indices = extract_backbone_features(
        model, test_loader, device
    )

    # åˆ›å»ºå·²çŸ¥/æœªçŸ¥æ©ç 
    mask, known_labels = create_known_unknown_mask(true_labels, args.train_classes)

    # ç‰¹å¾å¢å¼ºï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
    if args.enable_feature_enhancement and FEATURE_ENHANCEMENT_AVAILABLE:
        print(f"\nğŸ”§ ç‰¹å¾å¢å¼ºå¤„ç†...")
        print(f"   å¢å¼ºæ–¹æ³•: {args.enhancement_method}")

        # è®¡ç®—åŸå§‹å¯åˆ†æ€§
        original_separability = compute_class_separability(features, true_labels)

        # åˆå§‹åŒ–ç‰¹å¾å¢å¼ºå™¨
        enhancer = FeatureEnhancer(
            enhancement_method=args.enhancement_method,
            push_strength=args.push_strength,
            pull_strength=args.pull_strength
        )

        # æ‰§è¡Œç‰¹å¾å¢å¼º
        enhanced_features = enhancer.fit_transform(features, known_labels)

        # è®¡ç®—å¢å¼ºåå¯åˆ†æ€§
        enhanced_separability = compute_class_separability(enhanced_features, true_labels)

        print(f"   åŸå§‹å¯åˆ†æ€§: {original_separability:.3f}")
        print(f"   å¢å¼ºå¯åˆ†æ€§: {enhanced_separability:.3f}")
        if original_separability > 0:
            improvement = enhanced_separability / original_separability
            print(f"   æ”¹è¿›å€æ•°: {improvement:.2f}x")

        features = enhanced_features
        print(f"âœ… ç‰¹å¾å¢å¼ºå®Œæˆ!")
    elif args.enable_feature_enhancement and not FEATURE_ENHANCEMENT_AVAILABLE:
        print("âš ï¸ ç‰¹å¾å¢å¼ºå·²å¯ç”¨ä½†æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡ç‰¹å¾å¢å¼ºæ­¥éª¤")

    # åˆå§‹åŒ–èšç±»å™¨ï¼ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–è‡ªé€‚åº”å€¼ï¼‰
    # è®¡ç®—è‡ªé€‚åº”å‚æ•°
    adaptive_k = max(5, min(20, int(len(features) * 0.1)))
    adaptive_min_size = 3

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–è‡ªé€‚åº”å€¼
    k_neighbors = args.k_neighbors if args.k_neighbors is not None else adaptive_k
    min_cluster_size = args.min_cluster_size if args.min_cluster_size is not None else adaptive_min_size

    clusterer = AdaptiveDensityClustering(
        k_neighbors=k_neighbors,
        density_percentile=args.density_percentile,
        lambda_weight=args.lambda_weight,
        min_cluster_size=min_cluster_size,
        standardize=args.standardize_features
    )

    print(f"ğŸ”§ èšç±»å‚æ•°:")
    print(f"   kè¿‘é‚»æ•°: {clusterer.k}")
    print(f"   å¯†åº¦é˜ˆå€¼: {clusterer.density_percentile}åˆ†ä½æ•°")
    print(f"   æƒé‡ç³»æ•°Î»: {clusterer.lambda_weight}")
    print(f"   æœ€å°èšç±»å¤§å°: {clusterer.min_cluster_size}")

    # æ‰§è¡Œèšç±»ï¼ˆä½¿ç”¨æ–°çš„start_new_clusteringæ–¹æ³•ï¼‰
    # å°†ç‰¹å¾åˆ†ä¸ºè®­ç»ƒå’ŒæŸ¥è¯¢éƒ¨åˆ†ï¼ˆæ¨¡æ‹ŸåŸå§‹å‡½æ•°çš„è¾“å…¥æ–¹å¼ï¼‰
    mid_point = len(features) // 2
    train_x = features[:mid_point]
    query_x = features[mid_point:]
    train_y = true_labels[:mid_point]
    query_y = true_labels[mid_point:]

    print(f"ğŸ”„ ä½¿ç”¨æ–°ç‰ˆæœ¬èšç±»ç®—æ³•...")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_x)}")
    print(f"   æŸ¥è¯¢æ ·æœ¬: {len(query_x)}")

    # è°ƒç”¨æ–°çš„èšç±»æ–¹æ³•
    acc, nmi, ari, sh = clusterer.start_new_clustering(train_x, train_y, query_x, query_y)

    # è·å–èšç±»é¢„æµ‹ç»“æœ
    cluster_predictions = clusterer.cluster_assignments

    # åŸºç¡€è¯„ä¼°ï¼ˆä½¿ç”¨æ–°ç®—æ³•å†…éƒ¨è®¡ç®—çš„æŒ‡æ ‡ï¼‰
    basic_metrics = {
        'accuracy': acc,
        'nmi': nmi,
        'ari': ari,
        'silhouette': sh,
        'n_clusters_predicted': len(clusterer.clusters),
        'n_clusters_true': len(set(true_labels))
    }

    # GCDé£æ ¼è¯„ä¼°
    all_acc, old_acc, new_acc = log_accs_from_preds(
        y_true=true_labels,
        y_pred=cluster_predictions,
        mask=mask,
        eval_funcs=args.eval_funcs,
        save_name='Adaptive_Density_Clustering_NewVersion',
        writer=args.writer
    )

    # è¯†åˆ«æœªçŸ¥èšç±»
    unknown_clusters = clusterer.get_unknown_clusters(known_labels)

    # åˆ†æèšç±»è´¨é‡
    cluster_analysis = analyze_cluster_quality(
        clusterer.clusters, features, true_labels, known_labels
    )

    results = {
        'cluster_predictions': cluster_predictions,
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'basic_metrics': basic_metrics,
        'unknown_clusters': unknown_clusters,
        'cluster_analysis': cluster_analysis,
        'n_clusters_found': len(clusterer.clusters)
    }

    print(f"\nğŸ“ˆ GCDè¯„ä¼°ç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")
    print(f"   å‘ç°èšç±»æ•°: {len(clusterer.clusters)}")
    print(f"   æœªçŸ¥èšç±»æ•°: {len(unknown_clusters)}")

    return results


def analyze_cluster_quality(clusters, features, true_labels, known_labels):
    """
    åˆ†æèšç±»è´¨é‡

    Args:
        clusters: èšç±»åˆ—è¡¨
        features: ç‰¹å¾çŸ©é˜µ
        true_labels: çœŸå®æ ‡ç­¾
        known_labels: å·²çŸ¥æ ‡ç­¾

    Returns:
        analysis: åˆ†æç»“æœå­—å…¸
    """
    analysis = {
        'cluster_sizes': [],
        'cluster_purities': [],
        'known_cluster_count': 0,
        'unknown_cluster_count': 0,
        'mixed_cluster_count': 0
    }

    for cluster_id, cluster in enumerate(clusters):
        if len(cluster) == 0:
            continue

        cluster_points = list(cluster)
        cluster_size = len(cluster_points)
        analysis['cluster_sizes'].append(cluster_size)

        # è®¡ç®—çº¯åº¦
        cluster_true_labels = true_labels[cluster_points]
        most_common_label = np.bincount(cluster_true_labels).argmax()
        purity = np.sum(cluster_true_labels == most_common_label) / cluster_size
        analysis['cluster_purities'].append(purity)

        # åˆ†æèšç±»ç±»å‹
        cluster_known_labels = known_labels[cluster_points]
        has_known = np.any(cluster_known_labels != -1)
        has_unknown = np.any(cluster_known_labels == -1)

        if has_known and not has_unknown:
            analysis['known_cluster_count'] += 1
        elif has_unknown and not has_known:
            analysis['unknown_cluster_count'] += 1
        else:
            analysis['mixed_cluster_count'] += 1

    # è®¡ç®—å¹³å‡å€¼
    if analysis['cluster_sizes']:
        analysis['avg_cluster_size'] = np.mean(analysis['cluster_sizes'])
        analysis['avg_purity'] = np.mean(analysis['cluster_purities'])
    else:
        analysis['avg_cluster_size'] = 0
        analysis['avg_purity'] = 0

    print(f"\nğŸ” èšç±»è´¨é‡åˆ†æ:")
    print(f"   å¹³å‡èšç±»å¤§å°: {analysis['avg_cluster_size']:.2f}")
    print(f"   å¹³å‡çº¯åº¦: {analysis['avg_purity']:.4f}")
    print(f"   çº¯å·²çŸ¥èšç±»: {analysis['known_cluster_count']}")
    print(f"   çº¯æœªçŸ¥èšç±»: {analysis['unknown_cluster_count']}")
    print(f"   æ··åˆèšç±»: {analysis['mixed_cluster_count']}")

    return analysis


def load_trained_model(model_path, args, device):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    print(f"ğŸ”„ åŠ è½½è®­ç»ƒæ¨¡å‹: {model_path}")

    if args.base_model == 'vit_dino':
        model = vits.__dict__['vit_base']()

        # åŠ è½½DINOé¢„è®­ç»ƒæƒé‡
        if hasattr(args, 'dino_pretrain_path'):
            dino_state_dict = torch.load(dino_pretrain_path, map_location='cpu')
            model.load_state_dict(dino_state_dict, strict=False)
            print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡")

        # åŠ è½½è®­ç»ƒåçš„æƒé‡
        trained_state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(trained_state_dict)
        print(f"   åŠ è½½è®­ç»ƒæƒé‡")

        model.to(device)

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼å¹¶å…³é—­æ¢¯åº¦
        model.eval()
        for param in model.parameters():
            param.requires_grad = False

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model

    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.base_model}")


def filter_superclass_data(data_loader, superclass_name):
    """
    è¿‡æ»¤å‡ºæŒ‡å®šè¶…ç±»çš„æ•°æ®

    Args:
        data_loader: æ•°æ®åŠ è½½å™¨
        superclass_name: è¶…ç±»åç§°

    Returns:
        filtered_data: (features, labels, indices, label_masks)
    """
    if superclass_name not in CIFAR100_SUPERCLASSES:
        print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{superclass_name}'")
        return None

    superclass_classes = set(CIFAR100_SUPERCLASSES[superclass_name])
    print(f"ğŸ“Š è¶…ç±» '{superclass_name}' åŒ…å«ç±»åˆ«: {sorted(list(superclass_classes))}")

    filtered_images = []
    filtered_labels = []
    filtered_indices = []
    filtered_label_masks = []

    try:
        for batch_idx, batch_data in enumerate(data_loader):
            # å°è¯•è§£åŒ…æ•°æ®
            try:
                if len(batch_data) == 4:
                    # 4å…ƒç´ æ ¼å¼ï¼š(images, labels, indices, labeled_or_not)
                    images, labels, indices, labeled_or_not = batch_data  # æå–æ ‡ç­¾mask
                elif len(batch_data) == 3:
                    images, labels, indices = batch_data
                    # æ²¡æœ‰æ ‡ç­¾maskï¼Œå‡è®¾éƒ½æ˜¯æœ‰æ ‡ç­¾çš„
                    labeled_or_not = torch.ones(len(labels), dtype=torch.long)
                elif len(batch_data) == 2:
                    images, labels = batch_data
                    # ç”Ÿæˆé»˜è®¤ç´¢å¼•å’Œæ ‡ç­¾mask
                    batch_size = len(labels)
                    start_idx = batch_idx * data_loader.batch_size
                    indices = torch.arange(start_idx, start_idx + batch_size)
                    labeled_or_not = torch.ones(len(labels), dtype=torch.long)
                else:
                    print(f"âš ï¸ è·³è¿‡å¼‚å¸¸batchï¼Œæ•°æ®å…ƒç´ æ•°é‡: {len(batch_data)}")
                    continue
            except Exception as e:
                print(f"âš ï¸ è§£åŒ…batchæ•°æ®æ—¶å‡ºé”™: {e}")
                continue

            # è¿‡æ»¤å‡ºå±äºè¯¥è¶…ç±»çš„æ ·æœ¬
            try:
                mask = torch.tensor([label.item() in superclass_classes for label in labels])

                if mask.any():
                    filtered_images.append(images[mask])
                    filtered_labels.extend(labels[mask].tolist())
                    filtered_indices.extend(indices[mask].tolist())
                    filtered_label_masks.extend(labeled_or_not[mask].tolist())
            except Exception as e:
                print(f"âš ï¸ è¿‡æ»¤æ•°æ®æ—¶å‡ºé”™: {e}")
                continue

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return None

    if not filtered_images:
        print(f"âŒ è¶…ç±» '{superclass_name}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ·æœ¬")
        return None

    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    all_images = torch.cat(filtered_images, dim=0)
    all_labels = torch.tensor(filtered_labels)
    all_indices = torch.tensor(filtered_indices)
    all_label_masks = torch.tensor(filtered_label_masks)

    print(f"âœ… è¶…ç±»æ•°æ®è¿‡æ»¤å®Œæˆ: {len(all_labels)} ä¸ªæ ·æœ¬")
    print(f"   æœ‰æ ‡ç­¾æ ·æœ¬: {all_label_masks.sum().item()}")
    print(f"   æ— æ ‡ç­¾æ ·æœ¬: {(all_label_masks == 0).sum().item()}")

    return all_images, all_labels, all_indices, all_label_masks


def evaluate_superclass_clustering(model, superclass_name, args, device):
    """
    åœ¨æŒ‡å®šè¶…ç±»ä¸Šè¿›è¡Œèšç±»è¯„ä¼° - å¢å¼ºç‰ˆæœ¬Aï¼šåˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        superclass_name: è¶…ç±»åç§°
        args: å‚æ•°é…ç½®
        device: è®¾å¤‡

    Returns:
        results: èšç±»ç»“æœ
    """
    print(f"\n" + "="*80)
    print(f"ğŸ¯ è¶…ç±» '{superclass_name}' å¢å¼ºç‰ˆè‡ªé€‚åº”å¯†åº¦èšç±»æµ‹è¯•")
    print("="*80)

    # è·å–å®Œæ•´æ•°æ®é›†ï¼ˆè®­ç»ƒ+æµ‹è¯•ï¼‰
    from data.augmentations import get_transform
    train_transform, test_transform = get_transform('imagenet', image_size=args.image_size, args=args)

    train_dataset, test_dataset, unlabelled_train_examples_test, datasets = get_datasets(
        args.dataset_name, train_transform, test_transform, args
    )

    # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    train_loader = torch.utils.data.DataLoader(
        train_dataset, num_workers=args.num_workers,
        batch_size=args.batch_size, shuffle=False
    )
    test_loader = torch.utils.data.DataLoader(
        test_dataset, num_workers=args.num_workers,
        batch_size=args.batch_size, shuffle=False
    )

    # è¿‡æ»¤è¶…ç±»æ•°æ® - è®­ç»ƒé›†
    train_superclass_data = filter_superclass_data(train_loader, superclass_name)
    if train_superclass_data is None:
        print(f"âŒ è®­ç»ƒé›†ä¸­æœªæ‰¾åˆ°è¶…ç±» '{superclass_name}' çš„æ•°æ®")
        return None

    train_images, train_labels, train_indices, train_label_masks = train_superclass_data

    # è¿‡æ»¤è¶…ç±»æ•°æ® - æµ‹è¯•é›†
    test_superclass_data = filter_superclass_data(test_loader, superclass_name)
    if test_superclass_data is None:
        print(f"âŒ æµ‹è¯•é›†ä¸­æœªæ‰¾åˆ°è¶…ç±» '{superclass_name}' çš„æ•°æ®")
        return None

    test_images, test_labels, test_indices, test_label_masks = test_superclass_data

    print(f"ğŸ“Š è¶…ç±»æ•°æ®ç»Ÿè®¡:")
    print(f"   è®­ç»ƒé›†æ ·æœ¬: {len(train_labels)}")
    print(f"   æµ‹è¯•é›†æ ·æœ¬: {len(test_labels)}")

    # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨
    train_superclass_dataset = torch.utils.data.TensorDataset(train_images, train_labels, train_indices)
    train_superclass_loader = torch.utils.data.DataLoader(
        train_superclass_dataset, batch_size=args.batch_size, shuffle=False
    )

    test_superclass_dataset = torch.utils.data.TensorDataset(test_images, test_labels, test_indices)
    test_superclass_loader = torch.utils.data.DataLoader(
        test_superclass_dataset, batch_size=args.batch_size, shuffle=False
    )

    # æå–è®­ç»ƒé›†ç‰¹å¾
    train_features, train_true_labels, train_sample_indices = extract_backbone_features(
        model, train_superclass_loader, device
    )

    # æå–æµ‹è¯•é›†ç‰¹å¾
    test_features, test_true_labels, test_sample_indices = extract_backbone_features(
        model, test_superclass_loader, device
    )

    # é‡æ–°æ˜ å°„æ ‡ç­¾åˆ°è¿ç»­çš„è¶…ç±»å†…æ ‡ç­¾
    superclass_classes = sorted(list(CIFAR100_SUPERCLASSES[superclass_name]))
    label_mapping = {old_label: new_label for new_label, old_label in enumerate(superclass_classes)}

    train_mapped_labels = np.array([label_mapping[label] for label in train_true_labels])
    test_mapped_labels = np.array([label_mapping[label] for label in test_true_labels])

    print(f"ğŸ“Š æ ‡ç­¾æ˜ å°„: {label_mapping}")
    print(f"ğŸ“Š è¶…ç±»åŒ…å«ç±»åˆ«: {len(superclass_classes)} ä¸ª")

    # åˆå§‹åŒ–å¢å¼ºç‰ˆèšç±»å™¨
    n_classes_in_superclass = len(superclass_classes)
    total_samples = len(train_features) + len(test_features)

    # è®¡ç®—è‡ªé€‚åº”å‚æ•°
    adaptive_k = max(3, min(10, int(total_samples * 0.05)))
    adaptive_min_size = max(2, int(total_samples * 0.01))

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–è‡ªé€‚åº”å€¼
    k_neighbors = args.k_neighbors if args.k_neighbors is not None else adaptive_k
    min_cluster_size = args.min_cluster_size if args.min_cluster_size is not None else adaptive_min_size

    clusterer = AdaptiveDensityClustering(
        k_neighbors=k_neighbors,
        density_percentile=args.density_percentile,
        lambda_weight=args.lambda_weight,
        min_cluster_size=min_cluster_size,
        standardize=args.standardize_features,
        unknown_threshold=0.3  # æ–°å¢ï¼šæœªçŸ¥æ£€æµ‹é˜ˆå€¼
    )

    print(f"ğŸ”§ å¢å¼ºç‰ˆè¶…ç±»èšç±»å‚æ•°:")
    print(f"   è¶…ç±»ç±»åˆ«æ•°: {n_classes_in_superclass}")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   kè¿‘é‚»æ•°: {clusterer.k}")
    print(f"   å¯†åº¦é˜ˆå€¼: {clusterer.density_percentile}åˆ†ä½æ•°")
    print(f"   Î»æƒé‡: {clusterer.lambda_weight}")
    print(f"   æœ€å°èšç±»å¤§å°: {clusterer.min_cluster_size}")
    print(f"   æœªçŸ¥æ£€æµ‹é˜ˆå€¼: {clusterer.unknown_threshold}")

    # æ‰§è¡Œå¢å¼ºç‰ˆèšç±»ï¼ˆåˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼‰
    print(f"ğŸš€ æ‰§è¡Œå¢å¼ºç‰ˆèšç±»ç®—æ³•...")

    # è°ƒç”¨å¢å¼ºç‰ˆæ–¹æ³•ï¼ˆä¼ å…¥æ ‡ç­¾æ©ç ä¿¡æ¯ï¼‰
    test_predictions, test_acc, test_nmi, test_ari = clusterer.enhanced_fit_predict(
        train_features, train_mapped_labels, train_label_masks,
        test_features, test_mapped_labels, test_label_masks,
        set(args.train_classes)  # ä¼ å…¥å·²çŸ¥ç±»åˆ«é›†åˆ
    )

    # åŸºç¡€è¯„ä¼°ï¼ˆä½¿ç”¨å¢å¼ºç®—æ³•çš„æµ‹è¯•é›†è¯„ä¼°æŒ‡æ ‡ï¼‰
    basic_metrics = {
        'test_accuracy': test_acc,
        'test_nmi': test_nmi,
        'test_ari': test_ari,
        'n_clusters_predicted': len(clusterer.clusters),
        'n_classes_true': n_classes_in_superclass,
        'train_samples': len(train_features),
        'test_samples': len(test_features)
    }

    # ä¸ºGCDè¯„ä¼°åˆ›å»ºæ©ç ï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰
    # æ£€æŸ¥æµ‹è¯•é›†ä¸­æ˜¯å¦æœ‰æœªçŸ¥ç±»åˆ«æ ·æœ¬
    test_train_classes_mapped = set()
    for orig_class in args.train_classes:
        if orig_class in label_mapping:
            test_train_classes_mapped.add(label_mapping[orig_class])

    test_mask = np.array([label in test_train_classes_mapped for label in test_mapped_labels])
    test_known_labels = np.where(test_mask, test_mapped_labels, -1)

    has_unknown_classes = (~test_mask).sum() > 0

    print(f"ğŸ“Š æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"   å·²çŸ¥ç±»åˆ«æ ·æœ¬: {test_mask.sum()}")
    print(f"   æœªçŸ¥ç±»åˆ«æ ·æœ¬: {(~test_mask).sum()}")

    if has_unknown_classes:
        # æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šæ­£å¸¸è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆåªé’ˆå¯¹æµ‹è¯•é›†ï¼‰
        all_acc, old_acc, new_acc = log_accs_from_preds(
            y_true=test_mapped_labels,
            y_pred=test_predictions,
            mask=test_mask,
            eval_funcs=args.eval_funcs,
            save_name=f'Enhanced_Superclass_{superclass_name}_TestOnly',
            writer=args.writer
        )
    else:
        # æ²¡æœ‰æœªçŸ¥ç±»çš„æƒ…å†µï¼šåªè®¡ç®—å·²çŸ¥ç±»å‡†ç¡®ç‡
        from project_utils.cluster_utils import cluster_acc
        old_acc = cluster_acc(test_mapped_labels, test_predictions)
        all_acc = old_acc  # å½“æ²¡æœ‰æœªçŸ¥ç±»æ—¶ï¼ŒAll ACC = Old ACC
        new_acc = 0.0      # æ²¡æœ‰æœªçŸ¥ç±»ï¼ŒNew ACCä¸º0

        print(f"âš ï¸ è¶…ç±» '{superclass_name}' æµ‹è¯•é›†ä¸­æ²¡æœ‰æœªçŸ¥ç±»åˆ«æ ·æœ¬")

    # è¯†åˆ«æœªçŸ¥èšç±»ï¼ˆåŸºäºå…¨ä½“æ•°æ®çš„å·²çŸ¥æ ‡ç­¾ä¿¡æ¯ï¼‰
    # åˆ›å»ºå…¨ä½“æ•°æ®çš„å·²çŸ¥æ ‡ç­¾æ©ç 
    all_features = np.concatenate([train_features, test_features], axis=0)
    all_true_labels = np.concatenate([train_mapped_labels, test_mapped_labels], axis=0)

    # ä¸ºå…¨ä½“æ•°æ®åˆ›å»ºå·²çŸ¥æ ‡ç­¾æ©ç 
    all_train_classes_mapped = set()
    for orig_class in args.train_classes:
        if orig_class in label_mapping:
            all_train_classes_mapped.add(label_mapping[orig_class])

    all_mask = np.array([label in all_train_classes_mapped for label in all_true_labels])
    all_known_labels = np.where(all_mask, all_true_labels, -1)

    unknown_clusters = clusterer.get_unknown_clusters(all_known_labels)

    # åˆ†æèšç±»è´¨é‡ï¼ˆåŸºäºå…¨ä½“æ•°æ®ï¼‰
    cluster_analysis = analyze_cluster_quality(
        clusterer.clusters, all_features, all_true_labels, all_known_labels
    )

    results = {
        'superclass_name': superclass_name,
        'n_classes': n_classes_in_superclass,
        'n_train_samples': len(train_features),
        'n_test_samples': len(test_features),
        'n_total_samples': len(all_features),
        'test_predictions': test_predictions,
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'basic_metrics': basic_metrics,
        'unknown_clusters': unknown_clusters,
        'cluster_analysis': cluster_analysis,
        'n_clusters_found': len(clusterer.clusters)
    }

    print(f"\nğŸ“ˆ è¶…ç±» '{superclass_name}' å¢å¼ºç‰ˆèšç±»ç»“æœ:")
    print(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(train_features)}")
    print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(test_features)}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(all_features)}")
    print(f"   çœŸå®ç±»åˆ«æ•°: {n_classes_in_superclass}")
    print(f"   å‘ç°èšç±»æ•°: {len(clusterer.clusters)}")
    print(f"   æœªçŸ¥èšç±»æ•°: {len(unknown_clusters)}")
    print(f"   ")
    print(f"ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")
    print(f"   Test ACC: {test_acc:.4f}")
    print(f"   Test NMI: {test_nmi:.4f}")
    print(f"   Test ARI: {test_ari:.4f}")

    # è¾“å‡ºæ¯ä¸ªèšç±»çš„è¯¦ç»†æ ‡ç­¾å æ¯”ï¼ˆåŸºäºå…¨ä½“æ•°æ®ï¼‰
    print_enhanced_cluster_analysis(clusterer.clusters, all_true_labels, all_known_labels,
                                   len(train_features), superclass_classes, superclass_name)

    return results


def evaluate_all_superclasses_clustering(model, args, device):
    """
    æ‰¹é‡è¯„ä¼°æ‰€æœ‰è¶…ç±»çš„èšç±»æ€§èƒ½

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        args: å‚æ•°é…ç½®
        device: è®¾å¤‡

    Returns:
        all_results: æ‰€æœ‰è¶…ç±»çš„è¯„ä¼°ç»“æœ
    """
    print("\n" + "="*80)
    print("ğŸ” æ‰€æœ‰è¶…ç±»æ‰¹é‡èšç±»è¯„ä¼°")
    print("="*80)

    all_results = {}

    for superclass_name in CIFAR100_SUPERCLASSES.keys():
        try:
            print(f"\n{'='*20} è¯„ä¼°è¶…ç±»: {superclass_name} {'='*20}")

            result = evaluate_superclass_clustering(model, superclass_name, args, device)

            if result is not None:
                all_results[superclass_name] = result
                print(f"âœ… {superclass_name}: All {result['all_acc']:.4f} | "
                      f"Old {result['old_acc']:.4f} | New {result['new_acc']:.4f}")
            else:
                print(f"âŒ {superclass_name}: è¯„ä¼°å¤±è´¥")

        except Exception as e:
            print(f"âŒ {superclass_name}: è¯„ä¼°å‡ºé”™ - {e}")
            import traceback
            traceback.print_exc()

    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š æ‰€æœ‰è¶…ç±»èšç±»è¯„ä¼°æ±‡æ€»:")
    print(f"{'è¶…ç±»åç§°':<25} {'æ ·æœ¬æ•°':<8} {'ç±»åˆ«æ•°':<8} {'èšç±»æ•°':<8} {'All ACC':<10} {'Old ACC':<10} {'New ACC':<10}")
    print("-" * 90)

    total_samples = 0
    avg_all_acc = 0
    valid_results = 0

    for superclass_name, result in all_results.items():
        print(f"{superclass_name:<25} {result['n_samples']:<8} {result['n_classes']:<8} "
              f"{result['n_clusters_found']:<8} {result['all_acc']:<10.4f} "
              f"{result['old_acc']:<10.4f} {result['new_acc']:<10.4f}")

        total_samples += result['n_samples']
        avg_all_acc += result['all_acc']
        valid_results += 1

    if valid_results > 0:
        avg_all_acc /= valid_results
        print("-" * 90)
        print(f"{'å¹³å‡å€¼':<25} {total_samples:<8} {'-':<8} {'-':<8} {avg_all_acc:<10.4f}")

    return all_results


def print_enhanced_cluster_analysis(clusters, true_labels, known_labels, train_size, superclass_classes, superclass_name):
    """
    è¾“å‡ºå¢å¼ºç‰ˆèšç±»åˆ†æï¼Œè¯¦ç»†åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ·æœ¬

    Args:
        clusters: èšç±»åˆ—è¡¨
        true_labels: çœŸå®æ ‡ç­¾æ•°ç»„ (è®­ç»ƒé›†+æµ‹è¯•é›†)
        known_labels: å·²çŸ¥æ ‡ç­¾æ©ç  (-1è¡¨ç¤ºæœªçŸ¥)
        train_size: è®­ç»ƒé›†å¤§å°
        superclass_classes: è¶…ç±»åŒ…å«çš„ç±»åˆ«åˆ—è¡¨
        superclass_name: è¶…ç±»åç§°
    """
    print(f"\nğŸ” å¢å¼ºç‰ˆèšç±»åˆ†æ - è¶…ç±» '{superclass_name}':")
    print("=" * 100)

    # ä¸ºæ¯ä¸ªèšç±»è¿›è¡Œè¯¦ç»†åˆ†æ
    for cluster_id, cluster in enumerate(clusters):
        if len(cluster) == 0:
            continue

        cluster_points = list(cluster)
        cluster_size = len(cluster_points)

        print(f"\nğŸ“Š èšç±» {cluster_id} (æ€»æ ·æœ¬æ•°: {cluster_size})")
        print("-" * 80)

        # åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ·æœ¬
        train_points = [p for p in cluster_points if p < train_size]
        test_points = [p for p in cluster_points if p >= train_size]

        print(f"æ ·æœ¬åˆ†å¸ƒ: è®­ç»ƒé›† {len(train_points)} ä¸ª, æµ‹è¯•é›† {len(test_points)} ä¸ª")

        # åˆ†ææ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æƒ…å†µ
        from collections import defaultdict

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æƒ…å†µ
        class_stats = defaultdict(lambda: {
            'train_known': 0, 'train_unknown': 0, 'test_known': 0, 'test_unknown': 0
        })

        # å¤„ç†è®­ç»ƒé›†æ ·æœ¬
        for point in train_points:
            true_label = true_labels[point]
            is_known = known_labels[point] != -1

            if is_known:
                class_stats[true_label]['train_known'] += 1
            else:
                class_stats[true_label]['train_unknown'] += 1

        # å¤„ç†æµ‹è¯•é›†æ ·æœ¬
        for point in test_points:
            true_label = true_labels[point]
            is_known = known_labels[point] != -1

            if is_known:
                class_stats[true_label]['test_known'] += 1
            else:
                class_stats[true_label]['test_unknown'] += 1

        # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
        print(f"\nç±»åˆ«è¯¦ç»†åˆ†æ:")
        print(f"{'ç±»åˆ«':<8} {'è®­ç»ƒå·²çŸ¥':<10} {'è®­ç»ƒæœªçŸ¥':<10} {'æµ‹è¯•å·²çŸ¥':<10} {'æµ‹è¯•æœªçŸ¥':<10} {'æ€»è®¡':<8} {'å æ¯”':<8}")
        print("-" * 70)

        total_samples = 0
        for class_label in sorted(class_stats.keys()):
            stats = class_stats[class_label]
            class_total = sum(stats.values())
            percentage = class_total / cluster_size * 100
            total_samples += class_total

            print(f"{class_label:<8} {stats['train_known']:<10} {stats['train_unknown']:<10} "
                  f"{stats['test_known']:<10} {stats['test_unknown']:<10} "
                  f"{class_total:<8} {percentage:<7.1f}%")

        # æ±‡æ€»ç»Ÿè®¡
        total_train_known = sum(stats['train_known'] for stats in class_stats.values())
        total_train_unknown = sum(stats['train_unknown'] for stats in class_stats.values())
        total_test_known = sum(stats['test_known'] for stats in class_stats.values())
        total_test_unknown = sum(stats['test_unknown'] for stats in class_stats.values())

        print("-" * 70)
        print(f"{'æ±‡æ€»':<8} {total_train_known:<10} {total_train_unknown:<10} "
              f"{total_test_known:<10} {total_test_unknown:<10} "
              f"{cluster_size:<8} {'100.0%':<8}")

        # èšç±»ç‰¹å¾åˆ†æ
        print(f"\nèšç±»ç‰¹å¾:")

        # è®¡ç®—ä¸»å¯¼ç±»åˆ«
        dominant_class = max(class_stats.keys(), key=lambda x: sum(class_stats[x].values()))
        dominant_count = sum(class_stats[dominant_class].values())
        purity = dominant_count / cluster_size

        print(f"  ä¸»å¯¼ç±»åˆ«: {dominant_class} ({dominant_count}/{cluster_size} = {purity:.3f})")

        # åˆ¤æ–­èšç±»ç±»å‹
        if total_train_known > 0 and total_test_unknown > 0:
            cluster_type = "ğŸŸ¡ æ··åˆèšç±» (åŒ…å«è®­ç»ƒå·²çŸ¥ + æµ‹è¯•æœªçŸ¥)"
        elif total_train_known > 0 and total_test_unknown == 0:
            cluster_type = "ğŸŸ¢ å·²çŸ¥ç±»åˆ«èšç±» (ä¸»è¦ä¸ºè®­ç»ƒå·²çŸ¥æ ·æœ¬)"
        elif total_train_known == 0 and total_test_unknown > 0:
            cluster_type = "ğŸ”´ æ½œåœ¨æ–°ç±»åˆ«èšç±» (ä¸»è¦ä¸ºæµ‹è¯•æœªçŸ¥æ ·æœ¬)"
        elif total_train_unknown > 0:
            cluster_type = "ğŸŸ  è®­ç»ƒæœªçŸ¥èšç±» (åŒ…å«è®­ç»ƒæ—¶æœªçŸ¥æ ·æœ¬)"
        else:
            cluster_type = "âšª å…¶ä»–ç±»å‹èšç±»"

        print(f"  èšç±»ç±»å‹: {cluster_type}")

        # æ–°ç±»åˆ«å‘ç°æ½œåŠ›
        if total_test_unknown > 0:
            new_class_potential = total_test_unknown / cluster_size
            print(f"  æ–°ç±»åˆ«å‘ç°æ½œåŠ›: {new_class_potential:.3f} ({total_test_unknown}ä¸ªæµ‹è¯•æœªçŸ¥æ ·æœ¬)")

    # å…¨å±€ç»Ÿè®¡
    print(f"\nğŸ“ˆ å…¨å±€èšç±»ç»Ÿè®¡:")
    print("=" * 100)

    valid_clusters = [c for c in clusters if len(c) > 0]

    global_train_known = 0
    global_train_unknown = 0
    global_test_known = 0
    global_test_unknown = 0

    pure_known_clusters = 0
    pure_unknown_clusters = 0
    mixed_clusters = 0
    potential_new_class_clusters = 0

    for cluster in valid_clusters:
        cluster_points = list(cluster)

        train_points = [p for p in cluster_points if p < train_size]
        test_points = [p for p in cluster_points if p >= train_size]

        cluster_train_known = sum(1 for p in train_points if known_labels[p] != -1)
        cluster_train_unknown = len(train_points) - cluster_train_known
        cluster_test_known = sum(1 for p in test_points if known_labels[p] != -1)
        cluster_test_unknown = len(test_points) - cluster_test_known

        global_train_known += cluster_train_known
        global_train_unknown += cluster_train_unknown
        global_test_known += cluster_test_known
        global_test_unknown += cluster_test_unknown

        # èšç±»ç±»å‹åˆ†ç±»
        if cluster_train_known > 0 and cluster_test_unknown > 0:
            mixed_clusters += 1
        elif cluster_train_known > 0 and cluster_test_unknown == 0:
            pure_known_clusters += 1
        elif cluster_train_known == 0 and cluster_test_unknown > 0:
            potential_new_class_clusters += 1

    print(f"æ€»èšç±»æ•°: {len(valid_clusters)}")
    print(f"  ğŸŸ¢ çº¯å·²çŸ¥ç±»åˆ«èšç±»: {pure_known_clusters}")
    print(f"  ğŸ”´ æ½œåœ¨æ–°ç±»åˆ«èšç±»: {potential_new_class_clusters}")
    print(f"  ğŸŸ¡ æ··åˆç±»å‹èšç±»: {mixed_clusters}")
    print(f"")
    print(f"æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†å·²çŸ¥æ ·æœ¬: {global_train_known}")
    print(f"  è®­ç»ƒé›†æœªçŸ¥æ ·æœ¬: {global_train_unknown}")
    print(f"  æµ‹è¯•é›†å·²çŸ¥æ ·æœ¬: {global_test_known}")
    print(f"  æµ‹è¯•é›†æœªçŸ¥æ ·æœ¬: {global_test_unknown}")
    print(f"  ")
    print(f"æ–°ç±»åˆ«å‘ç°è¯„ä¼°:")
    total_test_samples = global_test_known + global_test_unknown
    if total_test_samples > 0:
        unknown_ratio = global_test_unknown / total_test_samples
        print(f"  æµ‹è¯•é›†æœªçŸ¥æ ·æœ¬æ¯”ä¾‹: {unknown_ratio:.3f} ({global_test_unknown}/{total_test_samples})")
        print(f"  æ½œåœ¨æ–°ç±»åˆ«èšç±»æ¯”ä¾‹: {potential_new_class_clusters/len(valid_clusters):.3f} ({potential_new_class_clusters}/{len(valid_clusters)})")

def print_cluster_label_distribution(clusters, mapped_labels, true_labels, superclass_classes, known_labels, superclass_name):
    """
    è¾“å‡ºæ¯ä¸ªèšç±»å†…éƒ¨çš„æ ‡ç­¾å æ¯”è¯¦æƒ…

    Args:
        clusters: èšç±»åˆ—è¡¨
        mapped_labels: æ˜ å°„åçš„æ ‡ç­¾ (0,1,2,...)
        true_labels: åŸå§‹æ ‡ç­¾ (CIFAR-100æ ‡ç­¾)
        superclass_classes: è¶…ç±»åŒ…å«çš„åŸå§‹ç±»åˆ«
        known_labels: å·²çŸ¥æ ‡ç­¾æ©ç 
        superclass_name: è¶…ç±»åç§°
    """
    print(f"\nğŸ” èšç±»å†…éƒ¨æ ‡ç­¾åˆ†å¸ƒè¯¦æƒ… - è¶…ç±» '{superclass_name}':")
    print("=" * 80)

    # åˆ›å»ºåŸå§‹æ ‡ç­¾åˆ°ç±»åˆ«åç§°çš„æ˜ å°„ï¼ˆå¦‚æœéœ€è¦ï¼‰
    cifar100_class_names = {
        # è¿™é‡Œå¯ä»¥æ·»åŠ CIFAR-100çš„ç±»åˆ«åç§°ï¼Œæš‚æ—¶ç”¨æ•°å­—
    }

    # ä¸ºæ¯ä¸ªèšç±»åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    for cluster_id, cluster in enumerate(clusters):
        if len(cluster) == 0:
            continue

        cluster_points = list(cluster)
        cluster_size = len(cluster_points)

        print(f"\nğŸ“Š èšç±» {cluster_id} (å¤§å°: {cluster_size})")
        print("-" * 50)

        # è·å–è¯¥èšç±»çš„æ ‡ç­¾ä¿¡æ¯
        cluster_mapped_labels = mapped_labels[cluster_points]
        cluster_true_labels = true_labels[cluster_points]
        cluster_known_labels = known_labels[cluster_points]

        # ç»Ÿè®¡å·²çŸ¥/æœªçŸ¥æ ·æœ¬
        known_count = np.sum(cluster_known_labels != -1)
        unknown_count = cluster_size - known_count

        print(f"å·²çŸ¥æ ·æœ¬: {known_count}/{cluster_size} ({known_count/cluster_size*100:.1f}%)")
        print(f"æœªçŸ¥æ ·æœ¬: {unknown_count}/{cluster_size} ({unknown_count/cluster_size*100:.1f}%)")

        # ç»Ÿè®¡åŸå§‹æ ‡ç­¾åˆ†å¸ƒ
        from collections import Counter
        true_label_counts = Counter(cluster_true_labels)

        print(f"\nåŸå§‹æ ‡ç­¾åˆ†å¸ƒ:")
        for orig_label, count in sorted(true_label_counts.items()):
            percentage = count / cluster_size * 100
            known_in_this_class = np.sum((cluster_true_labels == orig_label) &
                                       (cluster_known_labels != -1))
            unknown_in_this_class = count - known_in_this_class

            # è·å–ç±»åˆ«åç§°
            class_name = cifar100_class_names.get(orig_label, f"ç±»åˆ«{orig_label}")

            print(f"  {class_name} (æ ‡ç­¾{orig_label}): {count}æ ·æœ¬ ({percentage:.1f}%) "
                  f"[å·²çŸ¥:{known_in_this_class}, æœªçŸ¥:{unknown_in_this_class}]")

        # ç»Ÿè®¡æ˜ å°„åæ ‡ç­¾åˆ†å¸ƒ
        mapped_label_counts = Counter(cluster_mapped_labels)
        print(f"\nè¶…ç±»å†…æ ‡ç­¾åˆ†å¸ƒ:")
        for mapped_label, count in sorted(mapped_label_counts.items()):
            percentage = count / cluster_size * 100
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ ‡ç­¾
            orig_label = superclass_classes[mapped_label]
            print(f"  è¶…ç±»æ ‡ç­¾{mapped_label} (åŸå§‹{orig_label}): {count}æ ·æœ¬ ({percentage:.1f}%)")

        # è®¡ç®—èšç±»çº¯åº¦
        most_common_mapped = mapped_label_counts.most_common(1)[0]
        purity = most_common_mapped[1] / cluster_size
        dominant_mapped_label = most_common_mapped[0]
        dominant_orig_label = superclass_classes[dominant_mapped_label]

        print(f"\nèšç±»çº¯åº¦: {purity:.3f} (ä¸»å¯¼ç±»åˆ«: {dominant_orig_label})")

        # åˆ¤æ–­èšç±»ç±»å‹
        if known_count == 0:
            cluster_type = "ğŸ”´ çº¯æœªçŸ¥èšç±»"
        elif unknown_count == 0:
            cluster_type = "ğŸŸ¢ çº¯å·²çŸ¥èšç±»"
        else:
            cluster_type = "ğŸŸ¡ æ··åˆèšç±»"

        print(f"èšç±»ç±»å‹: {cluster_type}")

    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€»ä½“èšç±»ç»Ÿè®¡:")
    print("-" * 50)

    total_samples = len(mapped_labels)
    pure_known_clusters = 0
    pure_unknown_clusters = 0
    mixed_clusters = 0

    for cluster in clusters:
        if len(cluster) == 0:
            continue

        cluster_points = list(cluster)
        cluster_known_labels = known_labels[cluster_points]

        known_count = np.sum(cluster_known_labels != -1)
        unknown_count = len(cluster_points) - known_count

        if known_count == 0:
            pure_unknown_clusters += 1
        elif unknown_count == 0:
            pure_known_clusters += 1
        else:
            mixed_clusters += 1

    print(f"æ€»èšç±»æ•°: {len([c for c in clusters if len(c) > 0])}")
    print(f"çº¯å·²çŸ¥èšç±»: {pure_known_clusters}")
    print(f"çº¯æœªçŸ¥èšç±»: {pure_unknown_clusters}")
    print(f"æ··åˆèšç±»: {mixed_clusters}")

    # è®¡ç®—å¹³å‡èšç±»çº¯åº¦
    total_purity = 0
    valid_clusters = 0

    for cluster in clusters:
        if len(cluster) == 0:
            continue

        cluster_points = list(cluster)
        cluster_mapped_labels = mapped_labels[cluster_points]
        label_counts = Counter(cluster_mapped_labels)

        if label_counts:
            purity = max(label_counts.values()) / len(cluster_points)
            total_purity += purity
            valid_clusters += 1

    if valid_clusters > 0:
        avg_purity = total_purity / valid_clusters
        print(f"å¹³å‡èšç±»çº¯åº¦: {avg_purity:.3f}")

    print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä½¿ç”¨è‡ªé€‚åº”å¯†åº¦èšç±»æµ‹è¯•éª¨å¹²ç½‘ç»œ')

    # æ¨¡å‹é…ç½®
    parser.add_argument('--model_path', type=str, required=True, help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--base_model', type=str, default='vit_dino')
    parser.add_argument('--feat_dim', default=768, type=int)

    # æ•°æ®é›†é…ç½®
    parser.add_argument('--dataset_name', type=str, default='cifar100')
    parser.add_argument('--batch_size', default=128, type=int)
    parser.add_argument('--num_workers', default=8, type=int)
    parser.add_argument('--image_size', default=224, type=int)
    parser.add_argument('--prop_train_labels', type=float, default=0.5)
    parser.add_argument('--use_ssb_splits', type=str2bool, default=False)

    # è¯„ä¼°é…ç½®
    parser.add_argument('--eval_funcs', nargs='+', default=['v1'], help='è¯„ä¼°å‡½æ•°')
    parser.add_argument('--gpu', default=0, type=int, help='GPUè®¾å¤‡ID')

    # è¯„ä¼°æ¨¡å¼é…ç½®
    parser.add_argument('--eval_mode', type=str, choices=['full', 'superclass', 'all_superclasses'],
                        default='full', help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¶…ç±»åç§°è¿›è¡Œæµ‹è¯•')

    # èšç±»ç®—æ³•è¶…å‚æ•°é…ç½®
    parser.add_argument('--k_neighbors', type=int, default=None,
                        help='kè¿‘é‚»æ•°é‡ (é»˜è®¤è‡ªé€‚åº”: max(3, min(10, æ ·æœ¬æ•°*0.1)))')
    parser.add_argument('--density_percentile', type=int, default=70,
                        help='å¯†åº¦é˜ˆå€¼ç™¾åˆ†ä½æ•° (é»˜è®¤70)')
    parser.add_argument('--lambda_weight', type=float, default=0.7,
                        help='åŸå‹ç½®ä¿¡åº¦æƒé‡ (é»˜è®¤0.7)')
    parser.add_argument('--min_cluster_size', type=int, default=None,
                        help='æœ€å°èšç±»å¤§å° (é»˜è®¤è‡ªé€‚åº”: max(2, æ ·æœ¬æ•°*0.01))')
    parser.add_argument('--standardize_features', type=str2bool, default=True,
                        help='æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾ (é»˜è®¤True)')

    # ç‰¹å¾å¢å¼ºå‚æ•°
    parser.add_argument('--enable_feature_enhancement', type=str2bool, default=False,
                        help='æ˜¯å¦å¯ç”¨ç‰¹å¾å¢å¼º (é»˜è®¤False)')
    parser.add_argument('--enhancement_method', type=str, default='multi_scale',
                        choices=['prototype_push', 'contrastive_separation', 'dimension_weighting', 'multi_scale'],
                        help='ç‰¹å¾å¢å¼ºæ–¹æ³•')
    parser.add_argument('--push_strength', type=float, default=0.1,
                        help='åŸå‹æ¨ç¦»å¼ºåº¦')
    parser.add_argument('--pull_strength', type=float, default=0.2,
                        help='åŸå‹æ‹‰è¿‘å¼ºåº¦')

    args = parser.parse_args()

    # è®¾å¤‡é…ç½®
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
        print(f"ğŸ’» ä½¿ç”¨GPUè®¾å¤‡: cuda:{args.gpu}")
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    # è®¾ç½®å¿…è¦å‚æ•°
    args.device = device
    args.writer = None
    args.interpolation = 3
    args.crop_pct = 0.875

    # è·å–ç±»åˆ«åˆ’åˆ†
    args = get_class_splits(args)
    args.num_labeled_classes = len(args.train_classes)
    args.num_unlabeled_classes = len(args.unlabeled_classes)

    print(f"ğŸ“Š ç±»åˆ«ä¿¡æ¯:")
    print(f"   å·²çŸ¥ç±»åˆ«æ•°: {args.num_labeled_classes}")
    print(f"   æœªçŸ¥ç±»åˆ«æ•°: {args.num_unlabeled_classes}")

    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    model = load_trained_model(args.model_path, args, device)

    # æ ¹æ®è¯„ä¼°æ¨¡å¼æ‰§è¡Œç›¸åº”çš„æµ‹è¯•
    if args.eval_mode == 'full':
        # è·å–æ•°æ®é›†
        from data.augmentations import get_transform
        train_transform, test_transform = get_transform('imagenet', image_size=args.image_size, args=args)

        train_dataset, test_dataset, unlabelled_train_examples_test, datasets = get_datasets(
            args.dataset_name, train_transform, test_transform, args
        )

        test_loader = torch.utils.data.DataLoader(
            test_dataset, num_workers=args.num_workers,
            batch_size=args.batch_size, shuffle=False
        )

        # å®Œæ•´æ•°æ®é›†è¯„ä¼°
        results = adaptive_density_clustering_test(model, test_loader, args, device)

        print("\nğŸ‰ å®Œæ•´æ•°æ®é›†èšç±»æµ‹è¯•å®Œæˆ!")
        print(f"æœ€ç»ˆç»“æœ: All ACC: {results['all_acc']:.4f} | "
              f"Old ACC: {results['old_acc']:.4f} | "
              f"New ACC: {results['new_acc']:.4f}")

    elif args.eval_mode == 'superclass':
        # å•ä¸ªè¶…ç±»è¯„ä¼°
        if args.superclass_name is None:
            print("âŒ é”™è¯¯: è¶…ç±»è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --superclass_name")
            return

        results = evaluate_superclass_clustering(model, args.superclass_name, args, device)

        if results is not None:
            print(f"\nğŸ‰ è¶…ç±» '{args.superclass_name}' èšç±»æµ‹è¯•å®Œæˆ!")
            print(f"æœ€ç»ˆç»“æœ: All ACC: {results['all_acc']:.4f} | "
                  f"Old ACC: {results['old_acc']:.4f} | "
                  f"New ACC: {results['new_acc']:.4f}")

    elif args.eval_mode == 'all_superclasses':
        # æ‰€æœ‰è¶…ç±»æ‰¹é‡è¯„ä¼°
        results = evaluate_all_superclasses_clustering(model, args, device)

        print(f"\nğŸ‰ æ‰€æœ‰è¶…ç±»èšç±»æµ‹è¯•å®Œæˆ!")
        print(f"å…±è¯„ä¼°äº† {len(results)} ä¸ªè¶…ç±»")


if __name__ == "__main__":
    main()