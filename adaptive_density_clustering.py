#!/usr/bin/env python3
"""
è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•
åŸºäºæä¾›çš„åŸå§‹èšç±»é€»è¾‘é‡æ–°å®ç°ï¼Œå®Œå…¨ç¬¦åˆstart_newå‡½æ•°çš„ç‰ˆæœ¬
"""

import numpy as np
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import math


# è¾…åŠ©å‡½æ•°å®ç°
def euclidean_distance(point1, point2):
    """è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»"""
    return np.linalg.norm(point1 - point2)

def compute_and_identify_points(X, k, p):
    """
    è®¡ç®—è·ç¦»ã€é‚»å±…ç´¢å¼•ã€ç‚¹ç±»åˆ«ã€æ ¸å¿ƒç‚¹æ ‡è¯†å’Œå¯†åº¦

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        k: kè¿‘é‚»æ•°é‡
        p: å¯†åº¦é˜ˆå€¼ç™¾åˆ†ä½æ•°

    Returns:
        distances: è·ç¦»çŸ©é˜µ
        index_matrix: kè¿‘é‚»ç´¢å¼•çŸ©é˜µ
        point_categories: ç‚¹ç±»åˆ« (1=æ ¸å¿ƒç‚¹, 0=è¾¹ç•Œç‚¹)
        is_core: æ ¸å¿ƒç‚¹æ ‡è¯†
        delts: å¯†åº¦å€¼
    """
    n_samples = len(X)

    # è®¡ç®—æ‰€æœ‰ç‚¹ä¹‹é—´çš„è·ç¦»
    distances = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
            distances[i][j] = euclidean_distance(X[i], X[j])

    # è®¡ç®—kè¿‘é‚»ç´¢å¼•
    index_matrix = np.zeros((n_samples, k), dtype=int)
    for i in range(n_samples):
        # è·å–æœ€è¿‘çš„kä¸ªé‚»å±…ï¼ˆä¸åŒ…æ‹¬è‡ªèº«ï¼‰
        nearest_indices = np.argsort(distances[i])[1:k+1]
        index_matrix[i] = nearest_indices

    # è®¡ç®—å¯†åº¦ï¼ˆåŸºäºkè¿‘é‚»è·ç¦»çš„å€’æ•°ï¼‰
    delts = np.zeros(n_samples)
    for i in range(n_samples):
        knn_distances = distances[i][index_matrix[i]]
        avg_knn_dist = np.mean(knn_distances)
        delts[i] = 1.0 / (avg_knn_dist + 1e-10)

    # ç¡®å®šå¯†åº¦é˜ˆå€¼å¹¶è¯†åˆ«æ ¸å¿ƒç‚¹
    density_threshold = np.percentile(delts, p)
    is_core = delts > density_threshold
    point_categories = is_core.astype(int)

    return distances, index_matrix, point_categories, is_core, delts

def compute_knn_distances(index_matrix, distances):
    """è®¡ç®—kè¿‘é‚»è·ç¦»"""
    n_samples = len(index_matrix)
    knn_distances = []
    for i in range(n_samples):
        row_distances = [distances[i][j] for j in index_matrix[i]]
        knn_distances.append(row_distances)
    return knn_distances

def compute_proto(cluster, X):
    """è®¡ç®—èšç±»åŸå‹ï¼ˆè´¨å¿ƒï¼‰"""
    if len(cluster) == 0:
        return np.zeros(X.shape[1])
    cluster_points = list(cluster)
    return np.mean(X[cluster_points], axis=0)

def compute_confidence(point, clusters_x, clusters_label, total_clusters):
    """è®¡ç®—åŸºäºåŸå‹çš„ç½®ä¿¡åº¦"""
    confidences = []
    for cluster_id in total_clusters:
        if cluster_id < len(clusters_x):
            distance = euclidean_distance(point, clusters_x[cluster_id])
            confidence = 1.0 / (distance + 1e-10)
            confidences.append(confidence)
        else:
            confidences.append(0.0)

    # å½’ä¸€åŒ–
    total = sum(confidences)
    if total > 0:
        confidences = [c / total for c in confidences]

    return confidences

def compute_knn_confidence(point, X, pred, density, k=2):
    """è®¡ç®—åŸºäºkè¿‘é‚»çš„ç½®ä¿¡åº¦"""
    # è®¡ç®—åˆ°æ‰€æœ‰ç‚¹çš„è·ç¦»
    distances = [euclidean_distance(point, X[j]) for j in range(len(X))]

    # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
    knn_indices = np.argsort(distances)[:k]

    # ç»Ÿè®¡é‚»å±…çš„èšç±»åˆ†å¸ƒ
    neighbor_clusters = [pred[idx] for idx in knn_indices if pred[idx] != -1]

    if not neighbor_clusters:
        # å¦‚æœæ²¡æœ‰å·²åˆ†é…çš„é‚»å±…ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
        n_clusters = len(set(pred[pred != -1])) if len(set(pred[pred != -1])) > 0 else 1
        return [1.0 / n_clusters] * n_clusters

    # è®¡ç®—æ¯ä¸ªèšç±»çš„ç½®ä¿¡åº¦
    cluster_counts = Counter(neighbor_clusters)
    max_cluster = max(cluster_counts.keys()) if cluster_counts else 0
    confidences = []

    for cluster_id in range(max_cluster + 1):
        count = cluster_counts.get(cluster_id, 0)
        confidence = count / len(neighbor_clusters)
        confidences.append(confidence)

    return confidences

def calculate_accuracy(pred, true_labels):
    """è®¡ç®—èšç±»å‡†ç¡®ç‡"""
    from project_utils.cluster_utils import cluster_acc
    return cluster_acc(true_labels, pred)

def silhouette_score(X, clusters, x_cluster, clusters_num):
    """è®¡ç®—è½®å»“ç³»æ•°"""
    if clusters_num <= 1:
        return 0.0

    total_score = 0.0
    n_points = 0

    for i in range(len(X)):
        cluster_id = x_cluster[i]
        if cluster_id == -1 or len(clusters[cluster_id]) <= 1:
            continue

        # è®¡ç®—ç°‡å†…å¹³å‡è·ç¦»
        a_i = 0.0
        cluster_points = list(clusters[cluster_id])
        for j in cluster_points:
            if i != j:
                a_i += euclidean_distance(X[i], X[j])
        a_i /= (len(cluster_points) - 1)

        # è®¡ç®—åˆ°æœ€è¿‘ç°‡çš„å¹³å‡è·ç¦»
        b_i = float('inf')
        for other_cluster_id in range(clusters_num):
            if other_cluster_id != cluster_id and len(clusters[other_cluster_id]) > 0:
                avg_dist = 0.0
                other_points = list(clusters[other_cluster_id])
                for j in other_points:
                    avg_dist += euclidean_distance(X[i], X[j])
                avg_dist /= len(other_points)
                b_i = min(b_i, avg_dist)

        # è®¡ç®—è½®å»“ç³»æ•°
        if b_i != float('inf'):
            s_i = (b_i - a_i) / max(a_i, b_i)
            total_score += s_i
            n_points += 1

    return total_score / n_points if n_points > 0 else 0.0

class AdaptiveDensityClustering:
    """
    è‡ªé€‚åº”å¯†åº¦èšç±»å™¨ - ç‰ˆæœ¬Aï¼šèåˆstart_newç²¾ç»†ç®—æ³• + ä¿æŒGCDæœªçŸ¥ç±»åˆ«å‘ç°
    """

    def __init__(self, k_neighbors=3, density_percentile=70, lambda_weight=0.7,
                 min_cluster_size=3, standardize=True, unknown_threshold=0.3):
        """
        åˆå§‹åŒ–èšç±»å‚æ•°

        Args:
            k_neighbors: kè¿‘é‚»æ•°é‡
            density_percentile: å¯†åº¦é˜ˆå€¼ç™¾åˆ†ä½æ•° (å¯¹åº”åŸå§‹ä»£ç ä¸­çš„på‚æ•°)
            lambda_weight: åŸå‹ç½®ä¿¡åº¦ä¸knnç½®ä¿¡åº¦çš„æƒé‡ (å¯¹åº”åŸå§‹ä»£ç ä¸­çš„lamdaå‚æ•°)
            min_cluster_size: æœ€å°èšç±»å¤§å°
            standardize: æ˜¯å¦æ ‡å‡†åŒ–ç‰¹å¾
            unknown_threshold: æœªçŸ¥ç±»åˆ«æ£€æµ‹é˜ˆå€¼
        """
        self.k = k_neighbors
        self.density_percentile = density_percentile
        self.lambda_weight = lambda_weight
        self.min_cluster_size = min_cluster_size
        self.standardize = standardize
        self.unknown_threshold = unknown_threshold

        # èšç±»ç»“æœ
        self.clusters = []
        self.cluster_assignments = None
        self.cluster_prototypes = []
        self.densities = None
        self.train_size = 0  # è®°å½•è®­ç»ƒé›†å¤§å°ï¼Œç”¨äºåˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•ç»“æœ

    def enhanced_fit_predict(self, train_x, train_y, train_label_masks, test_x, test_y, test_label_masks, train_classes):
        """
        æ”¹è¿›ç‰ˆæœ¬Aï¼šåˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œèšç±»ï¼Œä¿æŒGCDæœªçŸ¥ç±»åˆ«å‘ç°èƒ½åŠ›

        Args:
            train_x: è®­ç»ƒç‰¹å¾
            train_y: è®­ç»ƒæ ‡ç­¾
            train_label_masks: è®­ç»ƒé›†æ ‡ç­¾æ©ç ï¼ˆ1=æœ‰æ ‡ç­¾ï¼Œ0=æ— æ ‡ç­¾ï¼‰
            test_x: æµ‹è¯•ç‰¹å¾
            test_y: æµ‹è¯•æ ‡ç­¾
            test_label_masks: æµ‹è¯•é›†æ ‡ç­¾æ©ç ï¼ˆ1=æœ‰æ ‡ç­¾ï¼Œ0=æ— æ ‡ç­¾ï¼‰
            train_classes: è®­ç»ƒæ—¶çš„å·²çŸ¥ç±»åˆ«é›†åˆ

        Returns:
            test_predictions: æµ‹è¯•é›†é¢„æµ‹ç»“æœ
            test_acc, test_nmi, test_ari: æµ‹è¯•é›†è¯„ä¼°æŒ‡æ ‡
        """
        print("Starting enhanced adaptive density clustering...")

        # 1. åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        X = np.concatenate((train_x, test_x), axis=0)
        Y = np.concatenate((train_y, test_y), axis=0)

        # ç¡®ä¿label_masksæ˜¯1Dæ•°ç»„
        train_label_masks = np.array(train_label_masks).flatten()
        test_label_masks = np.array(test_label_masks).flatten()
        label_masks = np.concatenate((train_label_masks, test_label_masks), axis=0)
        self.train_size = len(train_x)

        print(f"Data info:")
        print(f"   Train samples: {len(train_x)}")
        print(f"   Test samples: {len(test_x)}")
        print(f"   Total samples: {len(X)}")
        print(f"   Feature dimensions: {X.shape[1]}")

        # 2. åˆ›å»ºå·²çŸ¥/æœªçŸ¥æ ‡ç­¾æ©ç ï¼ˆå…³é”®æ”¹è¿›ï¼‰
        known_labels = self._create_enhanced_known_labels(Y, train_classes, label_masks)

        # 3. ç‰¹å¾æ ‡å‡†åŒ–
        if self.standardize:
            scaler = StandardScaler()
            X = scaler.fit_transform(X)
            print("Features standardized")

        # 4. ä½¿ç”¨æ”¹è¿›çš„start_newé£æ ¼èšç±»ç®—æ³•
        cluster_predictions = self._enhanced_clustering_algorithm(X, known_labels)

        # 5. åªè¯„ä¼°æµ‹è¯•é›†éƒ¨åˆ†
        test_predictions = cluster_predictions[self.train_size:]
        test_true_labels = test_y

        # 6. è®¡ç®—æµ‹è¯•é›†è¯„ä¼°æŒ‡æ ‡
        test_acc = calculate_accuracy(test_predictions, test_true_labels)
        test_nmi = normalized_mutual_info_score(test_true_labels, test_predictions)
        test_ari = adjusted_rand_score(test_true_labels, test_predictions)

        print(f"Clustering completed!")
        print(f"Test set results:")
        print(f"   Clusters found: {len(self.clusters)}")
        print(f"   Test accuracy: {test_acc:.4f}")
        print(f"   Test NMI: {test_nmi:.4f}")
        print(f"   Test ARI: {test_ari:.4f}")

        return test_predictions, test_acc, test_nmi, test_ari

    def _create_enhanced_known_labels(self, Y, train_classes, label_masks):
        """
        åˆ›å»ºå¢å¼ºçš„å·²çŸ¥æ ‡ç­¾æ•°ç»„

        å…³é”®ç‚¹ï¼š
        1. åªæœ‰å½“æ ·æœ¬æ—¢å±äºå·²çŸ¥ç±»åˆ«åˆåœ¨label_maskä¸­æ ‡è®°ä¸ºæœ‰æ ‡ç­¾æ—¶ï¼Œæ‰ä¿æŒåŸæ ‡ç­¾
        2. å…¶ä»–æ‰€æœ‰æƒ…å†µï¼ˆæœªçŸ¥ç±»åˆ«æˆ–æ— æ ‡ç­¾ï¼‰éƒ½æ ‡è®°ä¸º-1

        Args:
            Y: æ‰€æœ‰æ ‡ç­¾
            train_classes: å·²çŸ¥ç±»åˆ«é›†åˆ
            label_masks: æ ‡ç­¾æ©ç ï¼ˆ1=æœ‰æ ‡ç­¾ï¼Œ0=æ— æ ‡ç­¾ï¼‰
        """
        known_labels = np.full(len(Y), -1, dtype=int)

        train_labeled_known_count = 0
        train_unlabeled_known_count = 0
        train_labeled_unknown_count = 0
        train_unlabeled_unknown_count = 0
        test_labeled_known_count = 0
        test_unlabeled_known_count = 0
        test_labeled_unknown_count = 0
        test_unlabeled_unknown_count = 0

        # å¤„ç†æ‰€æœ‰æ ·æœ¬
        for i in range(len(Y)):
            is_known_class = Y[i] in train_classes
            is_labeled = label_masks[i] == 1
            is_train = i < self.train_size

            # GCDè®¾ç½®ï¼šåªæœ‰è®­ç»ƒé›†ä¸­æœ‰æ ‡ç­¾çš„å·²çŸ¥ç±»åˆ«æ ·æœ¬æ‰ä¿æŒåŸæ ‡ç­¾
            # æµ‹è¯•é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½è¢«è§†ä¸º"æœªçŸ¥"ï¼Œç”¨äºèšç±»å‘ç°
            if is_train and is_known_class and is_labeled:
                known_labels[i] = Y[i]
                train_labeled_known_count += 1
            else:
                known_labels[i] = -1
                if is_train:
                    if is_known_class:
                        train_unlabeled_known_count += 1
                    else:
                        if is_labeled:
                            train_labeled_unknown_count += 1
                        else:
                            train_unlabeled_unknown_count += 1
                else:
                    # æµ‹è¯•é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½è¢«è§†ä¸º"æœªçŸ¥"
                    if is_known_class:
                        test_unlabeled_known_count += 1
                    else:
                        test_unlabeled_unknown_count += 1

        print(f"Label mask analysis:")
        print(f"   Train - Known classes with labels: {train_labeled_known_count}")
        print(f"   Train - Known classes without labels: {train_unlabeled_known_count}")
        print(f"   Train - Unknown classes with labels: {train_labeled_unknown_count}")
        print(f"   Train - Unknown classes without labels: {train_unlabeled_unknown_count}")
        print(f"   Test - Known classes with labels: {test_labeled_known_count}")
        print(f"   Test - Known classes without labels: {test_unlabeled_known_count}")
        print(f"   Test - Unknown classes with labels: {test_labeled_unknown_count}")
        print(f"   Test - Unknown classes without labels: {test_unlabeled_unknown_count}")

        return known_labels

    def _enhanced_clustering_algorithm(self, X, known_labels):
        """
        å¢å¼ºç‰ˆèšç±»ç®—æ³•ï¼šèåˆstart_newçš„ç²¾ç»†ç®—æ³• + GCDæœªçŸ¥ç±»åˆ«å‘ç°
        """
        print("Executing enhanced clustering algorithm...")

        # 1. ä½¿ç”¨start_newçš„å¯†åº¦è®¡ç®—å’Œæ ¸å¿ƒç‚¹è¯†åˆ«
        distances, index_matrix, point_categories, is_core, delts = compute_and_identify_points(
            X, self.k, self.density_percentile
        )
        self.densities = delts

        print(f"   æ ¸å¿ƒç‚¹æ•°: {np.sum(point_categories == 1)}/{len(X)}")

        # 2. ä½¿ç”¨start_newçš„ä¸¤é˜¶æ®µèšç±»æ„å»º
        self.clusters, cluster_assignments = self._build_clusters_with_start_new_logic(
            X, distances, index_matrix, point_categories, is_core, known_labels
        )

        # 3. ä½¿ç”¨å¢å¼ºçš„æ··åˆç½®ä¿¡åº¦åˆ†é…è¾¹ç•Œç‚¹
        cluster_assignments = self._enhanced_assign_boundary_points(
            X, cluster_assignments, known_labels, delts
        )

        # 4. åå¤„ç†ï¼šåˆå¹¶å°èšç±»å’Œè´¨é‡ä¼˜åŒ–
        cluster_assignments = self._post_process_clusters(X, cluster_assignments, known_labels)

        self.cluster_assignments = cluster_assignments
        return cluster_assignments

    def _build_clusters_with_start_new_logic(self, X, distances, index_matrix, point_categories, is_core, known_labels):
        """
        ä½¿ç”¨start_newçš„ä¸¤é˜¶æ®µèšç±»æ„å»ºé€»è¾‘ï¼Œä½†å¢åŠ æ ‡ç­¾å…¼å®¹æ€§æ£€æŸ¥
        """
        print("Building initial clusters (start_new style)...")

        # åˆå§‹åŒ–èšç±»ç›¸å…³å˜é‡
        clusters = [set() for _ in range(1000)]
        clusters_num = 0
        is_cluster = np.zeros(len(X), dtype=bool)
        x_cluster = np.full(len(X), -1, dtype=int)
        x_far = np.full(len(X), -1, dtype=int)
        total_clusters = set()

        # 1. æ„å»ºåˆå§‹ç°‡ï¼ˆä»æ ¸å¿ƒç‚¹å¼€å§‹ï¼‰
        for i in range(len(X)):
            if point_categories[i] == 1 and is_cluster[i] == False:
                clusters[clusters_num].add(i)
                x_cluster[i] = clusters_num
                total_clusters.add(clusters_num)
                clusters_num = clusters_num + 1
                is_cluster[i] = True
                x_far[i] = i

                flag = True
                new_num = x_cluster[i]
                clusters_copy = list(clusters[new_num])

                while flag:
                    for j in clusters_copy:
                        if point_categories[j] == 1:
                            for a in index_matrix[j]:
                                if a == j:
                                    continue

                                # å¢å¼ºçš„æ ‡ç­¾å…¼å®¹æ€§æ£€æŸ¥
                                if not self._enhanced_label_compatible(j, a, known_labels):
                                    continue

                                if is_cluster[a] == False:
                                    clusters[new_num].add(a)
                                    x_cluster[a] = new_num
                                    is_cluster[a] = True
                                    x_far[a] = j
                                else:
                                    if point_categories[a] == 1:
                                        if x_cluster[a] != x_cluster[j]:
                                            # èšç±»åˆå¹¶å‰æ£€æŸ¥å…¼å®¹æ€§
                                            if self._clusters_compatible(clusters[x_cluster[a]], clusters[x_cluster[j]], known_labels):
                                                total_clusters.discard(x_cluster[a])
                                                for x in clusters[x_cluster[a]]:
                                                    clusters[x_cluster[j]].add(x)
                                                    x_cluster[x] = x_cluster[j]
                                                x_far[a] = j

                    if clusters_copy == list(clusters[new_num]):
                        flag = False
                    else:
                        clusters_copy = list(clusters[new_num])

        # 2. é‡æ–°æ•´ç†èšç±»
        clusters_num = len(total_clusters)
        clusters_new = [set() for _ in range(1000)]
        a = 0
        for i in total_clusters:
            clusters_new[a] = clusters[i]
            a += 1

        # 3. é‡æ–°åˆ†é…èšç±»ID
        cluster_assignments = np.full(len(X), -1, dtype=int)
        for cluster_id, cluster in enumerate(clusters_new[:clusters_num]):
            for point in cluster:
                cluster_assignments[point] = cluster_id

        print(f"   åˆå§‹èšç±»æ•°: {clusters_num}")
        return clusters_new[:clusters_num], cluster_assignments

    def _enhanced_label_compatible(self, point1, point2, known_labels):
        """
        å¢å¼ºçš„æ ‡ç­¾å…¼å®¹æ€§æ£€æŸ¥
        """
        if known_labels is None:
            return True

        label1 = known_labels[point1]
        label2 = known_labels[point2]

        # å¦‚æœéƒ½æ˜¯å·²çŸ¥æ ‡ç­¾ï¼Œå¿…é¡»ç›¸åŒ
        if label1 != -1 and label2 != -1:
            return label1 == label2

        # å·²çŸ¥å’ŒæœªçŸ¥å¯ä»¥åœ¨åŒä¸€èšç±»ï¼ˆè¿™æ˜¯å…³é”®æ”¹è¿›ï¼‰
        # ä½†ä¼˜å…ˆä¿æŒå·²çŸ¥ç±»åˆ«çš„çº¯åº¦
        return True

    def _clusters_compatible(self, cluster1, cluster2, known_labels):
        """
        æ£€æŸ¥ä¸¤ä¸ªèšç±»æ˜¯å¦å¯ä»¥åˆå¹¶
        """
        if known_labels is None:
            return True

        # è·å–ä¸¤ä¸ªèšç±»çš„å·²çŸ¥æ ‡ç­¾åˆ†å¸ƒ
        labels1 = set()
        labels2 = set()

        for point in cluster1:
            if known_labels[point] != -1:
                labels1.add(known_labels[point])

        for point in cluster2:
            if known_labels[point] != -1:
                labels2.add(known_labels[point])

        # å¦‚æœä¸¤ä¸ªèšç±»éƒ½æœ‰å·²çŸ¥æ ‡ç­¾ï¼Œå®ƒä»¬å¿…é¡»ç›¸åŒ
        if labels1 and labels2:
            return labels1 == labels2

        # å¦‚æœå…¶ä¸­ä¸€ä¸ªæˆ–ä¸¤ä¸ªéƒ½æ˜¯çº¯æœªçŸ¥èšç±»ï¼Œå¯ä»¥åˆå¹¶
        return True

    def _enhanced_assign_boundary_points(self, X, cluster_assignments, known_labels, delts):
        """
        ä½¿ç”¨å¢å¼ºçš„æ··åˆç½®ä¿¡åº¦åˆ†é…è¾¹ç•Œç‚¹
        """
        print("Assigning boundary points (enhanced mixed confidence)...")

        # è®¡ç®—èšç±»åŸå‹
        clusters_x = []
        valid_clusters = []
        for cluster_id, cluster in enumerate(self.clusters):
            if len(cluster) > 0:
                clusters_x.append(compute_proto(cluster, X))
                valid_clusters.append(cluster_id)

        unassigned_count = 0
        new_cluster_count = 0

        for i in range(len(X)):
            if cluster_assignments[i] == -1:
                # è®¡ç®—åŸå‹ç½®ä¿¡åº¦
                confidences_p = self._compute_enhanced_prototype_confidence(X[i], clusters_x, valid_clusters)

                # è®¡ç®—kè¿‘é‚»ç½®ä¿¡åº¦
                knn_confidences = self._compute_enhanced_knn_confidence(X[i], X, cluster_assignments, delts[i])

                # æ··åˆç½®ä¿¡åº¦
                if len(confidences_p) > 0 and len(knn_confidences) > 0:
                    # ç¡®ä¿ä¸¤ä¸ªç½®ä¿¡åº¦æ•°ç»„é•¿åº¦ç›¸åŒ
                    min_len = min(len(confidences_p), len(knn_confidences))
                    confidences_p = confidences_p[:min_len]
                    knn_confidences = knn_confidences[:min_len]

                    combined_confidences = [
                        self.lambda_weight * cp + (1 - self.lambda_weight) * kc
                        for cp, kc in zip(confidences_p, knn_confidences)
                    ]

                    max_confidence = max(combined_confidences) if combined_confidences else 0.0

                    # æœªçŸ¥æ£€æµ‹æœºåˆ¶
                    if max_confidence > self.unknown_threshold:
                        # ç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œåˆ†é…åˆ°æœ€ä½³èšç±»
                        best_cluster = valid_clusters[np.argmax(combined_confidences)]
                        cluster_assignments[i] = best_cluster
                        self.clusters[best_cluster].add(i)
                        unassigned_count += 1
                    else:
                        # ç½®ä¿¡åº¦å¤ªä½ï¼Œåˆ›å»ºæ–°èšç±»ï¼ˆæ½œåœ¨æœªçŸ¥ç±»åˆ«ï¼‰
                        new_cluster_id = len(self.clusters)
                        self.clusters.append(set([i]))
                        cluster_assignments[i] = new_cluster_id
                        new_cluster_count += 1
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„èšç±»ï¼Œåˆ›å»ºæ–°èšç±»
                    new_cluster_id = len(self.clusters)
                    self.clusters.append(set([i]))
                    cluster_assignments[i] = new_cluster_id
                    new_cluster_count += 1

        print(f"   å·²åˆ†é…è¾¹ç•Œç‚¹: {unassigned_count}")
        print(f"   æ–°å»ºèšç±»æ•°: {new_cluster_count}")

        return cluster_assignments

    def _compute_enhanced_prototype_confidence(self, point, prototypes, valid_clusters):
        """
        è®¡ç®—å¢å¼ºçš„åŸå‹ç½®ä¿¡åº¦
        """
        if not prototypes:
            return []

        confidences = []
        for prototype in prototypes:
            distance = euclidean_distance(point, prototype)
            confidence = 1.0 / (distance + 1e-10)
            confidences.append(confidence)

        # å½’ä¸€åŒ–
        total = sum(confidences)
        if total > 0:
            confidences = [c / total for c in confidences]

        return confidences

    def _compute_enhanced_knn_confidence(self, point, X, cluster_assignments, density, k=2):
        """
        è®¡ç®—å¢å¼ºçš„kè¿‘é‚»ç½®ä¿¡åº¦
        """
        # è®¡ç®—åˆ°æ‰€æœ‰ç‚¹çš„è·ç¦»
        distances = [euclidean_distance(point, X[j]) for j in range(len(X))]

        # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
        knn_indices = np.argsort(distances)[:k]

        # ç»Ÿè®¡é‚»å±…çš„èšç±»åˆ†å¸ƒ
        neighbor_clusters = [cluster_assignments[idx] for idx in knn_indices
                           if cluster_assignments[idx] != -1]

        if not neighbor_clusters:
            n_clusters = len(self.clusters)
            return [1.0 / n_clusters] * n_clusters if n_clusters > 0 else []

        # è®¡ç®—æ¯ä¸ªèšç±»çš„ç½®ä¿¡åº¦
        cluster_counts = Counter(neighbor_clusters)
        n_clusters = len(self.clusters)
        confidences = []

        for cluster_id in range(n_clusters):
            count = cluster_counts.get(cluster_id, 0)
            confidence = count / len(neighbor_clusters)
            confidences.append(confidence)

        return confidences

    def _post_process_clusters(self, X, cluster_assignments, known_labels):
        """
        åå¤„ç†ï¼šåˆå¹¶å°èšç±»å’Œè´¨é‡ä¼˜åŒ–
        """
        print("Post-processing clusters...")

        # 1. ç§»é™¤è¿‡å°çš„èšç±»
        cluster_sizes = [len(cluster) for cluster in self.clusters]
        small_clusters = [i for i, size in enumerate(cluster_sizes) if size < self.min_cluster_size]

        for small_cluster_id in small_clusters:
            if len(self.clusters[small_cluster_id]) > 0:
                # å°†å°èšç±»çš„ç‚¹é‡æ–°åˆ†é…åˆ°æœ€è¿‘çš„å¤§èšç±»
                for point in list(self.clusters[small_cluster_id]):
                    best_cluster = self._find_best_cluster_for_point(point, X, known_labels)
                    if best_cluster != -1 and best_cluster != small_cluster_id:
                        cluster_assignments[point] = best_cluster
                        self.clusters[best_cluster].add(point)

                self.clusters[small_cluster_id] = set()

        # 2. ç§»é™¤ç©ºèšç±»å¹¶é‡æ–°ç¼–å·
        non_empty_clusters = [cluster for cluster in self.clusters if len(cluster) > 0]
        self.clusters = non_empty_clusters

        # é‡æ–°åˆ†é…èšç±»ID
        new_assignments = np.full(len(cluster_assignments), -1)
        for new_id, cluster in enumerate(self.clusters):
            for point in cluster:
                new_assignments[point] = new_id

        print(f"   æœ€ç»ˆèšç±»æ•°: {len(self.clusters)}")
        return new_assignments

    def _find_best_cluster_for_point(self, point, X, known_labels):
        """
        ä¸ºå•ä¸ªç‚¹æ‰¾åˆ°æœ€ä½³èšç±»
        """
        if not self.clusters:
            return -1

        best_cluster = -1
        min_distance = float('inf')

        for cluster_id, cluster in enumerate(self.clusters):
            if len(cluster) >= self.min_cluster_size:
                # è®¡ç®—åˆ°èšç±»è´¨å¿ƒçš„è·ç¦»
                cluster_points = list(cluster)
                centroid = np.mean(X[cluster_points], axis=0)
                distance = euclidean_distance(X[point], centroid)

                # è€ƒè™‘æ ‡ç­¾å…¼å®¹æ€§
                if self._point_cluster_compatible(point, cluster, known_labels):
                    if distance < min_distance:
                        min_distance = distance
                        best_cluster = cluster_id

        return best_cluster

    def _point_cluster_compatible(self, point, cluster, known_labels):
        """
        æ£€æŸ¥ç‚¹ä¸èšç±»çš„å…¼å®¹æ€§
        """
        if known_labels is None:
            return True

        point_label = known_labels[point]

        # å¦‚æœç‚¹æ˜¯æœªçŸ¥æ ‡ç­¾ï¼Œå¯ä»¥åŠ å…¥ä»»ä½•èšç±»
        if point_label == -1:
            return True

        # å¦‚æœç‚¹æ˜¯å·²çŸ¥æ ‡ç­¾ï¼Œæ£€æŸ¥èšç±»ä¸­å·²çŸ¥æ ‡ç­¾çš„ä¸€è‡´æ€§
        cluster_known_labels = set()
        for cluster_point in cluster:
            if known_labels[cluster_point] != -1:
                cluster_known_labels.add(known_labels[cluster_point])

        # å¦‚æœèšç±»æ²¡æœ‰å·²çŸ¥æ ‡ç­¾ï¼Œå¯ä»¥åŠ å…¥
        if not cluster_known_labels:
            return True

        # å¦‚æœèšç±»æœ‰å·²çŸ¥æ ‡ç­¾ï¼Œå¿…é¡»ä¸ç‚¹çš„æ ‡ç­¾ç›¸åŒ
        return point_label in cluster_known_labels

    def start_new_clustering(self, train_x, train_y, query_x, query_y):
        """
        åŸºäºæä¾›çš„start_newå‡½æ•°çš„å®Œæ•´èšç±»æµç¨‹

        Args:
            train_x: è®­ç»ƒç‰¹å¾
            train_y: è®­ç»ƒæ ‡ç­¾
            query_x: æŸ¥è¯¢ç‰¹å¾
            query_y: æŸ¥è¯¢æ ‡ç­¾

        Returns:
            acc, nmi, ari, sh: è¯„ä¼°æŒ‡æ ‡
        """
        # åˆå¹¶æ•°æ®
        X = np.concatenate((train_x, query_x), axis=0)
        Y = np.concatenate((train_y, query_y), axis=0)

        # æ ‡å‡†åŒ–ç‰¹å¾
        if self.standardize:
            scaler = StandardScaler()
            X = scaler.fit_transform(X)

        # è®¡ç®—è·ç¦»ã€é‚»å±…ã€å¯†åº¦ç­‰
        distances, index_matrix, point_categories, is_core, delts = compute_and_identify_points(
            X, self.k, self.density_percentile
        )

        knn_distances = compute_knn_distances(index_matrix, distances)

        # åˆå§‹åŒ–èšç±»ç›¸å…³å˜é‡
        clusters = [set() for _ in range(1000)]
        clusters_num = 0
        clusters_label = np.full(1000, -1, dtype=int)
        is_cluster = np.zeros(len(X), dtype=bool)
        x_cluster = np.full(len(X), -1, dtype=int)
        x_far = np.full(len(X), -1, dtype=int)
        is_c = set()
        total_clusters = set()

        # 1. æ„å»ºåˆå§‹ç°‡
        for i in range(len(X)):
            if point_categories[i] == 1 and is_cluster[i] == False:
                clusters[clusters_num].add(i)
                x_cluster[i] = clusters_num
                total_clusters.add(clusters_num)
                clusters_num = clusters_num + 1
                is_cluster[i] = True
                x_far[i] = i

                flag = True
                new_num = x_cluster[i]
                clusters_copy = list(clusters[new_num])
                while flag:
                    for j in clusters_copy:
                        if point_categories[j] == 1:
                            for a in index_matrix[j]:
                                if a == j:
                                    continue
                                if is_cluster[a] == False:
                                    clusters[new_num].add(a)
                                    x_cluster[a] = new_num
                                    is_cluster[a] = True
                                    x_far[a] = j
                                else:
                                    if point_categories[a] == 1:
                                        if x_cluster[a] != x_cluster[j]:
                                            total_clusters.discard((x_cluster[a]))
                                            for x in clusters[x_cluster[a]]:
                                                clusters[x_cluster[j]].add(x)
                                                x_cluster[x] = x_cluster[j]
                                            x_far[a] = j
                    if clusters_copy == list(clusters[new_num]):
                        flag = False
                    else:
                        clusters_copy = list(clusters[new_num])

        # é‡æ–°æ•´ç†èšç±»
        clusters_num = len(total_clusters)
        clusters_new = [set() for _ in range(1000)]
        a = 0
        for i in total_clusters:
            clusters_new[a] = clusters[i]
            a += 1
        total_clusters = set()
        clusters_label = np.full(1000, -1, dtype=int)
        for i in range(clusters_num):
            total_clusters.add(i)
        x_cluster = np.full(len(X), -1, dtype=int)
        l = 0
        pred = np.full(len(X), -1, dtype=int)
        for i in total_clusters:
            for j in clusters_new[i]:
                pred[j] = l
                x_cluster[j] = i
            clusters_label[i] = l
            l += 1

        # è®¡ç®—èšç±»åŸå‹
        clusters_x = []
        for i in range(clusters_num):
            clusters_x.append(compute_proto(clusters_new[i], X))

        # åˆ†é…æœªåˆ†é…çš„ç‚¹
        a = 0
        for i in range(len(X)):
            if pred[i] == -1:
                confidences_p = compute_confidence(X[i], clusters_x, clusters_label, total_clusters)
                knn_confidences = compute_knn_confidence(X[i], X, pred, delts[i], k=2)
                confidences = [self.lambda_weight * cp + (1 - self.lambda_weight) * kn
                             for cp, kn in zip(confidences_p, knn_confidences)]
                pred[i] = np.argmax(confidences)
                a += 1

        # ä¿å­˜èšç±»ç»“æœ
        self.clusters = clusters_new[:clusters_num]
        self.cluster_assignments = pred.copy()

        # è¯„ä¼°
        pred_sorted = np.sort(pred)
        Y_sorted = np.sort(Y)

        acc = calculate_accuracy(pred_sorted, Y_sorted)
        nmi = normalized_mutual_info_score(pred_sorted, Y_sorted)
        ari = adjusted_rand_score(pred_sorted, Y_sorted)
        sh = silhouette_score(X, clusters_new, x_cluster, clusters_num) / clusters_num if clusters_num > 0 else 0

        return acc, nmi, ari, sh

    def fit_predict(self, X, known_labels=None):
        """
        æ‰§è¡Œèšç±»å¹¶è¿”å›é¢„æµ‹æ ‡ç­¾ - é€‚é…æ–°ç‰ˆæœ¬

        Args:
            X: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
            known_labels: å·²çŸ¥æ ‡ç­¾æ•°ç»„ï¼Œ-1è¡¨ç¤ºæœªçŸ¥

        Returns:
            predictions: èšç±»æ ‡ç­¾é¢„æµ‹
        """
        print("ğŸš€ å¼€å§‹è‡ªé€‚åº”å¯†åº¦èšç±»...")

        # ä¸ºäº†ä¿æŒæ¥å£å…¼å®¹æ€§ï¼Œå°†å•ä¸ªç‰¹å¾çŸ©é˜µåˆ†å‰²ä¸ºè®­ç»ƒå’ŒæŸ¥è¯¢éƒ¨åˆ†
        # è¿™é‡Œå‡è®¾å‰ä¸€åŠæ˜¯è®­ç»ƒæ•°æ®ï¼Œåä¸€åŠæ˜¯æŸ¥è¯¢æ•°æ®
        mid_point = len(X) // 2
        train_x = X[:mid_point]
        query_x = X[mid_point:]

        # åˆ›å»ºè™šæ‹Ÿæ ‡ç­¾
        if known_labels is not None:
            train_y = known_labels[:mid_point]
            query_y = known_labels[mid_point:]
        else:
            train_y = np.arange(mid_point)
            query_y = np.arange(len(query_x)) + mid_point

        # è°ƒç”¨æ–°çš„èšç±»æ–¹æ³•
        acc, nmi, ari, sh = self.start_new_clustering(train_x, train_y, query_x, query_y)

        print(f"ğŸ‰ èšç±»å®Œæˆ! å‘ç° {len(self.clusters)} ä¸ªèšç±»")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ: ACC={acc:.4f}, NMI={nmi:.4f}, ARI={ari:.4f}, SH={sh:.4f}")

        return self.cluster_assignments

    def get_unknown_clusters(self, known_labels):
        """
        è¯†åˆ«æœªçŸ¥ç±»åˆ«èšç±»

        Args:
            known_labels: å·²çŸ¥æ ‡ç­¾æ•°ç»„

        Returns:
            unknown_cluster_ids: æœªçŸ¥èšç±»çš„IDåˆ—è¡¨
        """
        if known_labels is None:
            return list(range(len(self.clusters)))

        unknown_clusters = []

        for cluster_id, cluster in enumerate(self.clusters):
            if len(cluster) == 0:
                continue

            # æ£€æŸ¥èšç±»ä¸­æ˜¯å¦åŒ…å«å·²çŸ¥æ ‡ç­¾
            has_known_labels = False

            for point in cluster:
                if point < len(known_labels) and known_labels[point] != -1:
                    has_known_labels = True
                    break

            if not has_known_labels:
                unknown_clusters.append(cluster_id)

        return unknown_clusters


def evaluate_clustering_results(predictions, true_labels):
    """
    è¯„ä¼°èšç±»ç»“æœ

    Args:
        predictions: èšç±»é¢„æµ‹
        true_labels: çœŸå®æ ‡ç­¾

    Returns:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    from project_utils.cluster_utils import cluster_acc

    # è®¡ç®—å„ç§æŒ‡æ ‡
    acc = cluster_acc(true_labels, predictions)
    nmi = normalized_mutual_info_score(true_labels, predictions)
    ari = adjusted_rand_score(true_labels, predictions)

    n_clusters_pred = len(set(predictions))
    n_clusters_true = len(set(true_labels))

    metrics = {
        'accuracy': acc,
        'nmi': nmi,
        'ari': ari,
        'n_clusters_predicted': n_clusters_pred,
        'n_clusters_true': n_clusters_true
    }

    print(f"ğŸ“Š èšç±»è¯„ä¼°ç»“æœ:")
    print(f"   å‡†ç¡®ç‡: {acc:.4f}")
    print(f"   NMI: {nmi:.4f}")
    print(f"   ARI: {ari:.4f}")
    print(f"   é¢„æµ‹èšç±»æ•°: {n_clusters_pred}")
    print(f"   çœŸå®èšç±»æ•°: {n_clusters_true}")

    return metrics


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ§ª è‡ªé€‚åº”å¯†åº¦èšç±»æµ‹è¯•")

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    from sklearn.datasets import make_blobs
    X, y = make_blobs(n_samples=300, centers=4, n_features=10,
                      random_state=42, cluster_std=1.5)

    # æ¨¡æ‹Ÿå·²çŸ¥/æœªçŸ¥æ ‡ç­¾
    known_mask = np.random.random(len(y)) < 0.5
    known_labels = np.where(known_mask, y, -1)

    # æ‰§è¡Œèšç±»
    clusterer = AdaptiveDensityClustering(
        k_neighbors=5,
        density_percentile=75,
        lambda_weight=0.7,
        min_cluster_size=3
    )

    predictions = clusterer.fit_predict(X, known_labels)

    # è¯„ä¼°ç»“æœ
    metrics = evaluate_clustering_results(predictions, y)

    # è¯†åˆ«æœªçŸ¥èšç±»
    unknown_clusters = clusterer.get_unknown_clusters(known_labels)
    print(f"ğŸ” å‘ç° {len(unknown_clusters)} ä¸ªæ½œåœ¨æœªçŸ¥ç±»åˆ«èšç±»: {unknown_clusters}")