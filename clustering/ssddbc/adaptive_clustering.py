#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SS-DDBCè‡ªé€‚åº”å¯†åº¦èšç±»ä¸»ç®—æ³•
é›†æˆå®Œæ•´çš„SS-DDBCç®—æ³•æµç¨‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
from .clustering import build_clusters_ssddbc
from .analysis import (analyze_ssddbc_clustering_result,
                       evaluate_high_density_clustering)
from .assignment import assign_sparse_points_density_based
from .merging import merge_isolated_clusters
from ..density.density_estimation import (compute_simple_density, compute_median_density,
                                          compute_normalized_inverse_density, compute_exponential_density,
                                          identify_high_density_points, compute_relative_density)
from ..utils.co_calculation import compute_co_value, get_co_mode_description
from ..information import init_logger, reset_logger
from ..prototypes.prototype_builder import build_prototypes
from ..unknown.detection import identify_unknown_clusters_from_predictions


def apply_label_guided_assignment(X, cluster_labels, clusters, labeled_mask, targets, prototypes, prototype_labels, silent=False):
    """
    æ ‡ç­¾å¼•å¯¼åˆ†é…ï¼šå°†ç¨€ç–ç‚¹ä¸­çš„å·²çŸ¥æ ‡ç­¾æ ·æœ¬ç›´æ¥åˆ†é…åˆ°å¯¹åº”æ ‡ç­¾çš„æ ¸å¿ƒç°‡

    Args:
        cluster_labels: å½“å‰èšç±»æ ‡ç­¾ï¼ˆæ ¸å¿ƒç‚¹å·²åˆ†é…ï¼Œç¨€ç–ç‚¹ä¸º-1ï¼‰
        clusters: æ ¸å¿ƒç°‡åˆ—è¡¨
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        targets: çœŸå®æ ‡ç­¾
        prototypes: æ ¸å¿ƒç°‡åŸå‹
        prototype_labels: æ ¸å¿ƒç°‡æ ‡ç­¾
        silent: é™é»˜æ¨¡å¼

    Returns:
        updated_labels: æ›´æ–°åçš„èšç±»æ ‡ç­¾
    """
    updated_labels = cluster_labels.copy()

    # æ‰¾åˆ°æ‰€æœ‰æœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼ˆåŒ…æ‹¬ç¨€ç–ç‚¹å’Œå¯èƒ½é”™è¯¯åˆ†é…çš„æ ¸å¿ƒç‚¹ï¼‰
    all_labeled_points = np.where(labeled_mask)[0]

    # åˆ†ç¦»ç¨€ç–ç‚¹å’Œå·²åˆ†é…ç‚¹
    sparse_labeled_points = all_labeled_points[cluster_labels[all_labeled_points] == -1]
    assigned_labeled_points = all_labeled_points[cluster_labels[all_labeled_points] != -1]

    if len(sparse_labeled_points) == 0 and len(assigned_labeled_points) == 0:
        return updated_labels

    # å¤„ç†æœ‰æ ‡ç­¾çš„æ ·æœ¬ï¼ˆç¨€ç–ç‚¹ + å¯èƒ½é”™è¯¯åˆ†é…çš„æ ¸å¿ƒç‚¹ï¼‰
    assigned_count = 0
    reassigned_count = 0

    # åˆå¹¶å¤„ç†æ‰€æœ‰æœ‰æ ‡ç­¾çš„æ ·æœ¬
    all_target_points = np.concatenate([sparse_labeled_points, assigned_labeled_points]) if len(assigned_labeled_points) > 0 else sparse_labeled_points

    for point_idx in all_target_points:
        point_label = targets[point_idx]

        # æ‰¾åˆ°æ ‡ç­¾åŒ¹é…çš„æ ¸å¿ƒç°‡
        matching_clusters = []
        for cluster_id, cluster_label in enumerate(prototype_labels):
            if cluster_label == point_label:
                matching_clusters.append(cluster_id)

        if len(matching_clusters) > 0:
            # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…çš„ç°‡ï¼Œé€‰æ‹©è·ç¦»æœ€è¿‘çš„
            if len(matching_clusters) == 1:
                best_cluster = matching_clusters[0]
            else:
                # è®¡ç®—åˆ°å„ä¸ªåŒ¹é…ç°‡åŸå‹çš„è·ç¦»ï¼Œé€‰æ‹©æœ€è¿‘çš„
                distances = []
                for cluster_id in matching_clusters:
                    if cluster_id < len(prototypes):
                        dist = np.linalg.norm(prototypes[cluster_id] - X[point_idx])
                        distances.append((dist, cluster_id))
                if distances:
                    best_cluster = min(distances)[1]
                else:
                    best_cluster = matching_clusters[0]

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†é…
            current_cluster = cluster_labels[point_idx]
            if current_cluster == -1:
                # ç¨€ç–ç‚¹ï¼Œæ–°åˆ†é…
                updated_labels[point_idx] = best_cluster
                assigned_count += 1
            elif current_cluster != best_cluster:
                # å·²åˆ†é…ç‚¹ï¼Œä½†ç°‡ä¸åŒ¹é…ï¼Œé‡æ–°åˆ†é…
                updated_labels[point_idx] = best_cluster
                reassigned_count += 1


    return updated_labels


def adaptive_density_clustering(X, targets, known_mask, labeled_mask,
                               k=10, density_percentile=75, random_state=0, train_size=None,
                               co_mode=2, co_manual=None,
                               eval_dense=False, eval_version='v1',
                               silent=False, dense_method=0, assign_model=2, voting_k=5, detail_dense=False,
                               label_guide=False):
    """
    SS-DDBCé£æ ¼çš„è‡ªé€‚åº”å¯†åº¦èšç±»ç®—æ³•ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰

    Args:
        X: ç‰¹å¾çŸ©é˜µ
        targets: çœŸå®æ ‡ç­¾ (ä»…ç”¨äºè¯„ä¼°)
        known_mask: å·²çŸ¥ç±»åˆ«æ©ç 
        labeled_mask: æœ‰æ ‡ç­¾æ©ç 
        k: kè¿‘é‚»å‚æ•°
        density_percentile: å¯†åº¦ç™¾åˆ†ä½æ•°é˜ˆå€¼
        random_state: éšæœºç§å­ (ä¸K-meansä¿æŒä¸€è‡´)
        train_size: è®­ç»ƒé›†å¤§å°ï¼ˆç”¨äºåŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰
        co_mode: coè®¡ç®—æ¨¡å¼ (1=æ‰‹åŠ¨æŒ‡å®š, 2=Kè¿‘é‚»å¹³å‡è·ç¦», 3=ç›¸å¯¹è‡ªé€‚åº”è·ç¦»ï¼Œé»˜è®¤2)
        co_manual: æ‰‹åŠ¨æŒ‡å®šçš„coå€¼ï¼ˆä»…å½“co_mode=1æ—¶ä½¿ç”¨ï¼‰
        eval_dense: æ˜¯å¦ä»…è¯„ä¼°é«˜å¯†åº¦ç‚¹ï¼ˆé»˜è®¤Falseï¼Œå¦‚ä¸ºTrueåˆ™è·³è¿‡ä½å¯†åº¦ç‚¹åˆ†é…ï¼‰
        eval_version: è¯„ä¼°ç‰ˆæœ¬ ('v1' æˆ– 'v2')
        silent: é™é»˜æ¨¡å¼ï¼Œå…³é—­æ‰€æœ‰ä¸å¿…è¦çš„æ‰“å°è¾“å‡ºï¼ˆç”¨äºç½‘æ ¼æœç´¢åŠ é€Ÿï¼Œé»˜è®¤Falseï¼‰
        dense_method: å¯†åº¦è®¡ç®—æ–¹æ³• (0=å¹³å‡è·ç¦», 1=ä¸­ä½æ•°è·ç¦», 2=å½’ä¸€åŒ–å€’æ•°, 3=æŒ‡æ•°å¯†åº¦ï¼Œé»˜è®¤0)
        assign_model: ç¨€ç–ç‚¹åˆ†é…ç­–ç•¥ (1=ç°‡åŸå‹å°±è¿‘, 2=KNNæŠ•ç¥¨åŠ æƒ, 3=ç°‡å†…Kè¿‘é‚»å¹³å‡è·ç¦»ï¼Œé»˜è®¤2)
        voting_k: KNNæŠ•ç¥¨æ—¶ä½¿ç”¨çš„è¿‘é‚»æ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        detail_dense: æ˜¯å¦è®°å½•éª¨å¹²ç½‘ç»œèšç±»è¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤Falseï¼‰
        label_guide: æ˜¯å¦å¯ç”¨æ ‡ç­¾å¼•å¯¼æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼ŒTrueæ—¶å…ˆå°†å·²çŸ¥æ ‡ç­¾ç¨€ç–ç‚¹åˆ†é…åˆ°å¯¹åº”æ ¸å¿ƒç°‡ï¼‰

    Returns:
        predictions: èšç±»é¢„æµ‹ç»“æœ
        n_clusters: èšç±»æ•°é‡
        unknown_clusters: æ½œåœ¨æœªçŸ¥ç±»èšç±»ç´¢å¼•
        prototypes: ç°‡åŸå‹ï¼ˆç”¨äºé”™è¯¯åˆ†æï¼‰
        updated_clusters: æ›´æ–°åçš„ç°‡æˆå‘˜é›†åˆï¼ˆåŒ…å«æ‰€æœ‰ç‚¹ï¼Œç”¨äºé”™è¯¯åˆ†æï¼‰
        cluster_category_labels: ç°‡IDåˆ°ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„å­—å…¸ï¼ˆåªåŒ…å«æœ‰ç±»åˆ«æ ‡ç­¾çš„ç°‡ï¼‰
        neighbors: Kè¿‘é‚»ç´¢å¼•çŸ©é˜µ (n_samples, k)ï¼ˆç”¨äºé¿å…é‡å¤è®¡ç®—ï¼‰
        core_clusters: æ ¸å¿ƒç‚¹ç°‡ï¼ˆåªåŒ…å«é«˜å¯†åº¦ç‚¹ï¼Œç”¨äºè®¡ç®—èšç±»è´¨é‡ç¬¬ä¸€é¡¹ï¼‰
        å¦‚æœeval_dense=Trueï¼Œè¿˜ä¼šè¿”å›è¯„ä¼°æŒ‡æ ‡: (all_acc, old_acc, new_acc, None)
    """
    # è®¾ç½®å±€éƒ¨numpyéšæœºç§å­ (éµå¾ªK-meansçš„è®¾è®¡æ¨¡å¼)
    np.random.seed(random_state)

    # åˆå§‹åŒ–è¯¦ç»†æ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    logger = None
    if detail_dense:
        logger = init_logger(enabled=True)
        if not silent:
            print("ğŸ“ éª¨å¹²ç½‘ç»œè¯¦ç»†æ—¥å¿—è®°å½•å·²å¯ç”¨")

    if not silent:
        print("ğŸš€ å¼€å§‹ç®€åŒ–SS-DDBCèšç±»...")

    # æ­¥éª¤1: æ ¹æ®dense_methodé€‰æ‹©å¯†åº¦è®¡ç®—æ–¹æ³•
    if dense_method == 0:
        # æ–¹æ³•0: ä½¿ç”¨å¹³å‡è·ç¦»è®¡ç®—å¯†åº¦
        densities, knn_distances, neighbors = compute_simple_density(X, k)
        if not silent:
            print(f"   å¯†åº¦è®¡ç®—æ–¹æ³•: å¹³å‡è·ç¦»å€’æ•° (dense_method=0)")
    elif dense_method == 1:
        # æ–¹æ³•1: ä½¿ç”¨ä¸­ä½æ•°è·ç¦»è®¡ç®—å¯†åº¦
        densities, knn_distances, neighbors = compute_median_density(X, k)
        if not silent:
            print(f"   å¯†åº¦è®¡ç®—æ–¹æ³•: ä¸­ä½æ•°è·ç¦»å€’æ•° (dense_method=1)")
    elif dense_method == 2:
        # æ–¹æ³•2: ä½¿ç”¨å½’ä¸€åŒ–å€’æ•°å¯†åº¦
        densities, knn_distances, neighbors = compute_normalized_inverse_density(X, k)
        if not silent:
            print(f"   å¯†åº¦è®¡ç®—æ–¹æ³•: å½’ä¸€åŒ–å€’æ•°å¯†åº¦ (dense_method=2)")
    elif dense_method == 3:
        # æ–¹æ³•3: ä½¿ç”¨æŒ‡æ•°å¯†åº¦
        densities, knn_distances, neighbors = compute_exponential_density(X, k)
        if not silent:
            print(f"   å¯†åº¦è®¡ç®—æ–¹æ³•: æŒ‡æ•°å¯†åº¦ (dense_method=3)")
    else:
        raise ValueError(f"æœªçŸ¥çš„dense_methodå€¼: {dense_method}. æ”¯æŒçš„å€¼: 0 (å¹³å‡è·ç¦»), 1 (ä¸­ä½æ•°è·ç¦»), 2 (å½’ä¸€åŒ–å€’æ•°), 3 (æŒ‡æ•°å¯†åº¦)")

    # æ­¥éª¤2: è®¡ç®—ç›¸å¯¹å¯†åº¦å¹¶è¯†åˆ«é«˜å¯†åº¦ç‚¹ï¼ˆä½¿ç”¨ç™¾åˆ†ä½æ•°é˜ˆå€¼ï¼‰
    relative_densities = compute_relative_density(densities, neighbors, k)
    high_density_mask = identify_high_density_points(relative_densities, density_percentile, use_relative=True)

    # æ­¥éª¤2.5: ä½¿ç”¨æ–°çš„coè®¡ç®—é€»è¾‘
    if not silent:
        print(f"\n[CO CALCULATION] coè®¡ç®—æ¨¡å¼: {co_mode} - {get_co_mode_description(co_mode)}")

    co = compute_co_value(
        co_mode=co_mode,
        knn_distances=knn_distances,
        densities=densities,
        neighbors=neighbors,
        k=k,
        co_manual=co_manual,
        silent=silent
    )

    # è®¾ç½®æ—¥å¿—å…ƒæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if logger is not None:
        n_high_density = np.sum(high_density_mask)
        # æ³¨æ„ï¼šè¿™é‡Œçš„train_sizeæ˜¯åŸå§‹åˆ’åˆ†çš„è®­ç»ƒé›†å¤§å°ï¼ˆç”¨æˆ·ä¼ å…¥ï¼‰
        # å¦‚æœæœªä¼ å…¥ï¼Œåˆ™é»˜è®¤è®¤ä¸ºæ‰€æœ‰æ ·æœ¬éƒ½æ˜¯è®­ç»ƒé›†
        actual_train_size = train_size if train_size is not None else len(X)
        actual_test_size = len(X) - actual_train_size if train_size is not None else 0

        logger.set_metadata(
            dataset_name='clustering_dataset',  # é€šç”¨åç§°ï¼Œæ— æ³•ä»è¿™é‡Œè·å–è¶…ç±»å
            total_points=len(X),
            n_high_density=n_high_density,
            train_size=actual_train_size,
            test_size=actual_test_size,
            co_mode=co_mode,
            co_value=co
        )

    # æ­¥éª¤3: èšç±»æ„å»º (é›†æˆå†²çªå¤„ç†)
    clusters, cluster_labels, high_density_neighbors_map, cluster_category_labels = build_clusters_ssddbc(
        X, high_density_mask, neighbors, labeled_mask, targets, densities, known_mask, k, co=co, silent=silent,
        logger=logger, train_size=train_size
    )

    # æ ¸å¿ƒå­ç©ºé—´å­¤ç«‹ç°‡åˆå¹¶ï¼ˆç°‡è§„æ¨¡<=3ï¼‰
    merged_core_labels, core_merge_info = merge_isolated_clusters(
        cluster_labels,
        X,
        targets,
        labeled_mask,
        cluster_category_labels=None,
        isolated_threshold=3,
        silent=silent
    )
    cluster_labels = merged_core_labels

    # æŒ‰ç…§æœ€æ–°æ ‡ç­¾é‡å»ºé«˜å¯†åº¦ç°‡é›†åˆ
    updated_clusters = []
    unique_core_ids = sorted(np.unique(cluster_labels[cluster_labels >= 0]))
    for cluster_id in unique_core_ids:
        members = np.flatnonzero(cluster_labels == cluster_id)
        updated_clusters.append(set(members.tolist()))
    clusters = updated_clusters

    if not silent and core_merge_info.get('n_merges', 0) > 0:
        print(f"   æ ¸å¿ƒå­ç©ºé—´å­¤ç«‹ç°‡åˆå¹¶: {core_merge_info['n_merges']}æ¬¡")

    # åˆ†æSS-DDBCèšç±»æ„å»ºç»“æœï¼ˆé™é»˜æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
    if not silent:
        analyze_ssddbc_clustering_result(clusters, cluster_labels, labeled_mask, targets, known_mask)

    # å¦‚æœä»…è¯„ä¼°é«˜å¯†åº¦ç‚¹ï¼Œåˆ™åœ¨æ­¤æå‰è¿”å›
    if eval_dense:
        if not silent:
            print(f"\nğŸ¯ ä»…è¯„ä¼°é«˜å¯†åº¦ç‚¹æ¨¡å¼")
            print(f"   è·³è¿‡æ­¥éª¤4-7ï¼ˆåŸå‹æ„å»ºã€ä½å¯†åº¦ç‚¹åˆ†é…ã€å­¤ç«‹ç°‡åˆå¹¶ã€æœªçŸ¥ç±»è¯†åˆ«ï¼‰")

        # è¯„ä¼°é«˜å¯†åº¦ç‚¹èšç±»å‡†ç¡®ç‡
        all_acc, old_acc, new_acc, n_clusters, _ = evaluate_high_density_clustering(
            cluster_labels, targets, known_mask, eval_version, X=X, silent=silent
        )

        if not silent:
            print(f"\nâœ… é«˜å¯†åº¦ç‚¹èšç±»è¯„ä¼°å®Œæˆ: {n_clusters}ä¸ªèšç±»")
            print(f"ğŸ“Š è¯„ä¼°ç»“æœ (ä»…é«˜å¯†åº¦ç‚¹):")
            print(f"   All ACC: {all_acc:.4f}")
            print(f"   Old ACC: {old_acc:.4f}")
            print(f"   New ACC: {new_acc:.4f}")

        # è¿”å›é«˜å¯†åº¦ç‚¹èšç±»ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡
        # eval_denseæ¨¡å¼ä¸è¿”å›core_clustersï¼ˆå› ä¸ºæ²¡æœ‰å®Œæ•´çš„èšç±»æµç¨‹ï¼‰
        return cluster_labels, n_clusters, [], all_acc, old_acc, new_acc, None, neighbors, None

    # ä¿å­˜æ ¸å¿ƒç‚¹ç°‡ï¼ˆåªåŒ…å«é«˜å¯†åº¦ç‚¹ï¼‰ï¼Œç”¨äºè®¡ç®—èšç±»è´¨é‡ç¬¬ä¸€é¡¹
    # è¿™ä¸ªcore_clustersåœ¨æ•´ä¸ªæµç¨‹ä¸­ä¿æŒä¸å˜ï¼ŒåªåŒ…å«æ ¸å¿ƒç‚¹
    core_clusters = [set(cluster) for cluster in clusters]  # æ·±æ‹·è´ä¿å­˜æ ¸å¿ƒç°‡

    # æ­¥éª¤4: åŸºäºpartial-clusteringç»“æœå»ºç«‹åŸå‹
    prototypes, prototype_labels = build_prototypes(X, clusters, labeled_mask, targets)

    # ä¸Šå¸è§†è§’ï¼šåŒæ—¶è®¡ç®—åŸå‹çš„çœŸå®ä¸»å¯¼æ ‡ç­¾ï¼ˆç”¨äºè°ƒè¯•åˆ†æï¼‰
    prototype_true_labels = []
    for cluster_id, cluster in enumerate(clusters):
        cluster_indices = list(cluster)
        cluster_true_labels = targets[cluster_indices]  # ä¸Šå¸è§†è§’
        if len(cluster_true_labels) > 0:
            unique_labels, counts = np.unique(cluster_true_labels, return_counts=True)
            true_dominant_label = unique_labels[np.argmax(counts)]
            prototype_true_labels.append(true_dominant_label)
        else:
            prototype_true_labels.append(-1)

    # æ­¥éª¤5: æ ‡ç­¾å¼•å¯¼åˆ†é…ï¼ˆå¯é€‰ï¼‰+ ç¨€ç–ç‚¹åˆ†é…

    # åˆå§‹åŒ–æœ€ç»ˆæ ‡ç­¾ä¸ºæ ¸å¿ƒç‚¹èšç±»ç»“æœ
    final_labels = cluster_labels.copy()

    # é‡å»ºclustersåˆ—è¡¨ç”¨äºç¨€ç–ç‚¹åˆ†é…
    # è¿™ä¸ªclustersä¼šéšç€label_guideçš„åˆ†é…è€Œæ›´æ–°ï¼Œç¡®ä¿ç¨€ç–ç‚¹åˆ†é…æ—¶èƒ½çœ‹åˆ°å®Œæ•´çš„æ ¸å¿ƒç‚¹ç»“æ„
    updated_clusters = [set(cluster) for cluster in clusters]  # æ·±æ‹·è´

    if label_guide:
        # æ ‡ç­¾å¼•å¯¼ï¼šå…ˆå°†ç¨€ç–ç‚¹ä¸­çš„å·²çŸ¥æ ‡ç­¾æ ·æœ¬ç›´æ¥åˆ†é…åˆ°å¯¹åº”æ ‡ç­¾çš„æ ¸å¿ƒç°‡
        final_labels = apply_label_guided_assignment(
            X, final_labels, updated_clusters, labeled_mask, targets, prototypes, prototype_labels, silent
        )

        # å…³é”®ï¼šæ›´æ–°clustersï¼Œå°†label_guideåˆ†é…çš„æœ‰æ ‡ç­¾ç¨€ç–ç‚¹åŠ å…¥åˆ°å¯¹åº”ç°‡ä¸­
        # è¿™æ ·åç»­çš„ç¨€ç–ç‚¹åˆ†é…å°±èƒ½çœ‹åˆ°è¿™äº›ç‚¹äº†
        if not silent:
            print(f"\n[CORE SPACE UPDATE] æ›´æ–°æ ¸å¿ƒç‚¹ç©ºé—´ï¼ˆåŠ å…¥æœ‰æ ‡ç­¾ç¨€ç–ç‚¹ï¼‰")

        # é‡æ–°æ„å»ºclustersï¼ŒåŒ…å«æ‰€æœ‰å·²åˆ†é…çš„ç‚¹
        updated_clusters = [set() for _ in range(len(updated_clusters))]
        for idx in range(len(final_labels)):
            cluster_id = final_labels[idx]
            if cluster_id != -1:
                updated_clusters[cluster_id].add(idx)

        if not silent:
            # ç»Ÿè®¡æœ‰æ ‡ç­¾ç¨€ç–ç‚¹çš„æ•°é‡
            labeled_sparse_added = 0
            for idx in range(len(final_labels)):
                # å¦‚æœè¿™ä¸ªç‚¹åœ¨åŸå§‹cluster_labelsä¸­æ˜¯-1ï¼ˆç¨€ç–ç‚¹ï¼‰ï¼Œä½†ç°åœ¨æœ‰ç°‡æ ‡ç­¾äº†
                if cluster_labels[idx] == -1 and final_labels[idx] != -1 and labeled_mask[idx]:
                    labeled_sparse_added += 1
            print(f"   æ ¸å¿ƒç‚¹ç©ºé—´æ‰©å±•: +{labeled_sparse_added}ä¸ªæœ‰æ ‡ç­¾ç¨€ç–ç‚¹")
            for cid, cluster in enumerate(updated_clusters):
                if len(cluster) > 0:
                    original_size = len(clusters[cid])
                    new_size = len(cluster)
                    added = new_size - original_size
                    if added > 0:
                        print(f"   ç°‡{cid}: {original_size} â†’ {new_size} (+{added})")

    # ç„¶ååˆ†é…å‰©ä½™çš„ç¨€ç–ç‚¹ï¼ˆæ— æ ‡ç­¾æˆ–æ ‡ç­¾å¼•å¯¼æ¨¡å¼æœªå¯ç”¨çš„æ‰€æœ‰ç¨€ç–ç‚¹ï¼‰
    # å…³é”®ï¼šä¼ é€’updated_clustersï¼Œè€Œä¸æ˜¯åŸå§‹çš„clusters
    final_labels = assign_sparse_points_density_based(
        X, updated_clusters, final_labels, densities, neighbors, labeled_mask, targets,
        label_threshold=0.1, purity_threshold=0.8, train_size=train_size, silent=silent,
        prototypes=prototypes, prototype_true_labels=prototype_true_labels,
        voting_k=voting_k, assign_model=assign_model,
        label_guide=label_guide  # ä¼ é€’æ ‡ç­¾å¼•å¯¼æ¨¡å¼æ ‡å¿—
    )

    # æ­¥éª¤6: è¯†åˆ«æ½œåœ¨æœªçŸ¥ç±»èšç±»
    unknown_clusters = identify_unknown_clusters_from_predictions(final_labels, labeled_mask)

    n_clusters = len(np.unique(final_labels))
    algorithm_name = "ç®€åŒ–SS-DDBC"
    if not silent:
        print(f"âœ… {algorithm_name}èšç±»å®Œæˆ: {n_clusters}ä¸ªèšç±»")

        if len(unknown_clusters) > 0:
            print(f"ğŸ” å‘ç°{len(unknown_clusters)}ä¸ªæ½œåœ¨æœªçŸ¥ç±»èšç±»: {unknown_clusters}")
        else:
            print("ğŸ” æœªå‘ç°æ½œåœ¨æœªçŸ¥ç±»èšç±»")

    # å†™å…¥è¯¦ç»†æ—¥å¿—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if logger is not None:
        logger.write_log()
        reset_logger()  # é‡ç½®å…¨å±€æ—¥å¿—å®ä¾‹ï¼Œä¸ºä¸‹æ¬¡è¿è¡Œåšå‡†å¤‡

    # é‡æ–°æ„å»ºç°‡ç±»åˆ«æ ‡ç­¾ï¼ˆåŸºäºæœ€ç»ˆçš„ç°‡æˆå‘˜ï¼‰
    # åº”ç”¨ä¸build_clusters_ssddbcç›¸åŒçš„è§„åˆ™ï¼š
    # 1. ç°‡è§„æ¨¡ â‰¥ 5
    # 2. å·²çŸ¥æ ‡ç­¾æ ·æœ¬å æ¯” > 25% ä¸”æ•°é‡ â‰  0
    # 3. å·²çŸ¥æ ·æœ¬ä¸­ï¼Œå•ç§æ ‡ç­¾çº¯åº¦ â‰¥ 80%
    final_cluster_category_labels = {}
    for cluster_id, cluster_members in enumerate(updated_clusters):
        cluster_indices = list(cluster_members)
        if len(cluster_indices) < 5:
            continue  # ç°‡å¤ªå°ï¼Œæ— æ ‡ç­¾

        # ç»Ÿè®¡ç°‡ä¸­çš„å·²çŸ¥æ ‡ç­¾æ ·æœ¬
        labeled_in_cluster = [idx for idx in cluster_indices if labeled_mask[idx]]
        if len(labeled_in_cluster) == 0:
            continue  # æ²¡æœ‰å·²çŸ¥æ ‡ç­¾æ ·æœ¬

        # æ£€æŸ¥å·²çŸ¥æ ‡ç­¾æ ·æœ¬å æ¯”
        labeled_ratio = len(labeled_in_cluster) / len(cluster_indices)
        if labeled_ratio <= 0.25:
            continue  # å·²çŸ¥æ ‡ç­¾æ ·æœ¬å æ¯” â‰¤ 25%

        # ç»Ÿè®¡å·²çŸ¥æ ·æœ¬ä¸­çš„æ ‡ç­¾åˆ†å¸ƒ
        label_counts = {}
        for idx in labeled_in_cluster:
            label = targets[idx]
            label_counts[label] = label_counts.get(label, 0) + 1

        # æ‰¾å‡ºä¸»å¯¼æ ‡ç­¾
        dominant_label = max(label_counts, key=label_counts.get)
        dominant_count = label_counts[dominant_label]
        purity = dominant_count / len(labeled_in_cluster)

        if purity >= 0.8:
            final_cluster_category_labels[cluster_id] = dominant_label  # çº¯åº¦ â‰¥ 80%ï¼Œç°‡æœ‰ç±»åˆ«æ ‡ç­¾

    # è¿”å›å®Œæ•´çš„èšç±»ä¿¡æ¯ï¼ˆåŒ…æ‹¬åŸå‹ã€æ›´æ–°åçš„ç°‡ã€ç°‡ç±»åˆ«æ ‡ç­¾ã€neighborså’Œæ ¸å¿ƒç°‡ï¼‰
    # updated_clustersåŒ…å«æ‰€æœ‰ç‚¹ï¼ˆæ ¸å¿ƒç‚¹+ç¨€ç–ç‚¹ï¼‰ï¼Œç”¨äºé”™è¯¯åˆ†æ
    # core_clustersåªåŒ…å«æ ¸å¿ƒç‚¹ï¼ˆé«˜å¯†åº¦ç‚¹ï¼‰ï¼Œç”¨äºè®¡ç®—èšç±»è´¨é‡ç¬¬ä¸€é¡¹
    # neighborsç”¨äºé¿å…åœ¨compute_local_density_penaltyä¸­é‡å¤è®¡ç®—KNN
    return final_labels, n_clusters, unknown_clusters, prototypes, updated_clusters, final_cluster_category_labels, neighbors, core_clusters
