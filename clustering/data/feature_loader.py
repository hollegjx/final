#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç‰¹å¾ç¼“å­˜åŠ è½½æ¨¡å—
è´Ÿè´£ä»ç¼“å­˜æ–‡ä»¶ä¸­åŠ è½½é¢„æå–çš„ç‰¹å¾
"""

import os
import pickle
import numpy as np

from config import feature_cache_dir, feature_cache_dir_nol2


class FeatureLoader:
    """
    ç‰¹å¾åŠ è½½å™¨ç±»
    è´Ÿè´£ä»ç£ç›˜ç¼“å­˜åŠ è½½é¢„å…ˆæå–å¥½çš„ç‰¹å¾æ•°æ®
    """

    def __init__(self, cache_base_dir=None, cache_base_dir_nol2=None):
        """
        åˆå§‹åŒ–ç‰¹å¾åŠ è½½å™¨

        Args:
            cache_base_dir: ç¼“å­˜æ–‡ä»¶çš„åŸºç¡€ç›®å½•
        """
        self.cache_base_dir = cache_base_dir or feature_cache_dir
        if cache_base_dir_nol2 is not None:
            self.cache_base_dir_nol2 = cache_base_dir_nol2
        elif cache_base_dir:
            self.cache_base_dir_nol2 = cache_base_dir.replace('features', 'features_nol2')
        else:
            self.cache_base_dir_nol2 = feature_cache_dir_nol2

    def load(self, dataset_name, use_l2=True, silent=False):
        """
        åŠ è½½æŒ‡å®šæ•°æ®é›†çš„ç‰¹å¾ç¼“å­˜

        Args:
            dataset_name: æ•°æ®é›†åç§°ï¼ˆå¦‚è¶…ç±»åç§°ï¼‰
            use_l2: æ˜¯å¦ä½¿ç”¨L2å½’ä¸€åŒ–çš„ç‰¹å¾
            silent: æ˜¯å¦é™é»˜æ¨¡å¼

        Returns:
            feature_dict: ç‰¹å¾æ•°æ®å­—å…¸ï¼ŒåŒ…å«:
                - 'all_features': æ‰€æœ‰æ ·æœ¬ç‰¹å¾ (n_samples, feat_dim)
                - 'all_targets': æ‰€æœ‰æ ·æœ¬æ ‡ç­¾ (n_samples,)
                - 'all_known_mask': å·²çŸ¥ç±»æ©ç  (n_samples,)
                - 'all_labeled_mask': æœ‰æ ‡ç­¾æ ·æœ¬æ©ç  (n_samples,)
                - 'train_features': è®­ç»ƒé›†ç‰¹å¾
                - 'train_targets': è®­ç»ƒé›†æ ‡ç­¾
                - 'train_known_mask': è®­ç»ƒé›†å·²çŸ¥ç±»æ©ç 
                - 'train_labeled_mask': è®­ç»ƒé›†æœ‰æ ‡ç­¾æ©ç 
                - 'test_features': æµ‹è¯•é›†ç‰¹å¾
                - 'test_targets': æµ‹è¯•é›†æ ‡ç­¾
                - 'test_known_mask': æµ‹è¯•é›†å·²çŸ¥ç±»æ©ç 
                - 'test_labeled_mask': æµ‹è¯•é›†æœ‰æ ‡ç­¾æ©ç 
            å¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        # æ ¹æ®use_l2é€‰æ‹©ç¼“å­˜ç›®å½•
        cache_dir = self.cache_base_dir if use_l2 else self.cache_base_dir_nol2
        l2_status = "L2å½’ä¸€åŒ–" if use_l2 else "æ— L2å½’ä¸€åŒ–"

        # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_file = os.path.join(cache_dir, dataset_name, 'features.pkl')

        # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(cache_file):
            if not silent:
                print(f"ğŸ” æœªå‘ç°ç¼“å­˜ç‰¹å¾æ–‡ä»¶ ({l2_status}): {cache_file}")
            return None

        # å°è¯•åŠ è½½ç¼“å­˜
        try:
            if not silent:
                print(f"ğŸ” å‘ç°ç¼“å­˜ç‰¹å¾æ–‡ä»¶ ({l2_status}): {cache_file}")
                print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ç¼“å­˜ç‰¹å¾...")

            with open(cache_file, 'rb') as f:
                feature_dict = pickle.load(f)

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            self._validate_feature_dict(feature_dict)

            if not silent:
                print(f"âœ… ç¼“å­˜ç‰¹å¾åŠ è½½æˆåŠŸ ({l2_status})")
                print(f"   ç‰¹å¾å½¢çŠ¶: {feature_dict['all_features'].shape}")
                print(f"   æ ·æœ¬æ€»æ•°: {len(feature_dict['all_features'])}")
                print(f"   è®­ç»ƒé›†: {len(feature_dict['train_features'])} æ ·æœ¬")
                print(f"   æµ‹è¯•é›†: {len(feature_dict['test_features'])} æ ·æœ¬")

            return feature_dict

        except Exception as e:
            if not silent:
                print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶æŸåæˆ–æ ¼å¼ä¸å…¼å®¹: {e}")
            return None

    def _validate_feature_dict(self, feature_dict):
        """
        éªŒè¯ç‰¹å¾å­—å…¸çš„å®Œæ•´æ€§

        Args:
            feature_dict: ç‰¹å¾æ•°æ®å­—å…¸

        Raises:
            ValueError: å¦‚æœæ•°æ®æ ¼å¼ä¸æ­£ç¡®
        """
        required_keys = [
            'all_features', 'all_targets', 'all_known_mask', 'all_labeled_mask',
            'train_features', 'train_targets', 'train_known_mask', 'train_labeled_mask',
            'test_features', 'test_targets', 'test_known_mask', 'test_labeled_mask'
        ]

        for key in required_keys:
            if key not in feature_dict:
                raise ValueError(f"ç¼“å­˜æ•°æ®ç¼ºå°‘å¿…éœ€å­—æ®µ: {key}")

        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        n_all = len(feature_dict['all_features'])
        n_train = len(feature_dict['train_features'])
        n_test = len(feature_dict['test_features'])

        if n_all != n_train + n_test:
            raise ValueError(f"æ•°æ®ä¸ä¸€è‡´: all({n_all}) != train({n_train}) + test({n_test})")

    def check_cache_exists(self, dataset_name, use_l2=True):
        """
        æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä¸åŠ è½½ï¼‰

        Args:
            dataset_name: æ•°æ®é›†åç§°
            use_l2: æ˜¯å¦L2å½’ä¸€åŒ–

        Returns:
            bool: ç¼“å­˜æ˜¯å¦å­˜åœ¨
        """
        cache_dir = self.cache_base_dir if use_l2 else self.cache_base_dir_nol2
        cache_file = os.path.join(cache_dir, dataset_name, 'features.pkl')
        return os.path.exists(cache_file)

    def get_cache_path(self, dataset_name, use_l2=True):
        """
        è·å–ç¼“å­˜æ–‡ä»¶çš„å®Œæ•´è·¯å¾„

        Args:
            dataset_name: æ•°æ®é›†åç§°
            use_l2: æ˜¯å¦L2å½’ä¸€åŒ–

        Returns:
            str: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        cache_dir = self.cache_base_dir if use_l2 else self.cache_base_dir_nol2
        return os.path.join(cache_dir, dataset_name, 'features.pkl')
