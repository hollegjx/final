# å¢å¼ºå‹æ•°æ®æä¾›å™¨ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

`EnhancedDataProvider` å’Œ `EnhancedDataset` æ˜¯åœ¨åŸæœ‰æ•°æ®è¯»å–ç³»ç»ŸåŸºç¡€ä¸Šçš„å¢å¼ºç‰ˆæœ¬ï¼Œæä¾›ï¼š

1. **æ›´å…¨é¢çš„ä¿¡æ¯**ï¼šåŒ…å«æ‰€æœ‰èšç±»æ‰€éœ€çš„å…ƒæ•°æ®
2. **æ›´é«˜çš„æ•ˆç‡**ï¼šé¢„è®¡ç®—å¸¸ç”¨ä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
3. **æ›´ç®€æ´çš„API**ï¼šä¸€æ¬¡è°ƒç”¨è·å–æ‰€æœ‰æ•°æ®ï¼Œå‡å°‘ä»£ç å†—ä½™

---

## å¯¹æ¯”ï¼šæ—§ vs æ–°

### âŒ æ—§æ–¹å¼ï¼ˆå­˜åœ¨é—®é¢˜ï¼‰

```python
# test_superclass.py ä¸­çš„æ—§ä»£ç 

# é—®é¢˜1: é‡å¤çš„æ•°æ®æå–é€»è¾‘
data_provider = DataProvider(cache_base_dir='/data/gjx/checkpoints/features')
cache_info = data_provider.get_cache_info(superclass_name, use_l2=use_l2)

if cache_info['exists']:
    feature_dict, source = data_provider.get_features(...)
    # æå–æ•°æ®ï¼ˆ12è¡Œé‡å¤ä»£ç ï¼‰
    all_feats = feature_dict['all_features']
    all_targets = feature_dict['all_targets']
    # ... 10 more lines
else:
    # å†æ¬¡æå–æ•°æ®ï¼ˆ12è¡Œé‡å¤ä»£ç ï¼‰
    all_feats = feature_dict['all_features']
    # ... 10 more lines

# é—®é¢˜2: å¤šæ¬¡è®¡ç®—ç›¸åŒçš„ä¿¡æ¯
train_size = len(train_feats) if use_train_and_test else None  # Line 176
# ...
test_start_idx = len(train_feats)  # Line 127
# ...
test_start_idx = len(train_feats)  # Line 174 (å†æ¬¡è®¡ç®—)

# é—®é¢˜3: ç¼ºå¤±ä¿¡æ¯
# - æ²¡æœ‰æ ·æœ¬ç´¢å¼•ï¼ˆindicesï¼‰
# - æ²¡æœ‰ç‰¹å¾ç»´åº¦ä¿¡æ¯
# - æ²¡æœ‰æ•°æ®æ¥æºæ ‡è®°
```

### âœ… æ–°æ–¹å¼ï¼ˆä¼˜åŒ–åï¼‰

```python
from clustering.data import EnhancedDataProvider

# ä¸€æ¬¡è°ƒç”¨è·å–æ‰€æœ‰æ•°æ®
provider = EnhancedDataProvider(cache_base_dir='/data/gjx/checkpoints/features')
dataset = provider.load_dataset(
    dataset_name='trees',
    model_path='/path/to/model.pt',  # ä»…ç¼“å­˜ä¸å­˜åœ¨æ—¶éœ€è¦
    use_l2=True,
    silent=False
)

# æ‰€æœ‰ä¿¡æ¯éƒ½å·²é¢„è®¡ç®—ï¼Œç›´æ¥ä½¿ç”¨
print(f"æ•°æ®æ¥æº: {dataset.source}")
print(f"è®­ç»ƒé›†å¤§å°: {dataset.train_size}")
print(f"æµ‹è¯•é›†èµ·å§‹ç´¢å¼•: {dataset.test_start_idx}")
print(f"ç‰¹å¾ç»´åº¦: {dataset.feat_dim}")

# è·å–èšç±»è¾“å…¥ï¼ˆä¸€è¡Œä»£ç ï¼‰
X, targets, known_mask, labeled_mask, train_size = dataset.get_clustering_input()

# è·å–æµ‹è¯•é›†å­é›†ï¼ˆä¸€è¡Œä»£ç ï¼‰
test_data = dataset.get_test_subset(predictions)
```

---

## EnhancedDataset å®Œæ•´åŠŸèƒ½

### 1. åŸºç¡€æ•°æ®ï¼ˆä¸åŸæ¥ç›¸åŒï¼‰

```python
dataset.all_features       # æ‰€æœ‰ç‰¹å¾
dataset.all_targets        # æ‰€æœ‰æ ‡ç­¾
dataset.all_known_mask     # å·²çŸ¥/æœªçŸ¥æ©ç 
dataset.all_labeled_mask   # æœ‰æ ‡ç­¾/æ— æ ‡ç­¾æ©ç 

dataset.train_features     # è®­ç»ƒé›†ç‰¹å¾
dataset.train_targets      # è®­ç»ƒé›†æ ‡ç­¾
dataset.train_known_mask   # è®­ç»ƒé›†å·²çŸ¥/æœªçŸ¥æ©ç 
dataset.train_labeled_mask # è®­ç»ƒé›†æ ‡ç­¾æ©ç 

dataset.test_features      # æµ‹è¯•é›†ç‰¹å¾
dataset.test_targets       # æµ‹è¯•é›†æ ‡ç­¾
dataset.test_known_mask    # æµ‹è¯•é›†å·²çŸ¥/æœªçŸ¥æ©ç 
dataset.test_labeled_mask  # æµ‹è¯•é›†æ ‡ç­¾æ©ç 
```

### 2. å…ƒä¿¡æ¯ï¼ˆæ–°å¢ï¼‰

```python
dataset.dataset_name       # æ•°æ®é›†åç§°: 'trees'
dataset.use_l2             # æ˜¯å¦L2å½’ä¸€åŒ–: True/False
dataset.source             # æ•°æ®æ¥æº: 'cache' or 'extraction'
dataset.feat_dim           # ç‰¹å¾ç»´åº¦: 768
```

### 3. é¢„è®¡ç®—ä¿¡æ¯ï¼ˆæ–°å¢ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰

```python
# æ ·æœ¬æ•°é‡
dataset.n_samples          # æ€»æ ·æœ¬æ•°
dataset.train_size         # è®­ç»ƒé›†æ ·æœ¬æ•°
dataset.test_size          # æµ‹è¯•é›†æ ·æœ¬æ•°
dataset.test_start_idx     # æµ‹è¯•é›†åœ¨åˆå¹¶æ•°æ®ä¸­çš„èµ·å§‹ç´¢å¼•

# åˆ’åˆ†æ ‡è®°
dataset.has_train_test_split  # æ˜¯å¦æœ‰è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†: True/False

# ç»Ÿè®¡ä¿¡æ¯
dataset.n_known            # å·²çŸ¥ç±»æ ·æœ¬æ•°
dataset.n_unknown          # æœªçŸ¥ç±»æ ·æœ¬æ•°
dataset.n_labeled          # æœ‰æ ‡ç­¾æ ·æœ¬æ•°
dataset.n_unlabeled        # æ— æ ‡ç­¾æ ·æœ¬æ•°

# ç±»åˆ«ä¿¡æ¯
dataset.n_classes          # æ€»ç±»åˆ«æ•°
dataset.n_known_classes    # å·²çŸ¥ç±»åˆ«æ•°
dataset.n_unknown_classes  # æœªçŸ¥ç±»åˆ«æ•°
```

### 4. ä¾¿æ·æ–¹æ³•

#### 4.1 è·å–èšç±»è¾“å…¥

```python
X, targets, known_mask, labeled_mask, train_size = dataset.get_clustering_input()

# ç›´æ¥ç”¨äºèšç±»
predictions, n_clusters, unknown_clusters = adaptive_density_clustering(
    X, targets, known_mask, labeled_mask,
    k=10, density_percentile=75, train_size=train_size, ...
)
```

#### 4.2 è·å–æµ‹è¯•é›†å­é›†

```python
# ç”¨äºACCè®¡ç®—ï¼ˆè‡ªåŠ¨å¤„ç†è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†ï¼‰
test_data = dataset.get_test_subset(predictions)

# test_data åŒ…å«:
# - features: æµ‹è¯•é›†ç‰¹å¾
# - targets: æµ‹è¯•é›†æ ‡ç­¾
# - known_mask: æµ‹è¯•é›†å·²çŸ¥/æœªçŸ¥æ©ç 
# - predictions: æµ‹è¯•é›†é¢„æµ‹ç»“æœ
# - n_samples: æµ‹è¯•é›†æ ·æœ¬æ•°

# ç›´æ¥ç”¨äºACCè®¡ç®—
all_acc, old_acc, new_acc = split_cluster_acc_v2(
    test_data['targets'],
    test_data['predictions'],
    test_data['known_mask']
)
```

#### 4.3 æ‰“å°æ‘˜è¦

```python
dataset.print_summary(silent=False)

# è¾“å‡ºç¤ºä¾‹:
# ğŸ“Š æ•°æ®é›†ä¿¡æ¯:
#    åç§°: trees
#    æ•°æ®æ¥æº: cache
#    L2å½’ä¸€åŒ–: æ˜¯
#    ç‰¹å¾ç»´åº¦: 768
#
# ğŸ“Š æ ·æœ¬ç»Ÿè®¡:
#    æ€»æ ·æœ¬æ•°: 3000
#    è®­ç»ƒé›†: 2500 æ ·æœ¬
#    æµ‹è¯•é›†: 500 æ ·æœ¬
#    å·²çŸ¥ç±»æ ·æœ¬: 2400 (80.0%)
#    æœªçŸ¥ç±»æ ·æœ¬: 600 (20.0%)
#    æœ‰æ ‡ç­¾æ ·æœ¬: 2000 (66.7%)
#    æ— æ ‡ç­¾æ ·æœ¬: 1000 (33.3%)
#
# ğŸ“Š ç±»åˆ«ç»Ÿè®¡:
#    æ€»ç±»åˆ«æ•°: 5
#    å·²çŸ¥ç±»åˆ«æ•°: 4
#    æœªçŸ¥ç±»åˆ«æ•°: 1
```

---

## å®Œæ•´ç¤ºä¾‹ï¼štest_superclass.py ä¼˜åŒ–å

### ä¼˜åŒ–å‰ï¼ˆ104è¡Œé‡å¤ä»£ç ï¼‰

```python
def test_adaptive_clustering_on_superclass(superclass_name, model_path, ...):
    data_provider = DataProvider(...)
    cache_info = data_provider.get_cache_info(superclass_name, use_l2=use_l2)

    if cache_info['exists']:
        feature_dict, source = data_provider.get_features(...)
        all_feats = feature_dict['all_features']
        all_targets = feature_dict['all_targets']
        # ... 10 more lines (12è¡Œæå–ä»£ç )
    else:
        model_loader = ModelLoader(...)
        model = model_loader.load(...)
        dataset_loader = DatasetLoader(...)
        data_loaders = dataset_loader.load(...)
        feature_dict, source = data_provider.get_features(...)
        all_feats = feature_dict['all_features']
        all_targets = feature_dict['all_targets']
        # ... 10 more lines (12è¡Œé‡å¤æå–ä»£ç )

    # æ‰‹åŠ¨æ‰“å°ç»Ÿè®¡ä¿¡æ¯ï¼ˆ8è¡Œï¼‰
    print(f"æ•°æ®æ¥æº: {source}")
    print(f"æ€»æ ·æœ¬æ•°: {len(all_feats)}")
    # ...

    train_size = len(train_feats) if use_train_and_test else None

    # èšç±»
    clustering_result = adaptive_density_clustering(
        all_feats, all_targets, all_known_mask, all_labeled_mask,
        train_size=train_size, ...
    )

    # è®¡ç®—æµ‹è¯•é›†èŒƒå›´ï¼ˆé‡å¤é€»è¾‘ï¼‰
    if use_train_and_test:
        test_start_idx = len(train_feats)
        test_predictions = predictions[test_start_idx:]
        test_targets_for_acc = all_targets[test_start_idx:]
        # ...
    else:
        test_predictions = predictions
        # ...

    # ACCè®¡ç®—
    all_acc, old_acc, new_acc = split_cluster_acc_v2(...)

    # å†æ¬¡è®¡ç®—test_start_idxï¼ˆé‡å¤ï¼‰
    if use_train_and_test:
        test_start_idx = len(train_feats)
        test_features_for_kmeans = all_feats[test_start_idx:]
    # ...
```

### ä¼˜åŒ–åï¼ˆä»£ç å‡å°‘50%+ï¼‰

```python
from clustering.data import EnhancedDataProvider

def test_adaptive_clustering_on_superclass(superclass_name, model_path, ...):
    # æ­¥éª¤1: è·å–è¶…ç±»é…ç½®
    superclass_info = get_superclass_info(superclass_name)

    # æ­¥éª¤2: åŠ è½½æ•°æ®ï¼ˆä¸€æ¬¡è°ƒç”¨ï¼Œè‡ªåŠ¨å¤„ç†ç¼“å­˜ï¼‰
    provider = EnhancedDataProvider(cache_base_dir='/data/gjx/checkpoints/features')
    dataset = provider.load_dataset(
        dataset_name=superclass_name,
        model_path=model_path,
        use_l2=use_l2,
        use_train_and_test=use_train_and_test,
        silent=silent
    )

    # æ‰“å°æ‘˜è¦ï¼ˆä¸€è¡Œä»£ç æ›¿ä»£8è¡Œï¼‰
    dataset.print_summary(silent=silent)

    # æ­¥éª¤3: è·å–èšç±»è¾“å…¥ï¼ˆä¸€è¡Œä»£ç ï¼‰
    X, targets, known_mask, labeled_mask, train_size = dataset.get_clustering_input()

    # æ­¥éª¤4: è¿è¡Œèšç±»
    clustering_result = adaptive_density_clustering(
        X, targets, known_mask, labeled_mask,
        k=k, density_percentile=density_percentile,
        train_size=train_size, ...
    )

    # æ­¥éª¤5: è·å–æµ‹è¯•é›†æ•°æ®å¹¶è®¡ç®—ACCï¼ˆä¸€è¡Œä»£ç æ›¿ä»£10è¡Œï¼‰
    predictions, n_clusters, unknown_clusters = clustering_result
    test_data = dataset.get_test_subset(predictions)

    all_acc, old_acc, new_acc = split_cluster_acc_v2(
        test_data['targets'],
        test_data['predictions'],
        test_data['known_mask']
    )

    # æ­¥éª¤6: K-meansåŸºçº¿ï¼ˆç›´æ¥ä½¿ç”¨test_dataï¼‰
    kmeans_baseline = test_kmeans_baseline(
        test_data['features'],
        test_data['targets'],
        test_data['known_mask'], ...
    )

    # æ­¥éª¤7: è¿”å›ç»“æœ
    return {
        'all_acc': all_acc,
        'old_acc': old_acc,
        'new_acc': new_acc,
        'n_clusters': n_clusters,
        'dataset': dataset,  # åŒ…å«æ‰€æœ‰å…ƒä¿¡æ¯
        ...
    }
```

---

## ä¼˜åŠ¿æ€»ç»“

### 1. æ¶ˆé™¤é‡å¤é€»è¾‘

| æ—§æ–¹å¼ | æ–°æ–¹å¼ | å‡å°‘ä»£ç  |
|--------|--------|----------|
| ä¸¤æ¬¡12è¡Œæ•°æ®æå– | ä¸€æ¬¡ | -12è¡Œ |
| 3æ¬¡è®¡ç®—train_size | é¢„è®¡ç®— | -2è¡Œ |
| 2æ¬¡è®¡ç®—test_start_idx | é¢„è®¡ç®— | -2è¡Œ |
| 8è¡Œç»Ÿè®¡æ‰“å° | 1è¡Œæ–¹æ³•è°ƒç”¨ | -7è¡Œ |
| 10è¡Œæµ‹è¯•é›†æå– | 1è¡Œæ–¹æ³•è°ƒç”¨ | -9è¡Œ |
| **æ€»è®¡** | **æ€»è®¡** | **-32è¡Œ** |

### 2. æ·»åŠ ç¼ºå¤±ä¿¡æ¯

| ä¿¡æ¯ | æ—§æ–¹å¼ | æ–°æ–¹å¼ |
|------|--------|--------|
| æ ·æœ¬ç´¢å¼• | âŒ æ—  | âœ… æ”¯æŒï¼ˆé¢„ç•™ï¼‰ |
| æ•°æ®æ¥æº | âš ï¸ å˜é‡ | âœ… dataset.source |
| ç‰¹å¾ç»´åº¦ | âŒ æ—  | âœ… dataset.feat_dim |
| train_size | âš ï¸ éœ€è®¡ç®— | âœ… dataset.train_size |
| test_start_idx | âš ï¸ éœ€è®¡ç®— | âœ… dataset.test_start_idx |
| ç»Ÿè®¡ä¿¡æ¯ | âŒ æ—  | âœ… å®Œæ•´ç»Ÿè®¡ |

### 3. æé«˜æ•ˆç‡

- âœ… é¿å…é‡å¤è®¡ç®—ï¼ˆtrain_size, test_start_idxç­‰ï¼‰
- âœ… ä¸€æ¬¡APIè°ƒç”¨è·å–æ‰€æœ‰æ•°æ®
- âœ… é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆn_known, n_classesç­‰ï¼‰
- âœ… ä¾¿æ·æ–¹æ³•å‡å°‘æ ·æ¿ä»£ç 

---

## è¿ç§»æŒ‡å—

### æ­¥éª¤1: å¯¼å…¥æ–°æ¨¡å—

```python
# æ—§ä»£ç 
from clustering.data import DataProvider, ModelLoader, DatasetLoader

# æ–°ä»£ç 
from clustering.data import EnhancedDataProvider
```

### æ­¥éª¤2: æ›¿æ¢æ•°æ®åŠ è½½é€»è¾‘

```python
# æ—§ä»£ç ï¼ˆ30+è¡Œï¼‰
data_provider = DataProvider(...)
cache_info = data_provider.get_cache_info(...)
if cache_info['exists']:
    feature_dict, source = data_provider.get_features(...)
    all_feats = feature_dict['all_features']
    # ... 20+ more lines
else:
    # ... 30+ more lines

# æ–°ä»£ç ï¼ˆ5è¡Œï¼‰
provider = EnhancedDataProvider(cache_base_dir='...')
dataset = provider.load_dataset(
    dataset_name=superclass_name,
    model_path=model_path,
    use_l2=use_l2
)
```

### æ­¥éª¤3: ä½¿ç”¨é¢„è®¡ç®—ä¿¡æ¯

```python
# æ—§ä»£ç 
train_size = len(train_feats) if use_train_and_test else None
test_start_idx = len(train_feats)

# æ–°ä»£ç 
train_size = dataset.train_size
test_start_idx = dataset.test_start_idx
```

### æ­¥éª¤4: ä½¿ç”¨ä¾¿æ·æ–¹æ³•

```python
# æ—§ä»£ç ï¼ˆè·å–èšç±»è¾“å…¥ï¼‰
clustering_result = adaptive_density_clustering(
    all_feats, all_targets, all_known_mask, all_labeled_mask,
    train_size=train_size, ...
)

# æ–°ä»£ç 
X, targets, known_mask, labeled_mask, train_size = dataset.get_clustering_input()
clustering_result = adaptive_density_clustering(
    X, targets, known_mask, labeled_mask,
    train_size=train_size, ...
)
```

```python
# æ—§ä»£ç ï¼ˆè·å–æµ‹è¯•é›†ï¼‰
if use_train_and_test:
    test_start_idx = len(train_feats)
    test_predictions = predictions[test_start_idx:]
    test_targets_for_acc = all_targets[test_start_idx:]
    test_known_mask_for_acc = all_known_mask[test_start_idx:]
else:
    test_predictions = predictions
    test_targets_for_acc = all_targets
    test_known_mask_for_acc = all_known_mask

# æ–°ä»£ç 
test_data = dataset.get_test_subset(predictions)
# ç›´æ¥ä½¿ç”¨ test_data['targets'], test_data['predictions'], test_data['known_mask']
```

---

## å…¼å®¹æ€§

- âœ… å®Œå…¨å‘åå…¼å®¹ï¼ˆæ—§ä»£ç ä»å¯ä½¿ç”¨ï¼‰
- âœ… æ•°æ®æ ¼å¼ç›¸åŒï¼ˆä¸ç¼“å­˜å…¼å®¹ï¼‰
- âœ… å¯é€æ­¥è¿ç§»ï¼ˆä¸éœ€è¦ä¸€æ¬¡æ€§ä¿®æ”¹æ‰€æœ‰ä»£ç ï¼‰

---

## æ›´æ–°å†å²

- **2025-01-20**: åˆå§‹ç‰ˆæœ¬ï¼Œåˆ›å»ºEnhancedDataProviderå’ŒEnhancedDataset
