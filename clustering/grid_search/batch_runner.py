#!/usr/bin/env python3
"""Batch grid-search runner with parallel support."""

import argparse
import os
from datetime import datetime
from itertools import product
from pathlib import Path
from typing import Iterable, List, Optional
from tqdm import tqdm
from multiprocessing import cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
import time
import signal
import sys

from config import grid_search_output_dir

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ä¿®å¤å¯¼å…¥é—®é¢˜ï¼šä½¿ç”¨ç»å¯¹å¯¼å…¥è€Œä¸æ˜¯ç›¸å¯¹å¯¼å…¥
from clustering.testing.test_superclass import test_adaptive_clustering_on_superclass


def _ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def _write_task_config(
    task_dir: Path,
    args: argparse.Namespace,
    superclasses: List[str],
    k_values: range,
    density_values: range,
    use_parallel: bool,
    max_workers: int,
    parsed_l2_components: Optional[List[str]],
) -> None:
    """å°†æœ¬æ¬¡æ‰¹å¤„ç†ä»»åŠ¡çš„å‚æ•°å¿«ç…§å†™å…¥ task_dir/task_config.txtã€‚

    å†…å®¹åŒ…å«ï¼šå‘½ä»¤è¡Œå‚æ•°ã€æ¨å¯¼å‚æ•°ï¼ˆk/density å–å€¼ã€å¹¶è¡Œã€workersã€è§£æåçš„ l2_componentsï¼‰ã€
    è¶…ç±»æ•°é‡ä¸åˆ—è¡¨ç­‰ï¼Œä¾¿äºåç»­è¿½æº¯ä¸å¤ç°å®éªŒã€‚
    """
    lines: List[str] = []
    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    lines.append('=' * 80)
    lines.append('Batch Grid Search Task Configuration')
    lines.append('=' * 80)
    lines.append(f'Task Directory: {task_dir.name}')
    lines.append(f'Generated At: {ts}')
    lines.append(f'Number of Superclasses: {len(superclasses)}')
    lines.append(f'Superclasses: {", ".join(superclasses)}')
    lines.append('')

    # å‘½ä»¤è¡Œå‚æ•°
    lines.append('=' * 80)
    lines.append('Command Line Arguments')
    lines.append('=' * 80)
    # ä»¥ç¨³å®šé¡ºåºå†™å‡º args.__dict__ï¼ˆæ³¨æ„å¯èƒ½åŒ…å« Noneï¼‰
    for key in sorted(vars(args).keys()):
        lines.append(f'{key}: {getattr(args, key)}')
    lines.append('')

    # æ¨å¯¼å‚æ•°
    lines.append('=' * 80)
    lines.append('Derived Parameters')
    lines.append('=' * 80)
    lines.append(f'k_range: [{k_values.start}, {k_values.stop})')
    lines.append(f'density_range: [{density_values.start}, {density_values.stop}) step={density_values.step}')
    lines.append(f'use_parallel: {use_parallel}')
    lines.append(f'actual_max_workers: {max_workers}')
    lines.append(f'parsed_l2_components: {parsed_l2_components}')
    # æ¯ä¸ªè¶…ç±»çš„ (k, density_percentile) ç»„åˆæ•°é‡
    total_per_superclass = len(k_values) * len(density_values)
    lines.append(f'total_combinations_per_superclass: {total_per_superclass}')
    lines.append('=' * 80)

    cfg_path = task_dir / 'task_config.txt'
    _ensure_dir(task_dir)
    cfg_path.write_text('\n'.join(lines), encoding='utf-8')


def _run_single_process(args_tuple):
    """åœ¨å•ç‹¬è¿›ç¨‹ä¸­è¿è¡Œå•ä¸ªå‚æ•°ç»„åˆï¼ˆç”¨äºå¤šè¿›ç¨‹æ± ï¼‰"""
    (superclass, model_path, k, density_percentile, eval_version, co_mode,
     co_manual, use_l2, eval_dense, dense_method, assign_model, voting_k,
     use_train_and_test, detail_dense, use_cluster_quality, cluster_distance_method,
     l1_type, separation_weight, penalty_weight, l2_components, l2_component_weights, run_kmeans) = args_tuple

    try:
        import io
        from contextlib import redirect_stdout, redirect_stderr

        captured_output = io.StringIO()

        with redirect_stdout(captured_output), redirect_stderr(captured_output):
            results = test_adaptive_clustering_on_superclass(
                superclass_name=superclass,
                model_path=model_path,
                use_train_and_test=use_train_and_test,
                k=k,
                density_percentile=density_percentile,
                eval_version=eval_version,
                run_kmeans_baseline=run_kmeans,
                co_mode=co_mode,
                co_manual=co_manual,
                use_l2=use_l2,
                eval_dense=eval_dense,
                silent=True,
                dense_method=dense_method,
                assign_model=assign_model,
                voting_k=voting_k,
                detail_dense=detail_dense,
                use_cluster_quality=use_cluster_quality,
                cluster_distance_method=cluster_distance_method,
                l1_type=l1_type,
                separation_weight=separation_weight,
                penalty_weight=penalty_weight,
                l2_components=l2_components,
                l2_component_weights=l2_component_weights,
            )

        return (k, density_percentile, results if isinstance(results, dict) else None)

    except Exception as exc:
        return (k, density_percentile, {'error': str(exc)})


def _run_single(
    superclass: str,
    model_path: str,
    k: int,
    density_percentile: int,
    *,
    eval_version: str,
    co_mode: int,
    co_manual: Optional[float],
    use_l2: bool,
    eval_dense: bool,
    dense_method: int,
    assign_model: int,
    voting_k: int,
    use_train_and_test: bool,
    detail_dense: bool,
    use_cluster_quality: bool,
    cluster_distance_method: int,
    l1_type: str,
    separation_weight: float,
    penalty_weight: float,
    l2_components,
    l2_component_weights,
    run_kmeans: bool,
) -> Optional[dict]:
    """Wrap test_adaptive_clustering_on_superclass with the desired defaults."""
    import io
    from contextlib import redirect_stdout, redirect_stderr

    try:
        # åˆ›å»ºä¸€ä¸ªç¼“å†²åŒºæ¥æ•è·æ‰€æœ‰è¾“å‡º
        captured_output = io.StringIO()

        # é‡å®šå‘æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯åˆ°ç¼“å†²åŒº
        with redirect_stdout(captured_output), redirect_stderr(captured_output):
            results = test_adaptive_clustering_on_superclass(
                superclass_name=superclass,
                model_path=model_path,
                use_train_and_test=use_train_and_test,
                k=k,
                density_percentile=density_percentile,
                eval_version=eval_version,
                run_kmeans_baseline=run_kmeans,
                co_mode=co_mode,
                co_manual=co_manual,
                use_l2=use_l2,
                eval_dense=eval_dense,
                silent=True,
                dense_method=dense_method,
                assign_model=assign_model,
                voting_k=voting_k,
                detail_dense=detail_dense,
                use_cluster_quality=use_cluster_quality,
                cluster_distance_method=cluster_distance_method,
                l1_type=l1_type,
                separation_weight=separation_weight,
                penalty_weight=penalty_weight,
                l2_components=l2_components,
                l2_component_weights=l2_component_weights,
            )

        return results if isinstance(results, dict) else None
    except Exception as exc:  # pylint: disable=broad-except
        return {'error': str(exc)}


def run_grid_search(
    superclass: str,
    model_path: str,
    k_values: Iterable[int],
    density_values: Iterable[int],
    *,
    output_dir: Path,
    eval_version: str,
    co_mode: int,
    co_manual: Optional[float],
    use_l2: bool,
    eval_dense: bool,
    dense_method: int,
    assign_model: int,
    voting_k: int,
    use_train_and_test: bool,
    detail_dense: bool,
    use_cluster_quality: bool = False,
    cluster_distance_method: int = 1,
    l1_type: str = 'cross_entropy',
    l2_components=None,
    max_workers: int = None,
    use_parallel: bool = True,
) -> Path:
    """Run grid search for a single superclass and persist results."""
    _ensure_dir(output_dir)
    timestamp = datetime.now().strftime('%m_%d_%H_%M')
    suffix = "_parallel" if use_parallel else ""
    output_file = output_dir / f"{superclass}_{timestamp}{suffix}.txt"

    k_values = list(k_values)
    density_values = list(density_values)
    total = len(k_values) * len(density_values)
    manual_co = co_manual

    if co_mode == 1 and manual_co is None:
        raise ValueError('co_mode=1 requires --co_manual')

    # ç¡®å®šå¹¶è¡Œè¿›ç¨‹æ•° - ä½¿ç”¨ä¸€åŠæ ¸å¿ƒæ•°é¿å…ç³»ç»Ÿè¿‡è½½
    if use_parallel and max_workers is None:
        max_workers = min(cpu_count() // 2, total)  # ä½¿ç”¨ä¸€åŠæ ¸å¿ƒæ•°

    if use_parallel:
        return _run_parallel_grid_search(
            superclass, model_path, k_values, density_values, output_file,
            eval_version, co_mode, manual_co, use_l2, eval_dense, dense_method,
            assign_model, voting_k, use_train_and_test, detail_dense,
            use_cluster_quality, cluster_distance_method, l1_type,
            1.0, 1.0, l2_components, None, max_workers
        )
    else:
        return _run_serial_grid_search(
            superclass, model_path, k_values, density_values, output_file,
            eval_version, co_mode, manual_co, use_l2, eval_dense, dense_method,
            assign_model, voting_k, use_train_and_test, detail_dense,
            use_cluster_quality, cluster_distance_method, l1_type,
            1.0, 1.0, l2_components, None
        )


def _run_parallel_grid_search(
    superclass, model_path, k_values, density_values, output_file,
    eval_version, co_mode, manual_co, use_l2, eval_dense, dense_method,
    assign_model, voting_k, use_train_and_test, detail_dense,
    use_cluster_quality, cluster_distance_method, l1_type,
    separation_weight, penalty_weight, l2_components, l2_component_weights, max_workers
):
    """å¹¶è¡Œç‰ˆæœ¬çš„ç½‘æ ¼æœç´¢"""
    total = len(k_values) * len(density_values)

    # å‡†å¤‡å‚æ•°ç»„åˆ
    param_combinations = list(product(k_values, density_values))
    process_args = []
    for i, (k, density_percentile) in enumerate(param_combinations):
        args_tuple = (
            superclass, model_path, k, density_percentile, eval_version,
            co_mode, manual_co, use_l2, eval_dense, dense_method,
            assign_model, voting_k, use_train_and_test, detail_dense,
            use_cluster_quality, cluster_distance_method, l1_type,
            separation_weight, penalty_weight, l2_components, l2_component_weights,
            i == 0  # åªåœ¨ç¬¬ä¸€æ¬¡è¿è¡ŒK-means
        )
        process_args.append(args_tuple)

    # å¼€å§‹å¹¶è¡Œæ‰§è¡Œ
    results_dict = {}
    best = None
    kmeans_cache = None
    start_time = time.time()
    interrupted = False

    def signal_handler(signum, frame):
        nonlocal interrupted
        interrupted = True
        print("\nğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        print("â³ ç­‰å¾…å½“å‰ä»»åŠ¡å®Œæˆå¹¶æ¸…ç†èµ„æº...")

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)

    try:
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_params = {
                executor.submit(_run_single_process, args): (args[2], args[3])
                for args in process_args
            }

            pbar = tqdm(
                total=total,
                desc=f"Grid search {superclass}",
                unit="param",
                ncols=120,
                position=1,
                leave=False
            )

            for future in as_completed(future_to_params):
                # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
                if interrupted:
                    pbar.set_description("ğŸ›‘ æ­£åœ¨å–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                    # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                    for f in future_to_params:
                        if not f.done():
                            f.cancel()
                    break

                k, density_percentile = future_to_params[future]

                try:
                    k_result, dp_result, results = future.result()

                    if results and 'error' not in results:
                        results_dict[(k, density_percentile)] = results

                        if kmeans_cache is None and 'kmeans_all_acc' in results:
                            kmeans_cache = {
                                'all_acc': results.get('kmeans_all_acc', 0.0),
                                'old_acc': results.get('kmeans_old_acc', 0.0),
                                'new_acc': results.get('kmeans_new_acc', 0.0),
                                'clusters': results.get('kmeans_n_clusters', 0),
                            }

                        current_acc = results.get('all_acc', 0.0)
                        if best is None or current_acc > best['all_acc']:
                            best = {
                                'params': (k, density_percentile),
                                'all_acc': current_acc,
                            }
                            pbar.set_postfix({
                                'best_k': k,
                                'best_dp': density_percentile,
                                'best_acc': f"{current_acc:.4f}",
                                'status': 'ğŸ¯NEW BEST'
                            })
                        else:
                            pbar.set_postfix({
                                'k': k,
                                'dp': density_percentile,
                                'acc': f"{current_acc:.4f}",
                                'best_acc': f"{best['all_acc']:.4f}" if best else "N/A"
                            })
                    else:
                        error_msg = results.get('error', 'unknown error') if results else 'no results'
                        pbar.set_postfix({
                            'k': k,
                            'dp': density_percentile,
                            'status': f'FAILED: {error_msg[:20]}...'
                        })

                    pbar.update(1)

                except Exception as exc:
                    pbar.set_postfix({
                        'k': k,
                        'dp': density_percentile,
                        'status': f'ERROR: {str(exc)[:20]}...'
                    })
                    pbar.update(1)

            pbar.close()

    except KeyboardInterrupt:
        interrupted = True
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†è¿›ç¨‹æ± ...")

    finally:
        # æ¢å¤é»˜è®¤ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, signal.SIG_DFL)

    elapsed_time = time.time() - start_time

    # å¦‚æœè¢«ä¸­æ–­ï¼Œåœ¨è¾“å‡ºæ–‡ä»¶ä¸­æ ‡è®°
    if interrupted:
        print(f"âš ï¸  æœç´¢è¢«ä¸­æ–­ï¼Œå·²å®Œæˆ {len(results_dict)}/{total} ä¸ªå‚æ•°ç»„åˆ")

    # å†™å…¥ç»“æœæ–‡ä»¶
    with output_file.open('w', encoding='utf-8') as handle:
        handle.write(f"Grid search for superclass: {superclass}\n")
        handle.write(f"Model path: {model_path}\n")
        handle.write(f"k values: {k_values}\n")
        handle.write(f"density percentiles: {density_values}\n")
        handle.write(f"l1_type: {l1_type}\n")
        handle.write(f"l2_components: {l2_components}\n")
        handle.write(f"l2_component_weights: {l2_component_weights}\n")
        handle.write(f"Parallel workers: {max_workers}\n")
        handle.write(f"Total time: {elapsed_time:.2f}s\n")
        if len(results_dict) > 0:
            handle.write(f"Avg per combination: {elapsed_time/len(results_dict):.2f}s\n")
        handle.write(f"Success: {len(results_dict)}/{total} combinations\n")
        if interrupted:
            handle.write("âš ï¸  Search was interrupted by user\n")
        handle.write('=' * 80 + '\n\n')

        # æŒ‰ç…§kå’Œdensity_percentileæ’åºå†™å…¥ç»“æœ
        for (k, density_percentile) in sorted(results_dict.keys()):
            results = results_dict[(k, density_percentile)]
            handle.write(f"k={k}, density_percentile={density_percentile}\n")
            handle.write('-' * 80 + '\n')
            handle.write(f"all_acc: {results.get('all_acc', 0.0):.4f}\n")
            handle.write(f"old_acc: {results.get('old_acc', 0.0):.4f}\n")
            handle.write(f"new_acc: {results.get('new_acc', 0.0):.4f}\n")
            handle.write(f"clusters: {results.get('n_clusters', 0)}\n")

            # æ·»åŠ labeled_acc
            labeled_acc_value = results.get('labeled_acc')
            if labeled_acc_value is not None:
                handle.write(f"labeled_acc: {labeled_acc_value:.4f}\n")
            else:
                handle.write("labeled_acc: N/A\n")

            # æ·»åŠ L1æŸå¤±å€¼
            l1_value = results.get('l1')
            if l1_value is not None:
                handle.write(f"l1_loss: {l1_value:.4f}\n")
            else:
                handle.write("l1_loss: N/A\n")

            # æ·»åŠ èšç±»è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            loss_dict = results.get('loss_dict', {})
            l2_metrics = loss_dict.get('l2_metrics', {})
            cluster_quality = l2_metrics.get('cluster_quality', {})

            quality_score = cluster_quality.get('quality_score')
            separation_score = cluster_quality.get('separation_score')
            penalty_score = cluster_quality.get('penalty_score')

            if quality_score is not None:
                handle.write(f"quality_score: {quality_score:.4f}\n")
            if separation_score is not None:
                handle.write(f"separation_score: {separation_score:.4f}\n")
            if penalty_score is not None:
                handle.write(f"penalty_score: {penalty_score:.4f}\n")

            components = l2_metrics.get('components', {})
            for comp_name, comp_info in components.items():
                if not isinstance(comp_info, dict):
                    continue
                value = comp_info.get('value')
                contribution = comp_info.get('contribution')
                orientation = comp_info.get('orientation')
                if value is not None:
                    handle.write(f"component_{comp_name}_value: {float(value):.4f}\n")
                if contribution is not None:
                    handle.write(f"component_{comp_name}_contribution: {float(contribution):.4f}\n")
                if orientation is not None:
                    handle.write(f"component_{comp_name}_orientation: {orientation}\n")

            handle.write(f"l2_components: {results.get('l2_components')}\n")
            handle.write(f"l2_component_weights: {results.get('l2_component_weights')}\n")
            handle.write(f"l2_component_params: {results.get('l2_component_params')}\n")

            handle.write('\n')

        handle.write('=' * 80 + '\n')
        if best:
            handle.write(f"Best params: k={best['params'][0]}, density_percentile={best['params'][1]}\n")
            handle.write(f"Best all_acc: {best['all_acc']:.4f}\n")

        if kmeans_cache:
            handle.write(f"K-means baseline: all_acc={kmeans_cache['all_acc']:.4f}\n")

    return output_file


def _run_serial_grid_search(
    superclass, model_path, k_values, density_values, output_file,
    eval_version, co_mode, manual_co, use_l2, eval_dense, dense_method,
    assign_model, voting_k, use_train_and_test, detail_dense,
    use_cluster_quality, cluster_distance_method, l1_type,
    separation_weight, penalty_weight, l2_components, l2_component_weights
):
    """ä¸²è¡Œç‰ˆæœ¬çš„ç½‘æ ¼æœç´¢ï¼ˆåŸå®ç°ï¼‰"""
    total = len(k_values) * len(density_values)

    with output_file.open('w', encoding='utf-8') as handle:
        handle.write(f"Grid search for superclass: {superclass}\n")
        handle.write(f"Model path: {model_path}\n")
        handle.write(f"k values: {k_values}\n")
        handle.write(f"density percentiles: {density_values}\n")
        handle.write(f"eval_version: {eval_version}\n")
        handle.write(f"co_mode: {co_mode}\n")
        handle.write(f"co_manual: {manual_co}\n")
        handle.write(f"use_l2: {use_l2}\n")
        handle.write(f"eval_dense: {eval_dense}\n")
        handle.write(f"dense_method: {dense_method}\n")
        handle.write(f"assign_model: {assign_model}\n")
        handle.write(f"voting_k: {voting_k}\n")
        handle.write(f"use_train_and_test: {use_train_and_test}\n")
        handle.write(f"detail_dense: {detail_dense}\n")
        handle.write(f"use_cluster_quality: {use_cluster_quality}\n")
        handle.write(f"cluster_distance_method: {cluster_distance_method}\n")
        handle.write(f"l1_type: {l1_type}\n")
        handle.write(f"l2_components: {l2_components}\n")
        handle.write(f"l2_component_weights: {l2_component_weights}\n")
        handle.write('=' * 80 + '\n\n')

        best = None
        kmeans_cache = None
        param_combinations = list(product(k_values, density_values))

        pbar = tqdm(
            param_combinations,
            desc=f"Grid search for {superclass}",
            unit="param",
            ncols=120,
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}",
            position=1,
            leave=False
        )

        for index, (k, density_percentile) in enumerate(pbar, start=1):
            pbar.set_postfix({
                'k': k,
                'dp': density_percentile,
                'best_acc': f"{best['all_acc']:.4f}" if best else "N/A"
            })

            handle.write(f"k={k}, density_percentile={density_percentile}\n")
            handle.write('-' * 80 + '\n')

            results = _run_single(
                superclass, model_path, k, density_percentile,
                eval_version=eval_version, co_mode=co_mode, co_manual=manual_co,
                use_l2=use_l2, eval_dense=eval_dense, dense_method=dense_method,
                assign_model=assign_model, voting_k=voting_k,
                use_train_and_test=use_train_and_test, detail_dense=detail_dense,
                use_cluster_quality=use_cluster_quality,
                cluster_distance_method=cluster_distance_method,
                l1_type=l1_type,
                separation_weight=separation_weight,
                penalty_weight=penalty_weight,
                l2_components=l2_components,
                l2_component_weights=l2_component_weights,
                run_kmeans=kmeans_cache is None,
            )

            if not results or 'error' in results:
                handle.write(f"FAILED: {results.get('error', 'unknown error')}\n\n")
                pbar.set_postfix({
                    'k': k, 'dp': density_percentile, 'status': 'FAILED',
                    'best_acc': f"{best['all_acc']:.4f}" if best else "N/A"
                })
                continue

            if kmeans_cache is None:
                kmeans_cache = {
                    'all_acc': results.get('kmeans_all_acc', 0.0),
                    'old_acc': results.get('kmeans_old_acc', 0.0),
                    'new_acc': results.get('kmeans_new_acc', 0.0),
                    'clusters': results.get('kmeans_n_clusters', 0),
                }
            else:
                results['kmeans_all_acc'] = kmeans_cache['all_acc']
                results['kmeans_old_acc'] = kmeans_cache['old_acc']
                results['kmeans_new_acc'] = kmeans_cache['new_acc']
                results['kmeans_n_clusters'] = kmeans_cache['clusters']

            current_acc = results.get('all_acc', 0.0)
            handle.write(f"all_acc: {current_acc:.4f}\n")
            handle.write(f"old_acc: {results.get('old_acc', 0.0):.4f}\n")
            handle.write(f"new_acc: {results.get('new_acc', 0.0):.4f}\n")
            handle.write(f"clusters: {results.get('n_clusters', 0)}\n")

            # æ·»åŠ labeled_acc
            labeled_acc_value = results.get('labeled_acc')
            if labeled_acc_value is not None:
                handle.write(f"labeled_acc: {labeled_acc_value:.4f}\n")
            else:
                handle.write("labeled_acc: N/A\n")

            # æ·»åŠ L1æŸå¤±å€¼
            l1_value = results.get('l1')
            if l1_value is not None:
                handle.write(f"l1_loss: {l1_value:.4f}\n")
            else:
                handle.write("l1_loss: N/A\n")

            # æ·»åŠ èšç±»è´¨é‡æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            loss_dict = results.get('loss_dict', {})
            l2_metrics = loss_dict.get('l2_metrics', {})
            cluster_quality = l2_metrics.get('cluster_quality', {})

            quality_score = cluster_quality.get('quality_score')
            separation_score = cluster_quality.get('separation_score')
            penalty_score = cluster_quality.get('penalty_score')

            if quality_score is not None:
                handle.write(f"quality_score: {quality_score:.4f}\n")
            if separation_score is not None:
                handle.write(f"separation_score: {separation_score:.4f}\n")
            if penalty_score is not None:
                handle.write(f"penalty_score: {penalty_score:.4f}\n")

            components = l2_metrics.get('components', {})
            for comp_name, comp_info in components.items():
                if not isinstance(comp_info, dict):
                    continue
                value = comp_info.get('value')
                contribution = comp_info.get('contribution')
                orientation = comp_info.get('orientation')
                if value is not None:
                    handle.write(f"component_{comp_name}_value: {float(value):.4f}\n")
                if contribution is not None:
                    handle.write(f"component_{comp_name}_contribution: {float(contribution):.4f}\n")
                if orientation is not None:
                    handle.write(f"component_{comp_name}_orientation: {orientation}\n")

            handle.write('\n')

            if best is None or current_acc > best['all_acc']:
                best = {'params': (k, density_percentile), 'all_acc': current_acc}
                pbar.set_postfix({
                    'k': k, 'dp': density_percentile, 'acc': f"{current_acc:.4f}",
                    'best_acc': f"{current_acc:.4f}", 'status': 'ğŸ¯NEW BEST'
                })
            else:
                pbar.set_postfix({
                    'k': k, 'dp': density_percentile, 'acc': f"{current_acc:.4f}",
                    'best_acc': f"{best['all_acc']:.4f}"
                })

        pbar.close()
        print(f"\r{' ' * 120}\r", end='', flush=True)

        handle.write('=' * 80 + '\n')
        handle.write(f"Completed {index if total else 0}/{total} combinations\n")
        if best:
            handle.write(f"Best params: k={best['params'][0]}, density_percentile={best['params'][1]}\n")
            handle.write(f"Best all_acc: {best['all_acc']:.4f}\n")

    return output_file


def parse_superclasses(defaults: List[str], override: Optional[str], file_path: Optional[str]) -> List[str]:
    if override:
        return [item.strip() for item in override.split(',') if item.strip()]
    if file_path:
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f'superclass list file not found: {file_path}')
        return [line.strip() for line in path.read_text(encoding='utf-8').splitlines() if line.strip() and not line.startswith('#')]
    return defaults


DEFAULT_SUPERCLASSES = [
    "trees",
    "humans",
    "vehicles",
    "buildings",
]

# ALL_15_SUPERCLASSES åˆ—è¡¨æ¥æºï¼šdata/cifar100_superclass.py ä¸­ CIFAR100_SUPERCLASSES å®šä¹‰ã€‚
# ä¸ºé™ä½è€¦åˆé¿å…å¯¼å…¥å¤±è´¥ï¼Œè¿™é‡Œç¡¬ç¼–ç ï¼›å¦‚ä¸Šæ¸¸å˜åŠ¨è¯·åŒæ­¥æ›´æ–°ã€‚
ALL_15_SUPERCLASSES = [
    "trees",
    "flowers",
    "fruits_vegetables",
    "mammals",
    "marine_animals",
    "insects_arthropods",
    "reptiles",
    "humans",
    "furniture",
    "containers",
    "vehicles",
    "electronic_devices",
    "buildings",
    "terrain",
    "weather_phenomena",
]


def main() -> None:
    parser = argparse.ArgumentParser(description='Python-based batch grid search runner.')
    parser.add_argument('--model_path', type=str, default=None,
                        help='é¢„è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¦‚ /path/to/model.pthï¼‰ã€‚é»˜è®¤Noneè¡¨ç¤ºä¼˜å…ˆä½¿ç”¨ç¼“å­˜çš„ç‰¹å¾æ–‡ä»¶ï¼›é¦–æ¬¡è¿è¡Œéœ€æä¾›æ¨¡å‹ä»¥æå–ç‰¹å¾ã€‚')
    parser.add_argument('--output_dir', type=str, default=grid_search_output_dir,
                        help='æœç´¢ç»“æœè¾“å‡ºç›®å½•ï¼ˆåŒ…å«æ¯ä¸ªè¶…ç±»çš„ *.txt ç»“æœæ–‡ä»¶ï¼‰')
    parser.add_argument('--superclasses', type=str, default=None,
                        help='é€—å·åˆ†éš”çš„è¶…ç±»åˆ—è¡¨ï¼ˆä¾‹å¦‚: trees,humansï¼‰ã€‚è‹¥æœªæä¾›ï¼Œå°†ä½¿ç”¨é»˜è®¤é›†åˆæˆ– --superclass_file æŒ‡å®šçš„åˆ—è¡¨ã€‚')
    parser.add_argument('--superclass_file', type=str, default=None,
                        help='å¯é€‰ï¼šæ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªè¶…ç±»åç§°ï¼›ä»¥ # å¼€å¤´çš„è¡Œä¸ºæ³¨é‡Šã€‚')
    parser.add_argument('--k_min', type=int, default=3,
                        help='KNN å›¾çš„æœ€å° k å€¼ï¼ˆé»˜è®¤3ï¼‰ã€‚k æ§åˆ¶è¿‘é‚»æ•°é‡ï¼Œå½±å“å›¾è¿é€šæ€§ä¸ç¨³å¥æ€§ã€‚')
    parser.add_argument('--k_max', type=int, default=21,
                        help='KNN å›¾çš„æœ€å¤§ k å€¼ä¸Šç•Œï¼ˆé»˜è®¤21ï¼ŒåŠå¼€åŒºé—´ï¼‰ã€‚')
    parser.add_argument('--density_min', type=int, default=40,
                        help='å¯†åº¦é˜ˆå€¼çš„æœ€å°ç™¾åˆ†ä½ï¼ˆé»˜è®¤40ï¼‰ã€‚')
    parser.add_argument('--density_max', type=int, default=100,
                        help='å¯†åº¦é˜ˆå€¼çš„æœ€å¤§ç™¾åˆ†ä½ï¼ˆé»˜è®¤100ï¼Œä¸å«ï¼‰ã€‚')
    parser.add_argument('--density_step', type=int, default=5,
                        help='å¯†åº¦é˜ˆå€¼æ­¥é•¿ï¼ˆé»˜è®¤5ï¼‰ã€‚')
    parser.add_argument('--co_mode', type=int, default=2, choices=[1, 2, 3],
                        help='cutoff æ¨¡å¼ï¼š1=æ‰‹åŠ¨ï¼ˆéœ€ --co_manualï¼‰ï¼Œ2=è‡ªåŠ¨ï¼ˆæ¨èï¼‰ï¼Œ3=æ—  cutoffã€‚')
    parser.add_argument('--co_manual', type=float, default=None,
                        help='co_mode=1 æ—¶çš„æ‰‹åŠ¨ cutoff å€¼ã€‚')
    parser.add_argument('--eval_version', type=str, default='v2', choices=['v1', 'v2'],
                        help='è¯„ä¼°ç‰ˆæœ¬ï¼ˆé»˜è®¤v2ï¼‰ã€‚')
    parser.add_argument('--dense_method', type=int, default=0, choices=[0, 1, 2, 3],
                        help='å¯†é›†æ ·æœ¬é€‰æ‹©æ–¹æ³•ï¼š0=å…³é—­ï¼Œ1=å±€éƒ¨å¯†åº¦ï¼Œ2=å…¨å±€å¯†åº¦ï¼Œ3=æ··åˆã€‚')
    parser.add_argument('--assign_model', type=int, default=2, choices=[1, 2, 3],
                        help='æ ·æœ¬åˆ†é…æ¨¡å‹ï¼š1=æœ€è¿‘è´¨å¿ƒï¼Œ2=KNN æŠ•ç¥¨ï¼ˆé»˜è®¤ï¼‰ï¼Œ3=è½¯åˆ†é…ã€‚')
    parser.add_argument('--voting_k', type=int, default=5,
                        help='KNN æŠ•ç¥¨çš„ kï¼ˆé»˜è®¤5ï¼‰ã€‚')
    parser.add_argument('--use_l2', action='store_true')
    parser.add_argument('--no_l2', dest='use_l2', action='store_false')
    parser.set_defaults(use_l2=True)
    parser.add_argument('--eval_dense', action='store_true')
    parser.add_argument('--use_train_and_test', action='store_true', default=True)
    parser.add_argument('--detail_dense', action='store_true')

    # èšç±»è´¨é‡è¯„ä¼°å‚æ•°
    parser.add_argument('--use_cluster_quality', action='store_true', default=False,
                        help='æ˜¯å¦ä½¿ç”¨èšç±»è´¨é‡è¯„ä¼°æŒ‡æ ‡ä½œä¸ºL2æŸå¤±')
    parser.add_argument('--cluster_distance_method', type=int, default=1, choices=[1, 2, 3],
                        help='ç°‡è·ç¦»è®¡ç®—æ–¹æ³•ï¼š1=æœ€è¿‘kå¯¹ç‚¹å¹³å‡è·ç¦»ï¼Œ2=æ‰€æœ‰ç‚¹å¯¹å¹³å‡è·ç¦»ï¼Œ3=åŸå‹è·ç¦»')
    parser.add_argument('--l1_type', type=str, default='cross_entropy', choices=['accuracy', 'cross_entropy'],
                        help='L1ç›‘ç£æŸå¤±ç±»å‹ï¼šaccuracy=åŸºäºåŒˆç‰™åˆ©ç®—æ³•çš„å‡†ç¡®ç‡æŸå¤±(1-ACC)ï¼Œcross_entropy=åŸºäºç°‡ç±»åˆ«åˆ†å¸ƒçš„äº¤å‰ç†µæŸå¤±ï¼ˆé»˜è®¤ï¼‰')

    # L2ç»„ä»¶é€‰æ‹©ï¼ˆä»…ç”¨äºæ•°æ®æ”¶é›†é˜¶æ®µï¼Œä¸æ¶‰åŠæƒé‡é…ç½®ï¼‰
    parser.add_argument('--l2_components', type=str, default=None,
                        help='æŒ‡å®šè¦è®¡ç®—çš„ L2 ç»„ä»¶ï¼ˆç©ºæ ¼æˆ–é€—å·åˆ†éš”ï¼‰ã€‚å¯é€‰: separation, penalty, silhouetteã€‚ç¤ºä¾‹: "separation silhouette"ã€‚')

    # å¹¶è¡Œç›¸å…³å‚æ•°
    parser.add_argument('--max_workers', type=int, default=None,
                        help='å¹¶è¡Œè¿›ç¨‹æ•°ä¸Šé™ï¼ˆé»˜è®¤è‡ªåŠ¨=CPUæ ¸å¿ƒæ•°çš„ä¸€åŠä¸”ä¸è¶…è¿‡ç»„åˆæ•°ï¼‰ã€‚')
    parser.add_argument('--no_parallel', action='store_true',
                        help='ç¦ç”¨å¹¶è¡Œï¼Œæ”¹ä¸ºä¸²è¡Œæ‰§è¡Œï¼ˆä¾¿äºè°ƒè¯•æˆ–å†…å­˜å—é™ç¯å¢ƒï¼‰ã€‚')
    parser.add_argument('--count_all', action='store_true', default=False,
                        help='å¯ç”¨å…¨éƒ¨15ä¸ªè‡ªå®šä¹‰è¶…ç±»ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œè¦†ç›– --superclasses ä¸ --superclass_fileï¼‰ã€‚ '
                             'åŒ…å«: trees, flowers, fruits_vegetables, mammals, marine_animals, insects_arthropods, '
                             'reptiles, humans, furniture, containers, vehicles, electronic_devices, buildings, terrain, weather_phenomenaã€‚')

    args = parser.parse_args()

    def _parse_l2_components(value: str):
        if value is None:
            return None
        return [part.strip() for part in value.replace(',', ' ').split() if part.strip()]

    parsed_l2_components = _parse_l2_components(args.l2_components)

    if args.count_all:
        superclasses = ALL_15_SUPERCLASSES
        names = ', '.join(superclasses)
        print(f"â„¹ï¸  å¯ç”¨å…¨éƒ¨ {len(superclasses)} ä¸ªè¶…ç±»ï¼ˆ--count_allï¼‰")
        print(f"   è¶…ç±»åˆ—è¡¨: {names}")
    else:
        superclasses = parse_superclasses(DEFAULT_SUPERCLASSES, args.superclasses, args.superclass_file)
    output_root = Path(args.output_dir)

    k_values = range(args.k_min, args.k_max)
    density_values = range(args.density_min, args.density_max, args.density_step)

    # ç¡®å®šæ¯ä¸ªè¶…ç±»çš„å¹¶è¡Œè¿›ç¨‹æ•°
    use_parallel = not args.no_parallel
    total_combinations = len(k_values) * len(density_values)

    if use_parallel:
        if args.max_workers:
            max_workers = min(args.max_workers, total_combinations)
        else:
            max_workers = min(cpu_count() // 2, total_combinations)  # ä½¿ç”¨ä¸€åŠæ ¸å¿ƒæ•°
    else:
        max_workers = 1

    # åˆ›å»ºä»»åŠ¡çº§ç›®å½•ï¼ˆæŒ‰è¶…ç±»æ•°é‡ä¸æ—¶é—´æˆ³å‘½åï¼‰å¹¶å†™å…¥é…ç½®
    timestamp = datetime.now().strftime('%m_%d_%H_%M')
    task_dirname = f"{len(superclasses)}class_{timestamp}"
    task_dir = output_root / task_dirname
    _ensure_dir(task_dir)
    _write_task_config(
        task_dir=task_dir,
        args=args,
        superclasses=superclasses,
        k_values=k_values,
        density_values=density_values,
        use_parallel=use_parallel,
        max_workers=max_workers,
        parsed_l2_components=parsed_l2_components,
    )

    # æ·»åŠ è¶…ç±»çº§åˆ«çš„è¿›åº¦æ¡
    superclass_pbar = tqdm(
        superclasses,
        desc="Processing superclasses",
        unit="superclass",
        position=0,
        leave=True,
        ncols=120
    )

    for superclass in superclass_pbar:
        superclass_pbar.set_description(f"Processing {superclass}")

        output_file = run_grid_search(
            superclass,
            args.model_path,
            k_values,
            density_values,
            output_dir=task_dir / superclass,
            eval_version=args.eval_version,
            co_mode=args.co_mode,
            co_manual=args.co_manual,
            use_l2=args.use_l2,
            eval_dense=args.eval_dense,
            dense_method=args.dense_method,
            assign_model=args.assign_model,
            voting_k=args.voting_k,
            use_train_and_test=args.use_train_and_test,
            detail_dense=args.detail_dense,
            use_cluster_quality=args.use_cluster_quality,
            cluster_distance_method=args.cluster_distance_method,
            l1_type=args.l1_type,
            l2_components=parsed_l2_components,
            max_workers=max_workers,
            use_parallel=use_parallel,
        )

        superclass_pbar.set_postfix_str(f"Saved: {output_file.name}")

    superclass_pbar.close()

    if use_parallel:
        mode_str = f"parallel ({max_workers} workers)"
    else:
        mode_str = "serial"

    print("\nğŸ‰ Batch grid search completed!")
    print(f"ğŸ“ Task directory: {task_dir}")
    print(f"ğŸ§¾ Config file: {task_dir / 'task_config.txt'}")
    print(f"ğŸ’¡ ä¸‹æ¸¸ L2/L1L2 å·¥å…·è¯·æŒ‡å®š --search_dir {task_dir}")
    print(f"ğŸ“Š Processed {len(superclasses)} superclasses ({mode_str})")


if __name__ == '__main__':
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    main()
