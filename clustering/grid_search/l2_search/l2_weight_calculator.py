#!/usr/bin/env python3
"""
L2æƒé‡è®¡ç®—å™¨æ¨¡å—
åŸºäºç°æœ‰heatmap.pyçš„load_existing_resultså‡½æ•°ï¼Œç²¾ç¡®å¤ç”¨å…¶è§£æé€»è¾‘
ç”¨äºè¯»å–ç½‘æ ¼æœç´¢ç»“æœå¹¶è®¡ç®—ä¸åŒæƒé‡ç»„åˆä¸‹çš„åŠ æƒL2æŸå¤±
"""

import os
import sys
import numpy as np
from typing import Dict, Tuple, Optional

from config import grid_search_output_dir

# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))

# ç›´æ¥å¤ç”¨ç°æœ‰çš„load_existing_resultså‡½æ•°
from clustering.grid_search.heatmap import load_existing_results


def load_raw_scores(superclass_name: str, search_dir: str = grid_search_output_dir) -> Optional[Dict]:
    """
    ä»batch_runnerçš„è¾“å‡ºæ–‡ä»¶ä¸­è§£æåŸå§‹æŒ‡æ ‡
    ç›´æ¥å¤ç”¨heatmap.pyä¸­å·²ç»éªŒè¯çš„load_existing_resultså‡½æ•°
    
    Args:
        superclass_name: è¶…ç±»åç§°
        search_dir: æœç´¢ç»“æœç›®å½•
    
    Returns:
        dict: {(k, dp): {'all_acc': float, 'separation_score': float, 'penalty_score': float, ...}} æˆ– None
    """
    print(f"ğŸ“‚ åŠ è½½ {superclass_name} çš„æœç´¢ç»“æœ...")
    
    # ç›´æ¥ä½¿ç”¨ç°æœ‰çš„è§£æå‡½æ•°
    results_dict = load_existing_results(superclass_name, search_dir)
    
    if results_dict is None:
        return None
    
    # éªŒè¯L2æƒé‡æ¢ç´¢æ‰€éœ€çš„å…³é”®å­—æ®µ
    total_count = len(results_dict)
    sep_available = sum(1 for v in results_dict.values() if v.get('separation_score') is not None)
    pen_available = sum(1 for v in results_dict.values() if v.get('penalty_score') is not None)
    
    print(f"ğŸ“Š å…³é”®æŒ‡æ ‡ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°ç»„åˆ: {total_count}")
    print(f"   separation_scoreå¯ç”¨: {sep_available}/{total_count}")
    print(f"   penalty_scoreå¯ç”¨: {pen_available}/{total_count}")
    
    if sep_available == 0:
        print("âŒ é”™è¯¯: separation_scoreå®Œå…¨ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡ŒL2æƒé‡æ¢ç´¢")
        return None
    
    if pen_available == 0:
        print("âŒ é”™è¯¯: penalty_scoreå®Œå…¨ç¼ºå¤±ï¼Œæ— æ³•è¿›è¡ŒL2æƒé‡æ¢ç´¢")
        return None
    
    valid_pairs = sum(1 for v in results_dict.values() 
                     if v.get('separation_score') is not None and v.get('penalty_score') is not None)
    
    if valid_pairs < total_count * 0.5:  # è‡³å°‘50%çš„æ•°æ®æœ‰æ•ˆ
        print(f"âš ï¸  è­¦å‘Š: æœ‰æ•ˆæ•°æ®å¯¹ä¸è¶³ ({valid_pairs}/{total_count})")
        print("   å»ºè®®é‡æ–°è¿è¡Œç½‘æ ¼æœç´¢ä»¥è·å–å®Œæ•´çš„èšç±»è´¨é‡æŒ‡æ ‡")
        return None
    
    print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œæœ‰æ•ˆæ•°æ®å¯¹: {valid_pairs}/{total_count}")
    return results_dict


def compute_weighted_l2(results_dict: Dict, w_sep: float, w_pen: float) -> Dict:
    """
    è®¡ç®—åŠ æƒL2æŸå¤±
    
    Args:
        results_dict: ä»load_raw_scoresè·å–çš„åŸå§‹ç»“æœå­—å…¸
        w_sep: separationæƒé‡ (1-9)
        w_pen: penaltyæƒé‡ (1-9)
    
    Returns:
        dict: {(k, dp): {'all_acc': float, 'l2_weighted': float, 'separation_score': float, 'penalty_score': float, ...}}
    """
    weighted_results = {}
    valid_count = 0
    skipped_count = 0
    
    for key, data in results_dict.items():
        separation_score = data.get('separation_score')
        penalty_score = data.get('penalty_score')
        
        # æ£€æŸ¥å¿…è¦æ•°æ®æ˜¯å¦å­˜åœ¨
        if separation_score is None or penalty_score is None:
            skipped_count += 1
            continue
        
        # è®¡ç®—åŠ æƒL2æŸå¤±
        # å…¬å¼ï¼šL2 = (penaltyæƒé‡ Ã— penaltyåˆ†æ•° - separationæƒé‡ Ã— separationåˆ†æ•°) / (w_sep + w_pen)
        # æ³¨æ„ï¼šseparation_scoreè¶Šå¤§è¶Šå¥½ï¼Œpenalty_scoreè¶Šå¤§è¶Šå·®
        weight_total = w_sep + w_pen
        if weight_total <= 0:
            raise ValueError("æƒé‡æ€»å’Œå¿…é¡»å¤§äº0")
        l2_weighted = (w_pen * penalty_score - w_sep * separation_score) / float(weight_total)
        
        # ä¿ç•™åŸå§‹æ•°æ®å¹¶æ·»åŠ L2è®¡ç®—ç»“æœ
        weighted_results[key] = {
            # åŸºç¡€æŒ‡æ ‡ï¼ˆæ¥è‡ªheatmap.pyè§£æï¼‰
            'all_acc': data.get('all_acc', 0.0),
            'old_acc': data.get('old_acc', 0.0),
            'new_acc': data.get('new_acc', 0.0),
            'n_clusters': data.get('n_clusters', 0),
            
            # L2ç›¸å…³æŒ‡æ ‡
            'l2_weighted': l2_weighted,
            'separation_score': separation_score,
            'penalty_score': penalty_score,
            'w_sep': w_sep,
            'w_pen': w_pen,
            
            # å¯é€‰æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            'quality_score': data.get('quality_score'),
            'labeled_acc': data.get('labeled_acc'),
            'l1_loss': data.get('l1_loss'),
            'silhouette': data.get('silhouette'),
            'db_score': data.get('db_score'),
            
            # K-meansåŸºçº¿ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            'kmeans_all_acc': data.get('kmeans_all_acc', 0.0),
            'kmeans_old_acc': data.get('kmeans_old_acc', 0.0),
            'kmeans_new_acc': data.get('kmeans_new_acc', 0.0)
        }
        valid_count += 1
    
    print(f"ğŸ“Š L2è®¡ç®—å®Œæˆ: æœ‰æ•ˆ={valid_count}, è·³è¿‡={skipped_count}, æƒé‡=(sep={w_sep}, pen={w_pen})")
    
    return weighted_results


def generate_l2_report(results_dict: Dict, w_sep: float, w_pen: float, 
                      superclass_name: str, output_path: str) -> Tuple[list, dict]:
    """
    ç”ŸæˆL2æƒé‡æ¢ç´¢æ–‡æœ¬æŠ¥å‘Š
    
    Args:
        results_dict: åŠ æƒç»“æœå­—å…¸
        w_sep: separationæƒé‡
        w_pen: penaltyæƒé‡
        superclass_name: è¶…ç±»åç§°
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    Returns:
        tuple: (top3_list, stats_dict)
    """
    if not results_dict:
        print(f"âš ï¸  æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return [], {}
    
    # æŒ‰L2æŸå¤±ä»å°åˆ°å¤§æ’åºï¼ˆæŸå¤±è¶Šå°è¶Šå¥½ï¼‰
    sorted_results = sorted(results_dict.items(), key=lambda x: x[1]['l2_weighted'])
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    l2_values = [data['l2_weighted'] for _, data in sorted_results]
    stats = {
        'count': len(sorted_results),
        'min_l2': min(l2_values),
        'max_l2': max(l2_values),
        'mean_l2': np.mean(l2_values),
        'std_l2': np.std(l2_values)
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write(f"L2æƒé‡æ¢ç´¢æŠ¥å‘Š - {superclass_name}\n")
        f.write(f"æƒé‡é…ç½®: separation={w_sep}, penalty={w_pen} (sum={w_sep + w_pen})\n")
        f.write("=" * 60 + "\n\n")
        
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write(f"  å‚æ•°ç»„åˆæ€»æ•°: {stats['count']}\n")
        f.write(f"  L2æŸå¤±èŒƒå›´: [{stats['min_l2']:.4f}, {stats['max_l2']:.4f}]\n")
        f.write(f"  L2æŸå¤±å‡å€¼: {stats['mean_l2']:.4f} Â± {stats['std_l2']:.4f}\n\n")
        
        f.write("å‚æ•°ç»„åˆæ’åºï¼ˆæŒ‰L2æŸå¤±ä»å°åˆ°å¤§ï¼‰:\n\n")
        
        for rank, (key, data) in enumerate(sorted_results, 1):
            k, dp = key
            l2_loss = data['l2_weighted']
            all_acc = data['all_acc']
            separation = data['separation_score']
            penalty = data['penalty_score']
            
            # é€‰æ‹©æ’åæ ‡å¿—
            if rank == 1:
                emoji = "ğŸ¥‡"
            elif rank == 2:
                emoji = "ğŸ¥ˆ"
            elif rank == 3:
                emoji = "ğŸ¥‰"
            else:
                emoji = f"#{rank:2d}"
            
            f.write(f"{emoji} Rank {rank}: k={k}, dp={dp}\n")
            f.write(f"     L2 Loss: {l2_loss:.4f}\n")
            f.write(f"     all_acc: {all_acc:.4f}\n")
            f.write(f"     separation: {separation:.4f}\n")
            f.write(f"     penalty: {penalty:.4f}\n")
            
            # æ·»åŠ å¯é€‰å­—æ®µ
            if data.get('labeled_acc') is not None:
                f.write(f"     labeled_acc: {data['labeled_acc']:.4f}\n")
            if data.get('quality_score') is not None:
                f.write(f"     quality_score: {data['quality_score']:.4f}\n")
            if data.get('n_clusters') is not None:
                f.write(f"     n_clusters: {data['n_clusters']}\n")
            
            f.write("\n")
        
        f.write("=" * 60 + "\n")
        f.write(f"é…ç½®è¯¦æƒ…:\n")
        f.write(f"  æƒé‡: separation={w_sep}, penalty={w_pen}\n")
        f.write(f"  å…¬å¼: L2 = ({w_pen} Ã— penalty - {w_sep} Ã— separation) / 10\n")
        
        if sorted_results:
            best_key, best_data = sorted_results[0]
            f.write(f"  æœ€ä½³å‚æ•°: k={best_key[0]}, dp={best_key[1]}\n")
            f.write(f"  æœ€ä½³L2æŸå¤±: {best_data['l2_weighted']:.4f}\n")
    
    print(f"ğŸ“„ L2æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    # è¿”å›Top 3å’Œç»Ÿè®¡ä¿¡æ¯
    top3 = [(key, data) for key, data in sorted_results[:3]]
    return top3, stats


def get_weight_configurations(weight_sum: int = 10) -> list:
    """
    è·å–æ‰€æœ‰æƒé‡é…ç½®
    
    Args:
        weight_sum: åˆ†ç¦»åº¦ä¸æƒ©ç½šé¡¹æƒé‡ä¹‹å’Œï¼Œå†³å®šä¹ç»„é…ç½®çš„å°ºåº¦
    
    Returns:
        list: [(w_sep, w_pen), ...] æƒé‡é…ç½®åˆ—è¡¨
    """
    if not isinstance(weight_sum, int):
        raise TypeError("weight_sum å¿…é¡»ä¸ºæ•´æ•°")
    if weight_sum < 2:
        raise ValueError("weight_sum å¿…é¡»å¤§äºç­‰äº 2")
    
    configs = []
    prev_sep = 0
    max_sep = weight_sum - 1
    
    for idx in range(1, 10):
        # åŸºäºåŸå§‹1-9æ¯”ä¾‹æŒ‰æ€»å’Œç¼©æ”¾ï¼Œå¹¶ä¿æŒå•è°ƒé€’å¢
        scaled = int(round((idx / 10.0) * weight_sum))
        w_sep = max(1, min(max_sep, scaled))
        if w_sep <= prev_sep:
            w_sep = min(max_sep, prev_sep + 1)
        w_pen = weight_sum - w_sep
        if w_pen < 1:
            w_pen = 1
            w_sep = weight_sum - w_pen
        prev_sep = w_sep
        configs.append((w_sep, w_pen))
    
    return configs


def validate_l2_requirements(results_dict: Dict) -> Tuple[bool, str]:
    """
    éªŒè¯æ•°æ®æ˜¯å¦æ»¡è¶³L2æƒé‡æ¢ç´¢è¦æ±‚
    
    Args:
        results_dict: ä»load_raw_scoresè·å–çš„ç»“æœå­—å…¸
    
    Returns:
        tuple: (æ˜¯å¦æ»¡è¶³è¦æ±‚, è¯¦ç»†ä¿¡æ¯)
    """
    if not results_dict:
        return False, "ç»“æœå­—å…¸ä¸ºç©º"
    
    total_count = len(results_dict)
    sep_available = sum(1 for v in results_dict.values() if v.get('separation_score') is not None)
    pen_available = sum(1 for v in results_dict.values() if v.get('penalty_score') is not None)
    
    if sep_available == 0:
        return False, f"separation_scoreå®Œå…¨ç¼ºå¤± (0/{total_count})"
    
    if pen_available == 0:
        return False, f"penalty_scoreå®Œå…¨ç¼ºå¤± (0/{total_count})"
    
    valid_pairs = sum(1 for v in results_dict.values() 
                     if v.get('separation_score') is not None and v.get('penalty_score') is not None)
    
    coverage = valid_pairs / total_count if total_count > 0 else 0
    
    if coverage < 0.5:  # è‡³å°‘50%çš„æ•°æ®æœ‰æ•ˆ
        return False, f"æœ‰æ•ˆæ•°æ®è¦†ç›–ç‡ä¸è¶³: {coverage:.1%} ({valid_pairs}/{total_count})"
    
    info = f"âœ… æ•°æ®æ»¡è¶³è¦æ±‚: separation={sep_available}/{total_count}, penalty={pen_available}/{total_count}, è¦†ç›–ç‡={coverage:.1%}"
    return True, info


def analyze_weight_stability(all_results: Dict[Tuple[float, float], Dict]) -> Dict:
    """
    åˆ†æä¸åŒæƒé‡é…ç½®ä¸‹å‚æ•°ç»„åˆçš„ç¨³å®šæ€§
    
    Args:
        all_results: {(w_sep, w_pen): {(k, dp): result_data}} æ‰€æœ‰æƒé‡é…ç½®çš„ç»“æœ
    
    Returns:
        dict: ç¨³å®šæ€§åˆ†æç»“æœ
    """
    param_stability = {}  # {(k, dp): [å‡ºç°æ¬¡æ•°, å¹³å‡æ’å, æƒé‡é…ç½®åˆ—è¡¨]}
    
    # åˆ†ææ¯ä¸ªæƒé‡é…ç½®çš„Top 3
    for (w_sep, w_pen), results in all_results.items():
        if not results:
            continue
            
        # æŒ‰L2æŸå¤±æ’åºï¼Œè·å–Top 3
        sorted_results = sorted(results.items(), key=lambda x: x[1]['l2_weighted'])
        top3_params = [key for key, _ in sorted_results[:3]]
        
        # ç»Ÿè®¡æ¯ä¸ªå‚æ•°ç»„åˆçš„å‡ºç°æƒ…å†µ
        for rank, param_key in enumerate(top3_params, 1):
            if param_key not in param_stability:
                param_stability[param_key] = {'count': 0, 'ranks': [], 'configs': []}
            
            param_stability[param_key]['count'] += 1
            param_stability[param_key]['ranks'].append(rank)
            param_stability[param_key]['configs'].append((w_sep, w_pen))
    
    # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
    stability_analysis = {}
    for param_key, data in param_stability.items():
        k, dp = param_key
        count = data['count']
        avg_rank = np.mean(data['ranks'])
        
        stability_analysis[param_key] = {
            'k': k,
            'dp': dp, 
            'top3_frequency': count,
            'avg_rank': avg_rank,
            'stability_score': count / len(all_results),  # ç¨³å®šæ€§å¾—åˆ†
            'configs': data['configs']
        }
    
    # æŒ‰ç¨³å®šæ€§å¾—åˆ†æ’åº
    stable_params = sorted(stability_analysis.items(), 
                          key=lambda x: (x[1]['stability_score'], -x[1]['avg_rank']), 
                          reverse=True)
    
    return {
        'stability_ranking': stable_params,
        'total_configs': len(all_results),
        'analysis_summary': {
            'most_stable': stable_params[0] if stable_params else None,
            'stable_count': len([p for p in stable_params if p[1]['stability_score'] >= 0.5])
        }
    }
