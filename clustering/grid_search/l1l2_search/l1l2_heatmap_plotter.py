#!/usr/bin/env python3
"""L1+L2æƒé‡çƒ­åŠ›å›¾ç»˜åˆ¶æ¨¡å—

èŒè´£ï¼š
- å¯¹æ ‡ L2ï¼šæŒ‰æƒé‡é…ç½® (w_l1, w_sep, w_sil) ç”Ÿæˆçƒ­åŠ›å›¾ï¼Œ
  èƒŒæ™¯æŒ‰ ACC æŒ‡æ ‡ï¼ˆall/new/oldï¼‰ç€è‰²ï¼Œæ³¨é‡Šæ˜¾ç¤º combined_scoreï¼›
- ç”Ÿæˆå•ç»„ä»¶ï¼ˆl1_lossã€separation_scoreã€silhouetteï¼‰çƒ­åŠ›å›¾ä¿å­˜åœ¨ single_metrics/ã€‚
"""

from __future__ import annotations

import os
import sys
from typing import Dict, List, Optional, Tuple
import re
from config import grid_search_output_dir, l1l2_search_output_dir

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from multiprocessing import Pool

try:  # ä¸ L2 ä¿æŒä¸€è‡´çš„è¿›åº¦æ¡ä¾èµ–
    from tqdm import tqdm
except Exception:  # pragma: no cover
    tqdm = None

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from .l1l2_weight_calculator import (
    WeightTriplet,
    compute_weighted_l1l2,
    load_raw_scores,
    validate_l1l2_requirements,
)

# æ—§æµç¨‹ï¼ˆåˆ‡ç‰‡æ¨¡å¼ï¼‰å·²ç§»é™¤ä»¥å‡å°‘å†—ä½™


# ---------------------------------
# æ–°æµç¨‹ï¼šå¯¹æ ‡ L2 çš„æƒé‡é…ç½®çƒ­åŠ›å›¾
# ---------------------------------

def plot_weighted_l1l2_heatmap(
    weighted_results: Dict[Tuple[int, int], Dict],
    color_metric: str,
    superclass_name: str,
    output_dir: str,
    weights: WeightTriplet,
) -> Dict[str, float]:
    """ç»˜åˆ¶å•ä¸ªæƒé‡é…ç½®ä¸‹çš„åŠ æƒçƒ­åŠ›å›¾ï¼ˆå¯¹æ ‡ L2ï¼‰ã€‚

    - èƒŒæ™¯ï¼šcolor_metricï¼ˆall_acc/new_acc/old_accï¼‰ï¼Œè¶Šé«˜è¶Šå¥½ï¼ˆé…è‰²æ–¹å‘ä½¿ç”¨ 'viridis'ï¼‰ã€‚
    - æ³¨é‡Šï¼šcombined_scoreï¼ˆè¶Šå°è¶Šå¥½ï¼‰ã€‚
    - ä¿å­˜è·¯å¾„ï¼š{output_dir}/{superclass}/{all|new|old}/color_metric_*.png
    """

    if not weighted_results:
        return {}

    # ç›®å½•æ˜ å°„
    metric_dir_map = {'all_acc': 'all', 'new_acc': 'new', 'old_acc': 'old'}
    metric_short = metric_dir_map.get(color_metric, color_metric)
    superclass_output_dir = os.path.join(output_dir, superclass_name, metric_short)
    os.makedirs(superclass_output_dir, exist_ok=True)

    # ç½‘æ ¼ç»´åº¦
    k_values = sorted({k for k, _ in weighted_results.keys()})
    dp_values = sorted({dp for _, dp in weighted_results.keys()})
    if not k_values or not dp_values:
        return {}

    color_data = np.full((len(dp_values), len(k_values)), np.nan)
    display_data = np.full((len(dp_values), len(k_values)), np.nan)

    # é€‰æ‹©æœ€ä¼˜ç‚¹ï¼ˆcombined_score è¶Šå°è¶Šå¥½ï¼‰
    best: Optional[Tuple[float, int, int, float]] = None  # (combined, i, j, color_value)
    valid_entries = []

    for i, dp in enumerate(dp_values):
        for j, k in enumerate(k_values):
            entry = weighted_results.get((k, dp)) or {}
            color_val = entry.get(color_metric)
            comb = entry.get('combined_score')
            if color_val is not None:
                color_data[i, j] = float(color_val)
            if comb is not None:
                display_data[i, j] = float(comb)
                valid_entries.append((comb, i, j))
                if best is None or comb < best[0]:
                    best = (comb, i, j, float(color_val) if color_val is not None else np.nan)

    if not valid_entries:
        return {}

    fig, ax = plt.subplots(figsize=(12, 8))
    # color_data èƒŒæ™¯ï¼Œdisplay_data æ³¨é‡Š
    cbar_label = f'{color_metric} (background)'
    sns.heatmap(
        color_data,
        xticklabels=k_values,
        yticklabels=dp_values,
        annot=display_data,
        fmt='.4f',
        cmap='viridis',
        cbar_kws={'label': cbar_label},
        ax=ax
    )

    best_k = best_dp = None
    best_combined = None
    best_color_metric_value = None
    if best:
        best_combined, i, j, cval = best
        best_k = k_values[j]
        best_dp = dp_values[i]
        ax.plot(j + 0.5, i + 0.5, marker='*', markersize=16, color='red',
                markeredgecolor='white', markeredgewidth=1.2)
        best_color_metric_value = cval
    if best_color_metric_value is None:
        best_color_metric_value = 0.0

    weight_str = f"{int(weights.w_l1)}_{int(weights.w_sep)}_{int(weights.w_sil)}"
    title_detail = (
        f"Best: k={best_k}, dp={best_dp}, {color_metric}={best_color_metric_value:.2f}, combined={best_combined:.4f}"
        if best_k is not None else "Best: N/A"
    )
    ax.set_title(
        f"Weighted L1+L2 (w_l1={int(weights.w_l1)}, w_sep={int(weights.w_sep)}, w_sil={int(weights.w_sil)}) - {superclass_name}\n"
        f"{title_detail}",
        fontsize=14, fontweight='bold'
    )
    ax.set_xlabel('k', fontsize=12)
    ax.set_ylabel('density_percentile', fontsize=12)

    plt.tight_layout()
    filename = f"{color_metric}_{best_color_metric_value:.4f}_{weight_str}.png"
    output_path = os.path.join(superclass_output_dir, filename)
    plt.savefig(output_path, dpi=300)
    plt.close(fig)

    summary = {
        'file': output_path,
        'entries': len(valid_entries),
        'min_combined': float(np.nanmin(display_data)),
        'max_combined': float(np.nanmax(display_data))
    }
    if best:
        score, i, j, _ = best
        summary.update({
            'best_combined': score,
            'best_k': k_values[j],
            'best_density_percentile': dp_values[i],
            f'best_{color_metric}': best_color_metric_value
        })
    return summary


def _plot_single_weight_task(
    raw_results: Dict[Tuple[int, int], Dict],
    weights: WeightTriplet,
    superclass_name: str,
    output_dir: str,
    color_metrics: List[str],
    available_components: List[str],
) -> Dict[str, object]:
    try:
        weighted = compute_weighted_l1l2(raw_results, weights, available_components)
        if not weighted:
            return {'error': 'weighted_empty', 'weights': weights}
        summaries = []
        for metric in color_metrics:
            s = plot_weighted_l1l2_heatmap(
                weighted_results=weighted,
                color_metric=metric,
                superclass_name=superclass_name,
                output_dir=output_dir,
                weights=weights,
            )
            if s:
                summaries.append(s)
        return {
            'weights': (int(weights.w_l1), int(weights.w_sep), int(weights.w_sil)),
            'summaries': summaries,
        }
    except Exception as exc:  # pragma: no cover
        return {'error': str(exc), 'weights': weights}


def create_weighted_l1l2_heatmaps(
    raw_results: Dict[Tuple[int, int], Dict],
    weight_sets: List[WeightTriplet],
    superclass_name: str,
    output_dir: str,
    color_metrics: List[str],
    available_components: List[str],
    num_workers: Optional[int] = None,
) -> List[Dict[str, object]]:
    """æ‰¹é‡ç”Ÿæˆæ¯ä¸ªæƒé‡é…ç½®åœ¨å„ä¸ª ACC èƒŒæ™¯ä¸‹çš„çƒ­åŠ›å›¾ã€‚"""
    if not weight_sets:
        return []
    cpu_count = os.cpu_count() or 2
    max_default = max(1, cpu_count - 1)
    if num_workers is None:
        num_workers = max_default
    num_workers = max(1, min(num_workers, cpu_count))

    results: List[Dict[str, object]] = []
    if num_workers <= 1:
        for w in weight_sets:
            out = _plot_single_weight_task(raw_results, w, superclass_name, output_dir, color_metrics, available_components)
            if out and not out.get('error'):
                results.append(out)
        return results

    task_args = [
        (raw_results, w, superclass_name, output_dir, color_metrics, available_components) for w in weight_sets
    ]
    with Pool(processes=num_workers) as pool:
        iterator = pool.starmap(_plot_single_weight_task, task_args)
        if tqdm is not None:
            iterator = tqdm(iterator, total=len(task_args), desc=f"Heatmaps {superclass_name}")
        for out in iterator:
            if isinstance(out, dict) and out.get('error'):
                print(f"âš ï¸ æƒé‡ {out.get('weights')} ç”Ÿæˆå¤±è´¥: {out['error']}")
                continue
            if out:
                results.append(out)
    return results


def create_l1l2_component_heatmap(
    results_dict: Dict[Tuple[int, int], Dict],
    component_name: str,
    color_metric: str,
    superclass_name: str,
    output_dir: str,
) -> Dict:
    """ç»˜åˆ¶å•ç»„ä»¶çƒ­åŠ›å›¾åˆ° single_metrics/ï¼Œå¯¹æ ‡ L2ã€‚"""
    component_name = component_name.strip()
    if not component_name:
        return {}

    k_values = sorted({k for k, _ in results_dict.keys()})
    dp_values = sorted({dp for _, dp in results_dict.keys()})
    if not k_values or not dp_values:
        return {}

    color_data = np.full((len(dp_values), len(k_values)), np.nan)
    display_data = np.full((len(dp_values), len(k_values)), np.nan)

    # ç»„ä»¶å–å€¼åŠæ–¹å‘
    def get_component_value(metrics: Dict) -> Optional[float]:
        if component_name == 'l1_loss':
            val = metrics.get('l1_loss')
            return float(val) if val is not None else None
        if component_name == 'separation_score':
            from .l1l2_weight_calculator import _extract_separation  # å…¼å®¹æ–°æ—§æ ¼å¼
            val = _extract_separation(metrics)
            return float(val) if val is not None else None
        if component_name == 'silhouette':
            from .l1l2_weight_calculator import _extract_silhouette  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯
            val = _extract_silhouette(metrics)
            return float(val) if val is not None else None
        return None

    for i, dp in enumerate(dp_values):
        for j, k in enumerate(k_values):
            m = results_dict.get((k, dp))
            if not m:
                continue
            cval = get_component_value(m)
            if color_metric in m and m[color_metric] is not None:
                color_data[i, j] = float(m[color_metric])
            if cval is not None:
                display_data[i, j] = float(cval)

    if np.all(np.isnan(display_data)):
        return {}

    # ç»„ä»¶æ–¹å‘ï¼šl1_loss è¶Šå°è¶Šå¥½ï¼Œå…¶ä½™è¶Šå¤§è¶Šå¥½ï¼Œä»…å½±å“æ ‡é¢˜æè¿°
    orientation = 'minimize' if component_name == 'l1_loss' else 'maximize'

    fig, ax = plt.subplots(figsize=(12, 8))
    cbar_label = f'{color_metric} (background)'
    sns.heatmap(
        color_data,
        xticklabels=k_values,
        yticklabels=dp_values,
        annot=display_data,
        fmt='.4f',
        cmap='viridis',
        cbar_kws={'label': cbar_label},
        ax=ax
    )
    ax.set_title(
        f"Component: {component_name} (bg={color_metric}) - {superclass_name}\n"
        f"Annotation: {component_name} value; orientation: {orientation}",
        fontsize=14, fontweight='bold'
    )
    ax.set_xlabel('k', fontsize=12)
    ax.set_ylabel('density_percentile', fontsize=12)

    plt.tight_layout()
    superclass_dir = os.path.join(output_dir, superclass_name, 'single_metrics')
    os.makedirs(superclass_dir, exist_ok=True)
    filename = f"component_{component_name}_colored_by_{color_metric}.png"
    path = os.path.join(superclass_dir, filename)
    plt.savefig(path, dpi=300)
    plt.close(fig)
    print(f"ğŸ“ ç»„ä»¶çƒ­åŠ›å›¾å·²ä¿å­˜: {path}")
    return {'file': path}


def create_l1l2_single_metric_heatmap(
    results_dict: Dict[Tuple[int, int], Dict],
    metric: str,
    superclass_name: str,
    output_dir: str,
    save_plots: bool = True,
    higher_is_better: bool = True,
) -> Dict:
    """ç»˜åˆ¶ ACC å•æŒ‡æ ‡çƒ­åŠ›å›¾ï¼ˆèƒŒæ™¯ä¸æ³¨é‡Šå‡ä¸ºåŒä¸€æŒ‡æ ‡ï¼‰ã€‚"""

    if not results_dict:
        return {}

    k_values = sorted({k for k, _ in results_dict.keys()})
    dp_values = sorted({dp for _, dp in results_dict.keys()})
    if not k_values or not dp_values:
        return {}

    metric_data = np.full((len(dp_values), len(k_values)), np.nan)
    for i, dp in enumerate(dp_values):
        for j, k in enumerate(k_values):
            entry = results_dict.get((k, dp))
            if entry is None:
                continue
            val = entry.get(metric)
            if val is not None:
                metric_data[i, j] = float(val)

    if np.all(np.isnan(metric_data)):
        print(f"âš ï¸ æŒ‡æ ‡ {metric} å®Œå…¨ç¼ºå¤±ï¼Œè·³è¿‡å•æŒ‡æ ‡çƒ­åŠ›å›¾")
        return {}

    # é€‰å‡ºTop-3
    valid = []
    for i in range(metric_data.shape[0]):
        for j in range(metric_data.shape[1]):
            v = metric_data[i, j]
            if not np.isnan(v):
                valid.append((v, i, j))
    if not valid:
        return {}
    valid.sort(key=lambda x: x[0], reverse=higher_is_better)
    top3 = valid[:3]

    fig, ax = plt.subplots(figsize=(12, 8))
    cmap = 'viridis' if higher_is_better else 'viridis_r'
    cbar_label = f"{metric} ({'higher better' if higher_is_better else 'lower better'})"
    sns.heatmap(
        metric_data,
        xticklabels=k_values,
        yticklabels=dp_values,
        annot=True,
        fmt='.4f',
        cmap=cmap,
        cbar_kws={'label': cbar_label},
        ax=ax,
    )
    for rank, (val, i, j) in enumerate(top3, 1):
        ax.plot(j + 0.5, i + 0.5, marker='*', markersize=18,
                color='red' if rank == 1 else 'orange' if rank == 2 else 'yellow',
                markeredgecolor='white', markeredgewidth=1.0)

    ax.set_title(f'{metric} Single Metric Heatmap - {superclass_name}', fontsize=14, fontweight='bold')
    ax.set_xlabel('k', fontsize=12)
    ax.set_ylabel('density_percentile', fontsize=12)
    plt.tight_layout()

    if save_plots:
        superclass_dir = os.path.join(output_dir, superclass_name, 'single_metrics')
        os.makedirs(superclass_dir, exist_ok=True)
        filename = f"single_metric_{metric}.png"
        out_path = os.path.join(superclass_dir, filename)
        plt.savefig(out_path, dpi=300)
        print(f"ğŸ“ å•æŒ‡æ ‡çƒ­åŠ›å›¾å·²ä¿å­˜: {out_path}")
    plt.close(fig)

    return {
        'metric': metric,
        'top3': [(k_values[j], dp_values[i], float(val)) for val, i, j in top3],
        'min': float(np.nanmin(metric_data)),
        'max': float(np.nanmax(metric_data)),
        'mean': float(np.nanmean(metric_data)),
        'std': float(np.nanstd(metric_data)),
    }


def generate_l1l2_report(
    weighted_results: Dict[Tuple[int, int], Dict],
    weights: WeightTriplet,
    superclass_name: str,
    output_path: str,
) -> Tuple[List[Tuple[Tuple[int, int], Dict]], Dict]:
    """ç”Ÿæˆå•ä¸ªæƒé‡é…ç½®çš„æ–‡æœ¬æŠ¥å‘Šï¼ˆcombined_score è¶Šå°è¶Šå¥½ï¼‰ã€‚"""
    if not weighted_results:
        print("âš ï¸ æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
        return [], {}

    sorted_results = sorted(weighted_results.items(), key=lambda x: x[1]['combined_score'])
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    values = [d['combined_score'] for _, d in sorted_results]
    import numpy as _np
    stats = {
        'count': len(sorted_results),
        'min_combined': float(min(values)),
        'max_combined': float(max(values)),
        'mean_combined': float(_np.mean(values)),
        'std_combined': float(_np.std(values)),
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write(f"L1+L2 æƒé‡æ¢ç´¢æŠ¥å‘Š - {superclass_name}\n")
        f.write(
            f"æƒé‡é…ç½®: w_l1={int(weights.w_l1)}, w_sep={int(weights.w_sep)}, w_sil={int(weights.w_sil)} (sum={int(weights.total)})\n"
        )
        f.write("=" * 60 + "\n\n")
        f.write("ç»Ÿè®¡æ‘˜è¦:\n")
        f.write(f"  å‚æ•°ç»„åˆæ€»æ•°: {stats['count']}\n")
        f.write(f"  CombinedèŒƒå›´: [{stats['min_combined']:.4f}, {stats['max_combined']:.4f}]\n")
        f.write(f"  Combinedå‡å€¼: {stats['mean_combined']:.4f} Â± {stats['std_combined']:.4f}\n\n")
        f.write("å‚æ•°ç»„åˆæ’åºï¼ˆæŒ‰ Combined ä»å°åˆ°å¤§ï¼‰:\n\n")
        for rank, (key, data) in enumerate(sorted_results, 1):
            k, dp = key
            f.write(f"#{rank:2d} k={k}, dp={dp}, combined={data['combined_score']:.4f}, all_acc={data.get('all_acc')}, new_acc={data.get('new_acc')}, old_acc={data.get('old_acc')}\n")
    print(f"ğŸ“„ L1+L2æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    return sorted_results[:3], stats


def generate_l1l2_summary_report(
    all_stats: Dict[Tuple[int, int, int], Dict],
    superclass_name: str,
    output_path: str,
) -> None:
    """ç”Ÿæˆè·¨æƒé‡é…ç½®çš„æ±‡æ€»æŠ¥å‘Šï¼ˆå¯¹æ ‡ L2 çš„ summaryï¼‰ã€‚"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write(f"L1+L2 æƒé‡æ¢ç´¢æ±‡æ€»æŠ¥å‘Š - {superclass_name}\n")
        f.write("=" * 80 + "\n\n")
        f.write("æƒé‡é…ç½®ç»Ÿè®¡:\n")
        f.write("-" * 40 + "\n")
        for (wl1, wsep, wsil), stats in all_stats.items():
            f.write(f"é…ç½® w_l1={wl1}, w_sep={wsep}, w_sil={wsil}:\n")
            f.write(f"  å‚æ•°ç»„åˆæ•°: {stats.get('count', 0)}\n")
            f.write(f"  CombinedèŒƒå›´: [{stats.get('min_combined', float('nan')):.4f}, {stats.get('max_combined', float('nan')):.4f}]\n")
            f.write(f"  Combinedå‡å€¼: {stats.get('mean_combined', float('nan')):.4f} Â± {stats.get('std_combined', float('nan')):.4f}\n\n")
    print(f"ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {output_path}")


def plot_all_l1l2_configurations(
    superclass_name: str,
    search_dir: str = grid_search_output_dir,
    output_dir: str = l1l2_search_output_dir,
    weight_sets: Optional[List[WeightTriplet]] = None,
    color_metrics: Optional[List[str]] = None,
    weight_sum: int = 10,
    cleanup_reports: bool = True,
) -> Dict:
    """ç»Ÿä¸€è°ƒåº¦ï¼šå¯¹æ ‡ L2ï¼Œæ‰¹é‡ç»˜åˆ¶æ‰€æœ‰æƒé‡é…ç½®ã€‚"""
    if not color_metrics:
        color_metrics = ['new_acc', 'all_acc', 'old_acc']

    print("=" * 60)
    print(f"ğŸ¨ L1+L2æƒé‡æ¢ç´¢ - {superclass_name} (metrics: {','.join(color_metrics)}, weight_sum={weight_sum})")
    print("=" * 60)

    print("\nğŸ“‚ Step 1: åŠ è½½åŸå§‹æœç´¢æ•°æ®")
    raw_loaded = load_raw_scores(superclass_name, search_dir)
    if not raw_loaded:
        print(f"âŒ æ— æ³•åŠ è½½ {superclass_name} çš„æœç´¢ç»“æœ")
        return {}
    raw_data, available_components = raw_loaded

    valid, info = validate_l1l2_requirements(raw_data, available_components)
    if not valid:
        print(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {info}")
        return {}
    print(f"âœ… æ•°æ®éªŒè¯é€šè¿‡: {info}")

    if not weight_sets:
        from .l1l2_weight_calculator import get_weight_configurations
        weight_sets = get_weight_configurations(weight_sum)
    print(f"ğŸ”§ Step 2: å¤„ç† {len(weight_sets)} ç§æƒé‡é…ç½®")

    # æ¯ä¸ªæƒé‡é…ç½®ç”Ÿæˆä¸‰å¼ çƒ­åŠ›å›¾
    _ = create_weighted_l1l2_heatmaps(
        raw_data,
        weight_sets,
        superclass_name,
        output_dir,
        color_metrics=color_metrics,
        available_components=available_components,
        num_workers=None,
    )

    # å•ç»„ä»¶çƒ­åŠ›å›¾ï¼ˆä»¥ new_acc ä½œä¸ºèƒŒæ™¯ï¼‰
    try:
        create_l1l2_component_heatmap(raw_data, 'l1_loss', 'new_acc', superclass_name, output_dir)
        create_l1l2_component_heatmap(raw_data, 'separation_score', 'new_acc', superclass_name, output_dir)
        create_l1l2_component_heatmap(raw_data, 'silhouette', 'new_acc', superclass_name, output_dir)
    except Exception as exc:  # pragma: no cover
        print(f"âš ï¸ å•ç»„ä»¶çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {exc}")

    # å•ACCæŒ‡æ ‡çƒ­åŠ›å›¾ï¼ˆèƒŒæ™¯ä¸æ³¨é‡Šå‡åŒä¸€æŒ‡æ ‡ï¼‰
    try:
        create_l1l2_single_metric_heatmap(raw_data, 'all_acc', superclass_name, output_dir)
        create_l1l2_single_metric_heatmap(raw_data, 'new_acc', superclass_name, output_dir)
        create_l1l2_single_metric_heatmap(raw_data, 'old_acc', superclass_name, output_dir)
    except Exception as exc:  # pragma: no cover
        print(f"âš ï¸ å•æŒ‡æ ‡çƒ­åŠ›å›¾ç”Ÿæˆå¤±è´¥: {exc}")

    # ç®€è¦æ±‡æ€»æŠ¥å‘Šï¼ˆè·¨é…ç½®ï¼‰
    all_stats: Dict[Tuple[int, int, int], Dict] = {}
    for w in weight_sets:
        weighted = compute_weighted_l1l2(raw_data, w, available_components)
        if not weighted:
            continue
        top3, stats = generate_l1l2_report(
            weighted,
            w,
            superclass_name,
            os.path.join(output_dir, superclass_name, f"l1l2_report_wl1{int(w.w_l1)}_sep{int(w.w_sep)}_sil{int(w.w_sil)}.txt"),
        )
        all_stats[(int(w.w_l1), int(w.w_sep), int(w.w_sil))] = {'count': stats.get('count', 0),
                                                                 'min_combined': stats.get('min_combined', float('nan')),
                                                                 'max_combined': stats.get('max_combined', float('nan')),
                                                                 'mean_combined': stats.get('mean_combined', float('nan')),
                                                                 'std_combined': stats.get('std_combined', float('nan'))}

    generate_l1l2_summary_report(all_stats, superclass_name, os.path.join(output_dir, superclass_name, 'l1l2_weights_summary.txt'))

    if cleanup_reports:
        try:
            result = cleanup_intermediate_reports(superclass_name, output_dir, dry_run=False)
            if result.get('deleted_count', 0) > 0:
                freed_kb = result.get('freed_bytes', 0) / 1024.0
                print(f"ğŸ§¹ å·²æ¸…ç† {result['deleted_count']} ä¸ªä¸­é—´æŠ¥å‘Šï¼Œé‡Šæ”¾ç©ºé—´ {freed_kb:.2f} KB")
        except Exception as exc:  # pragma: no cover
            print(f"âš ï¸ æ¸…ç†ä¸­é—´æŠ¥å‘Šå¤±è´¥: {exc}")

    return {
        'superclass': superclass_name,
        'output_dir': os.path.join(output_dir, superclass_name),
        'color_metrics': color_metrics,
        'weight_sum': weight_sum,
    }


INTERMEDIATE_REPORT_PATTERN = re.compile(r'^l1l2_report_wl1\d+_sep\d+_sil\d+\.txt$')


def cleanup_intermediate_reports(superclass_name: str,
                                 output_dir: str,
                                 dry_run: bool = False) -> Dict[str, int]:
    """æ¸…ç†å•ä¸ªè¶…ç±»ç›®å½•ä¸‹çš„ä¸­é—´txtæŠ¥å‘Šæ–‡ä»¶ï¼Œä¿ç•™æ±‡æ€»æŠ¥å‘Šä¸PNGã€‚

    è¿”å› {'deleted_count': int, 'freed_bytes': int}
    """
    base_dir = os.path.join(output_dir, superclass_name)
    if not os.path.isdir(base_dir):
        return {'deleted_count': 0, 'freed_bytes': 0}
    deleted = 0
    freed = 0
    for name in os.listdir(base_dir):
        if not name.endswith('.txt'):
            continue
        if name == 'l1l2_weights_summary.txt':
            continue
        if not INTERMEDIATE_REPORT_PATTERN.match(name):
            continue
        path = os.path.join(base_dir, name)
        try:
            size = os.path.getsize(path)
        except OSError:
            size = 0
        if not dry_run:
            try:
                os.remove(path)
                deleted += 1
                freed += int(size)
            except OSError:
                # å¿½ç•¥åˆ é™¤å¤±è´¥
                pass
    return {'deleted_count': deleted, 'freed_bytes': freed}
