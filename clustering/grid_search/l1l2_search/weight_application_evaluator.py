#!/usr/bin/env python3
"""æƒé‡åº”ç”¨è¯„ä¼°æ ¸å¿ƒé€»è¾‘

æ–‡ä»¶ä½œç”¨ï¼š
- è§£æ findL åŒºåŸŸæœç´¢æŠ¥å‘Šä¸­çš„é‡åˆæƒé‡ (w_l1, w_sep, w_sil)
- ä»æŒ‡å®šä»»åŠ¡(searchç›®å½•)åŠ è½½å„è¶…ç±»çš„åŸå§‹æœç´¢ç»“æœä¸å¯ç”¨ç»„ä»¶
- å¤ç”¨ L1+L2 åŠ¨æ€åŠ æƒé€»è¾‘ï¼Œå¯¹æ¯ä¸ªæƒé‡Ã—è¶…ç±»è¯„ä¼°æœ€ä¼˜ç‚¹ACCä¸å…¨å±€æœ€ä½³ACCçš„å·®å€¼
- ç”Ÿæˆç»“æ„åŒ–ç»“æœä¾› CLI å…¥å£å†™å…¥æŠ¥å‘Š

å®ç°çº¦æŸï¼š
- ä»…ä¾èµ–åŸå§‹æœç´¢æ•°æ®ï¼ˆsearch_dir_base/{task_folder}/{superclass}/*.txtï¼‰
- ç»„ä»¶æŒ‰â€œåŠ¨æ€æ£€æµ‹ + è‡ªé€‚åº”åŠ æƒâ€ï¼Œä¸ç°æœ‰ L1+L2 æœç´¢ä¿æŒä¸€è‡´
"""

from __future__ import annotations

import os
import re
import sys
from statistics import median
from typing import Dict, List, Optional, Sequence, Tuple

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from clustering.grid_search.heatmap import detect_available_superclasses
from .l1l2_weight_calculator import (
    WeightTriplet,
    load_raw_scores,
    compute_weighted_l1l2,
)


def parse_findl_weights(report_path: str) -> List[WeightTriplet]:
    """ä» findL åŒºåŸŸæœç´¢æŠ¥å‘Šä¸­è§£ææ‰€æœ‰æƒé‡ç»„åˆã€‚

    å…¼å®¹è¡Œå½¢å¦‚ï¼š
      "#1 æƒé‡ç»„åˆ: (w_l1=1, w_sep=4, w_sil=8)"

    è¿”å›å»é‡åçš„ WeightTriplet åˆ—è¡¨ï¼ŒæŒ‰ (w_l1, w_sep, w_sil) å‡åºã€‚
    """
    if not os.path.isfile(report_path):
        raise FileNotFoundError(f"æŠ¥å‘Šä¸å­˜åœ¨: {report_path}")
    pattern = re.compile(r"w_l1\s*=\s*(\d+).*?w_sep\s*=\s*(\d+).*?w_sil\s*=\s*(\d+)")
    seen: set[Tuple[int, int, int]] = set()
    ordered: List[WeightTriplet] = []
    with open(report_path, 'r', encoding='utf-8') as f:
        for line in f:
            m = pattern.search(line)
            if not m:
                continue
            wl1, wsep, wsil = int(m.group(1)), int(m.group(2)), int(m.group(3))
            key = (wl1, wsep, wsil)
            if key in seen:
                continue
            seen.add(key)
            ordered.append(WeightTriplet(wl1, wsep, wsil))
    if not ordered:
        raise ValueError("æœªåœ¨æŠ¥å‘Šä¸­è§£æåˆ°ä»»ä½•æƒé‡é…ç½®ï¼ˆæœŸæœ›åŒ…å« w_l1=, w_sep=, w_sil=ï¼‰")
    # ä¿æŒæŠ¥å‘ŠåŸå§‹é¡ºåºè¿”å›
    return ordered


def load_task_data(search_dir_base: str, task_folder: str) -> Dict[str, Tuple[Dict, List[str]]]:
    """åŠ è½½ä»»åŠ¡ä¸‹æ‰€æœ‰è¶…ç±»çš„åŸå§‹æœç´¢æ•°æ®ä¸å¯ç”¨ç»„ä»¶ã€‚

    è¿”å› {superclass: (results_dict, available_components)}ã€‚
    è‹¥ä¸ªåˆ«è¶…ç±»åŠ è½½å¤±è´¥åˆ™è‡ªåŠ¨è·³è¿‡ã€‚
    """
    task_path = os.path.join(search_dir_base, task_folder)
    if not os.path.isdir(task_path):
        raise FileNotFoundError(f"ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {task_path}")

    superclasses = detect_available_superclasses(task_path)
    if not superclasses:
        raise ValueError(f"æœªåœ¨ä»»åŠ¡ç›®å½•ä¸­å‘ç°ä»»ä½•è¶…ç±»ç»“æœ: {task_path}")

    out: Dict[str, Tuple[Dict, List[str]]] = {}
    for sc in superclasses:
        try:
            loaded = load_raw_scores(sc, search_dir=task_path)
            if not loaded:
                print(f"âš ï¸ è¶…ç±» {sc} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            results_dict, available_components = loaded
            out[sc] = (results_dict, available_components)
            print(f"â„¹ï¸ è¶…ç±» {sc} å¯ç”¨ç»„ä»¶: {','.join(available_components)}")
        except Exception as exc:
            print(f"âš ï¸ è¶…ç±» {sc} åŠ è½½å¤±è´¥: {exc}")
    return out


def _best_acc_over_grid(results_dict: Dict, acc_mode: str) -> Optional[float]:
    best: Optional[float] = None
    for metrics in results_dict.values():
        v = metrics.get(acc_mode)
        if v is None:
            continue
        try:
            fv = float(v)
        except Exception:
            continue
        if best is None or fv > best:
            best = fv
    return best


def evaluate_weight_on_superclass(results_dict: Dict,
                                  available_components: List[str],
                                  weight: WeightTriplet,
                                  acc_mode: str) -> Tuple[Optional[float], Optional[float], Optional[float]]:
    """å¯¹å•ä¸ªè¶…ç±»åº”ç”¨å•ä¸ªæƒé‡ï¼Œè¿”å› (diff, optimal_acc, best_acc)ã€‚

    diff = optimal_acc - best_accï¼›è‹¥æ— æ³•è®¡ç®—åˆ™è¿”å› (None, None, best_acc)ã€‚
    """
    if not results_dict:
        return None, None, None

    best_acc = _best_acc_over_grid(results_dict, acc_mode)
    try:
        weighted = compute_weighted_l1l2(results_dict, weight, available_components)
        if not weighted:
            return None, None, best_acc
        # é€‰æ‹© combined_score æœ€å°çš„ç‚¹
        best_key, best_entry = min(weighted.items(), key=lambda kv: kv[1]['combined_score'])
        optimal_acc = weighted[best_key].get(acc_mode)
        if optimal_acc is None or best_acc is None:
            return None, optimal_acc if optimal_acc is not None else None, best_acc
        return float(optimal_acc) - float(best_acc), float(optimal_acc), float(best_acc)
    except Exception:
        return None, None, best_acc


def evaluate_all_weights(weights: Sequence[WeightTriplet],
                         task_data: Dict[str, Tuple[Dict, List[str]]],
                         acc_mode: str,
                         sort_by_avg: bool = False) -> List[Dict[str, object]]:
    """å¯¹æ‰€æœ‰æƒé‡åœ¨æ‰€æœ‰è¶…ç±»ä¸Šè¯„ä¼°ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœã€‚

    è¿”å›åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ ï¼š
        {
          'weight': (wl1,wsep,wsil),
          'results': {superclass: {'diff': float|None, 'optimal': float|None, 'best': float|None}},
          'stats': {'pos': int, 'neg': int, 'zero': int, 'fail': int, 'avg': float|None, 'median': float|None}
        }
    """
    outputs: List[Dict[str, object]] = []
    for w in weights:
        per_sc: Dict[str, Dict[str, Optional[float]]] = {}
        diffs: List[float] = []
        pos = neg = zero = fail = 0
        for sc, (results_dict, available_components) in task_data.items():
            diff, opt_acc, best_acc = evaluate_weight_on_superclass(results_dict, available_components, w, acc_mode)
            entry = {'diff': diff, 'optimal': opt_acc, 'best': best_acc}
            per_sc[sc] = entry
            if diff is None:
                fail += 1
            else:
                diffs.append(float(diff))
                if diff > 1e-12:
                    pos += 1
                elif diff < -1e-12:
                    neg += 1
                else:
                    zero += 1
        avg_val = float(sum(diffs) / len(diffs)) if diffs else None
        med_val = float(median(diffs)) if diffs else None
        outputs.append({
            'weight': (int(w.w_l1), int(w.w_sep), int(w.w_sil)),
            'results': per_sc,
            'stats': {'pos': pos, 'neg': neg, 'zero': zero, 'fail': fail, 'avg': avg_val, 'median': med_val},
        })

    # å¯é€‰æ’åºï¼šæŒ‰å¹³å‡å·®å€¼é™åºï¼ˆå¹³å‡å·®å€¼è¶Šå¤§è¶Šå¥½ï¼ŒNoneæ’åœ¨æœ€åï¼‰
    if sort_by_avg:
        outputs.sort(key=lambda x: x['stats']['avg'] if x['stats']['avg'] is not None else float('-inf'), reverse=True)
    return outputs


def generate_application_report(findl_report_path: str,
                                task_folder: str,
                                acc_mode: str,
                                results: List[Dict[str, object]],
                                report_dir: str,
                                sorted_by_avg: bool = False) -> str:
    """å°†è¯„ä¼°ç»“æœå†™å…¥æŠ¥å‘Šæ–‡ä»¶ï¼Œå¹¶è¿”å›è·¯å¾„ã€‚"""
    os.makedirs(report_dir, exist_ok=True)
    report_name = os.path.splitext(os.path.basename(findl_report_path))[0]
    out_path = os.path.join(report_dir, f"{report_name}_applied_to_{task_folder}.txt")

    lines: List[str] = []
    lines.append('=' * 40)
    lines.append('æƒé‡åº”ç”¨è¯„ä¼°æŠ¥å‘Š')
    lines.append('=' * 40)
    lines.append(f"æ¥æºæŠ¥å‘Š: {os.path.basename(findl_report_path)}")
    lines.append(f"ç›®æ ‡ä»»åŠ¡: {task_folder}")
    # è¶…ç±»æ•°é‡ï¼šä»ä¸€ä¸ªç»“æœé¡¹å–é•¿åº¦ï¼ˆè‹¥ä¸ºç©ºåˆ™0ï¼‰
    sc_count = 0
    if results:
        any_item = results[0]
        sc_count = len(any_item.get('results', {}))  # type: ignore[arg-type]
    lines.append(f"è¶…ç±»æ•°é‡: {sc_count}")
    lines.append(f"ACCæŒ‡æ ‡: {acc_mode}")
    lines.append(f"æƒé‡æ’åº: {'æŒ‰å¹³å‡å·®å€¼é™åº' if sorted_by_avg else 'æŒ‰findLæŠ¥å‘ŠåŸåº'}")
    import datetime as _dt
    lines.append(f"è¯„ä¼°æ—¶é—´: {_dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append('=' * 40)
    lines.append('')

    for idx, item in enumerate(results, 1):
        wl1, wsep, wsil = item['weight']  # type: ignore[assignment]
        stats = item['stats']  # type: ignore[assignment]
        per_sc: Dict[str, Dict[str, Optional[float]]] = item['results']  # type: ignore[assignment]
        lines.append(f"æƒé‡{idx} (w_l1:{wl1}, w_sep:{wsep}, w_sil:{wsil}):")
        for sc in sorted(per_sc.keys()):
            r = per_sc[sc]
            d = r.get('diff')
            opt = r.get('optimal')
            bst = r.get('best')
            if d is None:
                lines.append(f"  {sc}: N/A")
            else:
                sign = '+' if d > 0 else ''
                if opt is None or bst is None:
                    lines.append(f"  {sc}: {sign}{d:.3f}")
                else:
                    lines.append(f"  {sc}: {sign}{d:.3f} ({opt:.3f}/{bst:.3f})")
        lines.append(f"  [æ­£å‘: {stats['pos']}, è´Ÿå‘: {stats['neg']}, æŒå¹³: {stats['zero']}, å¤±è´¥: {stats['fail']}]")
        avg_val = stats.get('avg')
        med_val = stats.get('median')
        if avg_val is not None and med_val is not None:
            lines.append(f"  [å¹³å‡å·®å€¼: {avg_val:.3f}, ä¸­ä½æ•°å·®å€¼: {med_val:.3f}]")
        lines.append('')

    with open(out_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    print(f"ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {out_path}")
    return out_path


def generate_simplified_report(findl_report_path: str,
                               task_folder: str,
                               acc_mode: str,
                               results: List[Dict[str, object]],
                               report_dir: str,
                               weights: Sequence[WeightTriplet]) -> str:
    """ç”Ÿæˆç®€åŒ–ç‰ˆè¯„ä¼°æŠ¥å‘Šï¼Œä»…æ˜¾ç¤ºæ¯ä¸ªæƒé‡çš„å¹³å‡å·®å€¼ã€‚

    - ä¿æŒä¼ å…¥ results çš„é¡ºåºï¼ˆå— --sort æ§åˆ¶ï¼‰ã€‚
    - æ–‡ä»¶åï¼š{report_name}_applied_to_{task_folder}_simplified.txt
    """
    os.makedirs(report_dir, exist_ok=True)
    report_name = os.path.splitext(os.path.basename(findl_report_path))[0]
    out_path = os.path.join(report_dir, f"{report_name}_applied_to_{task_folder}_simplified.txt")

    lines: List[str] = []
    lines.append('=' * 60)
    lines.append('æƒé‡åº”ç”¨è¯„ä¼°æŠ¥å‘Š (ç®€åŒ–ç‰ˆ)')
    lines.append('=' * 60)
    lines.append(f"æ¥æºæŠ¥å‘Š: {os.path.basename(findl_report_path)}")
    lines.append(f"ç›®æ ‡ä»»åŠ¡: {task_folder}")
    lines.append(f"ACCæŒ‡æ ‡: {acc_mode}")
    lines.append("æƒé‡æ’åº: ä¿æŒfindLæŠ¥å‘ŠåŸåº")
    import datetime as _dt
    lines.append(f"è¯„ä¼°æ—¶é—´: {_dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append('=' * 60)
    lines.append('')

    # æ„å»º (wl1,wsep,wsil) -> result æ˜ å°„
    results_map: Dict[Tuple[int, int, int], Dict[str, object]] = {}
    for item in results:
        key = item['weight']  # type: ignore[assignment]
        results_map[key] = item  # type: ignore[assignment]

    # æŒ‰åŸå§‹ weights é¡ºåºè¾“å‡º
    for idx, w in enumerate(weights, 1):
        wl1, wsep, wsil = int(w.w_l1), int(w.w_sep), int(w.w_sil)
        item = results_map.get((wl1, wsep, wsil))
        if not item:
            continue
        stats = item['stats']  # type: ignore[assignment]
        avg_val = stats.get('avg')
        weight_str = f"[w_l1:{wl1}, w_sep:{wsep}, w_sil:{wsil}]"
        if avg_val is not None:
            sign = '+' if avg_val > 0 else ''
            avg_str = f"{sign}{avg_val:.3f}"
        else:
            avg_str = "N/A"
        lines.append(f"æƒé‡{idx} {weight_str} å¹³å‡å·®å€¼: {avg_str}")

    lines.append('')
    lines.append('=' * 60)

    with open(out_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    print(f"ğŸ“„ ç®€åŒ–ç‰ˆæŠ¥å‘Šå·²ä¿å­˜: {out_path}")
    return out_path
