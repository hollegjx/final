#!/usr/bin/env python3
"""
ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆGCDçš„test_kmeanså‡½æ•°åˆ›å»ºçš„è¯„ä¼°è„šæœ¬
å®Œå…¨å¤åˆ¶åŸç‰ˆé€»è¾‘ï¼Œä¸åšä»»ä½•ä¿®æ”¹
"""

import argparse
import torch
import numpy as np
from tqdm import tqdm
import os

# å¯¼å…¥åŸç‰ˆGCDçš„æ¨¡å—ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import CIFAR100_SUPERCLASSES
from methods.contrastive_training.contrastive_training import test_kmeans_superclass_eval
from models import vision_transformer as vits
from config import dino_pretrain_path, exp_root
from project_utils.general_utils import str2bool
from project_utils.cluster_utils import log_accs_from_preds
from sklearn.cluster import KMeans
from data.augmentations import get_transform


def test_kmeans_original(model, test_loader, epoch, save_name, args, device=None):
    """
    å®Œå…¨å¤åˆ¶åŸç‰ˆGCDçš„test_kmeanså‡½æ•°ï¼Œä¸€ä¸ªå­—éƒ½ä¸æ”¹
    """
    model.eval()

    all_feats = []
    targets = np.array([])
    mask = np.array([])

    print('Collating features...')
    # First extract all features
    for batch_idx, (images, label, _) in enumerate(tqdm(test_loader)):
        if device is None:
            device = torch.device('cuda:0')  # é»˜è®¤è®¾å¤‡
        images = images.to(device)

        # Pass features through base model and then additional learnable transform (linear layer)
        feats = model(images)

        feats = torch.nn.functional.normalize(feats, dim=-1)

        all_feats.append(feats.cpu().numpy())
        targets = np.append(targets, label.cpu().numpy())
        mask = np.append(mask, np.array([True if x.item() in range(len(args.train_classes))
                                         else False for x in label]))

    # -----------------------
    # K-MEANS
    # -----------------------
    print('Fitting K-Means...')
    all_feats = np.concatenate(all_feats)
    kmeans = KMeans(n_clusters=args.num_labeled_classes + args.num_unlabeled_classes, random_state=0, n_init=10).fit(all_feats)
    preds = kmeans.labels_
    print('Done!')

    # -----------------------
    # EVALUATE
    # -----------------------
    all_acc, old_acc, new_acc = log_accs_from_preds(y_true=targets, y_pred=preds, mask=mask,
                                                    T=epoch, eval_funcs=args.eval_funcs, save_name=save_name,
                                                    writer=args.writer)

    return all_acc, old_acc, new_acc


def load_model_original_way(model_path, args, device):
    """
    æŒ‰ç…§åŸç‰ˆGCDçš„æ–¹å¼åŠ è½½æ¨¡å‹
    """
    print(f"ğŸ”„ æŒ‰ç…§åŸç‰ˆGCDæ–¹å¼åŠ è½½æ¨¡å‹: {model_path}")

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    # æŒ‰ç…§åŸç‰ˆæ–¹å¼æ„å»ºæ¨¡å‹
    if args.base_model == 'vit_dino':
        model = vits.__dict__['vit_base']()

        # å…ˆåŠ è½½DINOé¢„è®­ç»ƒæƒé‡
        if os.path.exists(dino_pretrain_path):
            print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡: {dino_pretrain_path}")
            state_dict = torch.load(dino_pretrain_path, map_location='cpu')
            model.load_state_dict(state_dict)

        # å†åŠ è½½è®­ç»ƒåçš„æƒé‡
        print(f"   åŠ è½½è®­ç»ƒæƒé‡...")
        trained_state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(trained_state_dict)

        model.to(device)

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! ä½¿ç”¨768ç»´base modelç‰¹å¾è¿›è¡Œæµ‹è¯•")
        return model
    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.base_model}")


def evaluate_full_dataset(model, args, device):
    """
    åœ¨å®Œæ•´CIFAR-100æ•°æ®é›†ä¸Šè¯„ä¼°
    """
    print("\n" + "="*80)
    print("ğŸŒ å®Œæ•´CIFAR-100æ•°æ®é›†è¯„ä¼° (åŸç‰ˆGCDæ–¹å¼)")
    print("="*80)

    # æŒ‰ç…§åŸç‰ˆè®¾ç½®å‚æ•°
    args_eval = argparse.Namespace(**vars(args))
    args_eval.dataset_name = 'cifar100'

    # è·å–ç±»åˆ«åˆ’åˆ†
    args_eval = get_class_splits(args_eval)
    args_eval.num_labeled_classes = len(args_eval.train_classes)
    args_eval.num_unlabeled_classes = len(args_eval.unlabeled_classes)

    print(f"ğŸ“Š å·²çŸ¥ç±»åˆ«: {len(args_eval.train_classes)}ä¸ª")
    print(f"ğŸ“Š æœªçŸ¥ç±»åˆ«: {len(args_eval.unlabeled_classes)}ä¸ª")
    print(f"ğŸ“Š æ€»èšç±»æ•°: {args_eval.num_labeled_classes + args_eval.num_unlabeled_classes}")

    # è·å–æ•°æ®é›†å’Œtransform (æŒ‰ç…§åŸç‰ˆcontrastive_training.pyæ–¹å¼)
    train_transform, test_transform = get_transform(args_eval.transform, image_size=args_eval.image_size, args=args_eval)

    train_dataset, test_dataset, unlabelled_train_examples_test, datasets = get_datasets(
        args_eval.dataset_name, train_transform, test_transform, args_eval
    )

    print(f"ğŸ” æ•°æ®é…ç½®æ£€æŸ¥:")
    print(f"   Transformç±»å‹: {args_eval.transform}")
    print(f"   å›¾åƒå¤§å°: {args_eval.image_size}")
    print(f"   æ•°æ®é›†åç§°: {args_eval.dataset_name}")

    # æ£€æŸ¥å®é™…çš„å›¾åƒå¤§å°
    sample_image, _, _ = test_dataset[0]
    print(f"   å®é™…å›¾åƒshape: {sample_image.shape}")

    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        num_workers=args_eval.num_workers,
        batch_size=args_eval.batch_size,
        shuffle=False
    )

    # ä½¿ç”¨åŸç‰ˆtest_kmeanså‡½æ•°
    all_acc, old_acc, new_acc = test_kmeans_original(
        model, test_loader, epoch=0, save_name='Original_GCD_Full_Test',
        args=args_eval, device=device
    )

    print(f"ğŸ“ˆ å®Œæ•´æ•°æ®é›†è¯„ä¼°ç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")

    return all_acc, old_acc, new_acc


def evaluate_superclass(model, superclass_name, args, device):
    """
    åœ¨æŒ‡å®šè¶…ç±»ä¸Šè¯„ä¼°
    """
    print(f"\n" + "="*80)
    print(f"ğŸ¯ è¶…ç±» '{superclass_name}' è¯„ä¼° (åŸç‰ˆGCDæ–¹å¼)")
    print("="*80)

    if superclass_name not in CIFAR100_SUPERCLASSES:
        print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{superclass_name}'")
        return None, None, None

    # æŒ‰ç…§åŸç‰ˆè®¾ç½®å‚æ•°
    args_eval = argparse.Namespace(**vars(args))
    args_eval.dataset_name = 'cifar100'

    # è·å–ç±»åˆ«åˆ’åˆ†
    args_eval = get_class_splits(args_eval)
    args_eval.num_labeled_classes = len(args_eval.train_classes)
    args_eval.num_unlabeled_classes = len(args_eval.unlabeled_classes)

    # è·å–æ•°æ®é›†å’Œtransform
    train_transform, test_transform = get_transform(args_eval.transform, image_size=args_eval.image_size, args=args_eval)

    train_dataset, test_dataset, unlabelled_train_examples_test, datasets = get_datasets(
        args_eval.dataset_name, train_transform, test_transform, args_eval
    )

    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        num_workers=args_eval.num_workers,
        batch_size=args_eval.batch_size,
        shuffle=False
    )

    # ä½¿ç”¨åŸç‰ˆçš„è¶…ç±»è¯„ä¼°å‡½æ•°
    all_acc, old_acc, new_acc = test_kmeans_superclass_eval(
        model, test_loader, epoch=0, save_name=f'Original_GCD_Superclass_{superclass_name}_Test',
        args=args_eval, eval_superclass=superclass_name, device=device
    )

    return all_acc, old_acc, new_acc


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æŒ‰ç…§åŸç‰ˆGCDæ–¹å¼è¯„ä¼°æ¨¡å‹')

    # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    parser.add_argument('--model_path', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')

    # è¯„ä¼°æ¨¡å¼
    parser.add_argument('--eval_mode', type=str, choices=['full', 'superclass'],
                        default='full', help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¶…ç±»åç§° (å½“eval_mode=superclassæ—¶ä½¿ç”¨)')

    # æ¨¡å‹é…ç½® (ä¸åŸç‰ˆGCDä¸€è‡´)
    parser.add_argument('--base_model', type=str, default='vit_dino')
    parser.add_argument('--feat_dim', default=768, type=int)
    parser.add_argument('--image_size', default=224, type=int)  # ViTå›ºå®šä½¿ç”¨224

    # æ•°æ®é›†é…ç½® (ä¸åŸç‰ˆGCDä¸€è‡´)
    parser.add_argument('--batch_size', default=256, type=int)
    parser.add_argument('--num_workers', default=8, type=int)
    parser.add_argument('--eval_funcs', nargs='+', default=['v1'], help='è¯„ä¼°å‡½æ•°')
    parser.add_argument('--transform', type=str, default='imagenet')  # CIFARç”¨imagenet transform resizeåˆ°224
    parser.add_argument('--prop_train_labels', type=float, default=0.5)
    parser.add_argument('--use_ssb_splits', type=str2bool, default=False)

    # è®¾å¤‡é…ç½®
    parser.add_argument('--gpu', default=0, type=int, help='GPUè®¾å¤‡ID')

    args = parser.parse_args()

    # è®¾å¤‡åˆå§‹åŒ–
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
        print(f"ğŸ’» ä½¿ç”¨GPUè®¾å¤‡: cuda:{args.gpu}")
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    # è®¾ç½®å¿…è¦å‚æ•°
    args.device = device
    args.writer = None
    args.interpolation = 3
    args.crop_pct = 0.875

    print("ğŸš€ åŸç‰ˆGCDè¯„ä¼°å·¥å…·")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¯„ä¼°æ¨¡å¼: {args.eval_mode}")
    if args.eval_mode == 'superclass':
        print(f"ç›®æ ‡è¶…ç±»: {args.superclass_name}")
    print("=" * 80)

    try:
        # æŒ‰ç…§åŸç‰ˆæ–¹å¼åŠ è½½æ¨¡å‹
        model = load_model_original_way(args.model_path, args, device)

        # æ ¹æ®è¯„ä¼°æ¨¡å¼è¿›è¡Œè¯„ä¼°
        if args.eval_mode == 'full':
            evaluate_full_dataset(model, args, device)

        elif args.eval_mode == 'superclass':
            if args.superclass_name is None:
                print("âŒ é”™è¯¯: è¶…ç±»è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --superclass_name")
                return
            evaluate_superclass(model, args.superclass_name, args, device)

        print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()