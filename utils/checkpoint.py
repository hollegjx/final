#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Checkpointç®¡ç†å·¥å…·æ¨¡å—
æä¾›æ¨¡åž‹ä¿å­˜ã€è¶…å‚æ•°è®°å½•ã€æ—§æ–‡ä»¶æ¸…ç†ç­‰ç»Ÿä¸€æŽ¥å£
"""

import os
import argparse
from datetime import datetime
from typing import Dict, Optional, Tuple
import torch
import torch.nn as nn


def save_hyperparameters_to_txt(
    txt_path: str,
    args: argparse.Namespace,
    current_epoch: int,
    metrics: Dict[str, float],
    model_path: str
) -> None:
    """
    ä¿å­˜è¶…å‚æ•°åˆ°txtæ–‡ä»¶

    Args:
        txt_path: è¶…å‚æ•°æ–‡ä»¶è·¯å¾„
        args: è®­ç»ƒå‚æ•°å‘½åç©ºé—´
        current_epoch: å½“å‰è®­ç»ƒè½®æ•°
        metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸ï¼ˆå¦‚{'all_acc': 0.79, 'old_acc': 0.76}ï¼‰
        model_path: å¯¹åº”çš„æ¨¡åž‹æ–‡ä»¶è·¯å¾„
    """
    # ç”Ÿæˆå½“å‰æ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    with open(txt_path, 'w', encoding='utf-8') as f:
        # æ ‡é¢˜
        f.write("=" * 80 + "\n")
        f.write("è®­ç»ƒè¶…å‚æ•°è®°å½•\n")
        f.write("=" * 80 + "\n")
        f.write(f"æ¨¡åž‹æ–‡ä»¶: {os.path.basename(model_path)}\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {timestamp}\n")
        f.write(f"è®­ç»ƒè½®æ•°: {current_epoch}/{getattr(args, 'epochs', 'N/A')}\n")
        f.write("\n")

        # æ€§èƒ½æŒ‡æ ‡
        if metrics:
            f.write("æ€§èƒ½æŒ‡æ ‡:\n")
            for key, value in metrics.items():
                if isinstance(value, float):
                    f.write(f"  - {key}: {value*100:.2f}%\n")
                else:
                    f.write(f"  - {key}: {value}\n")
            f.write("\n")

        # æ•°æ®é›†å‚æ•°
        f.write("=" * 80 + "\n")
        f.write("æ•°æ®é›†å‚æ•°\n")
        f.write("=" * 80 + "\n")
        dataset_params = [
            'dataset_name', 'superclass_name', 'batch_size',
            'prop_train_labels', 'seed', 'num_workers'
        ]
        for param in dataset_params:
            if hasattr(args, param):
                value = getattr(args, param)
                f.write(f"{param:25s}: {value}\n")
        f.write("\n")

        # æ¨¡åž‹å‚æ•°
        f.write("=" * 80 + "\n")
        f.write("æ¨¡åž‹å‚æ•°\n")
        f.write("=" * 80 + "\n")
        model_params = [
            'base_model', 'feat_dim', 'image_size',
            'mlp_out_dim', 'num_mlp_layers', 'grad_from_block'
        ]
        for param in model_params:
            if hasattr(args, param):
                value = getattr(args, param)
                f.write(f"{param:25s}: {value}\n")
        f.write("\n")

        # è®­ç»ƒå‚æ•°
        f.write("=" * 80 + "\n")
        f.write("è®­ç»ƒå‚æ•°\n")
        f.write("=" * 80 + "\n")
        training_params = [
            'epochs', 'lr', 'weight_decay', 'momentum',
            'warmup_teacher_temp', 'teacher_temp', 'warmup_teacher_temp_epochs'
        ]
        for param in training_params:
            if hasattr(args, param):
                value = getattr(args, param)
                f.write(f"{param:25s}: {value}\n")
        f.write("\n")

        # æŸå¤±å‡½æ•°å‚æ•°
        f.write("=" * 80 + "\n")
        f.write("æŸå¤±å‡½æ•°å‚æ•°\n")
        f.write("=" * 80 + "\n")
        loss_params = [
            'sup_weight', 'contrast_weight', 'temperature',
            'contrast_loss_weight', 'sup_con_weight'
        ]
        for param in loss_params:
            if hasattr(args, param):
                value = getattr(args, param)
                f.write(f"{param:25s}: {value}\n")
        f.write("\n")

        # å…¶ä»–å‚æ•°ï¼ˆè¿‡æ»¤æŽ‰å†…éƒ¨å±žæ€§å’Œå·²è®°å½•çš„å‚æ•°ï¼‰
        recorded_params = set(dataset_params + model_params + training_params + loss_params)
        other_params = []
        for attr in dir(args):
            if not attr.startswith('_') and attr not in recorded_params:
                value = getattr(args, attr)
                # è¿‡æ»¤æŽ‰æ–¹æ³•å’Œå¤æ‚å¯¹è±¡
                if not callable(value) and not isinstance(value, (type, type(None))):
                    other_params.append((attr, value))

        if other_params:
            f.write("=" * 80 + "\n")
            f.write("å…¶ä»–å‚æ•°\n")
            f.write("=" * 80 + "\n")
            for param, value in sorted(other_params):
                f.write(f"{param:25s}: {value}\n")
            f.write("\n")


def save_best_checkpoint_with_hyperparams(
    model: nn.Module,
    checkpoint_dir: str,
    filename: str,
    args: argparse.Namespace,
    current_epoch: int,
    metrics: Dict[str, float],
    old_checkpoint_path: Optional[str] = None
) -> Tuple[str, str]:
    """
    ä¿å­˜æœ€ä¼˜checkpointå¹¶ç”Ÿæˆå¯¹åº”çš„è¶…å‚æ•°æ–‡ä»¶ï¼ŒåŒæ—¶åˆ é™¤æ—§æ–‡ä»¶

    Args:
        model: è¦ä¿å­˜çš„æ¨¡åž‹
        checkpoint_dir: checkpointä¿å­˜ç›®å½•
        filename: æ–°çš„æ–‡ä»¶åï¼ˆå¦‚ "allacc_79_date_2025_11_13_16_10.pt"ï¼‰
        args: è®­ç»ƒå‚æ•°å‘½åç©ºé—´
        current_epoch: å½“å‰è®­ç»ƒè½®æ•°
        metrics: æ€§èƒ½æŒ‡æ ‡å­—å…¸
        old_checkpoint_path: æ—§çš„checkpointè·¯å¾„ï¼ˆç”¨äºŽåˆ é™¤ï¼‰

    Returns:
        (model_path, txt_path): æ–°ç”Ÿæˆçš„æ¨¡åž‹æ–‡ä»¶è·¯å¾„å’Œè¶…å‚æ•°æ–‡ä»¶è·¯å¾„

    Raises:
        RuntimeError: å¦‚æžœæ¨¡åž‹ä¿å­˜å¤±è´¥
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)

    # æž„å»ºæ–°æ–‡ä»¶è·¯å¾„
    model_path = os.path.join(checkpoint_dir, filename)
    txt_filename = filename.replace('.pt', '.txt').replace('.pth', '.txt')
    txt_path = os.path.join(checkpoint_dir, txt_filename)

    try:
        # ç¬¬1æ­¥ï¼šä¿å­˜æ¨¡åž‹
        print(f"ðŸ’¾ ä¿å­˜æœ€ä¼˜æ¨¡åž‹: {filename}")
        torch.save(model.state_dict(), model_path)

        # ç¬¬2æ­¥ï¼šä¿å­˜è¶…å‚æ•°
        print(f"ðŸ“ ç”Ÿæˆè¶…å‚æ•°è®°å½•: {txt_filename}")
        save_hyperparameters_to_txt(
            txt_path=txt_path,
            args=args,
            current_epoch=current_epoch,
            metrics=metrics,
            model_path=model_path
        )

        # ç¬¬3æ­¥ï¼šåˆ é™¤æ—§æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        if old_checkpoint_path and os.path.exists(old_checkpoint_path):
            try:
                # åˆ é™¤æ—§çš„.ptæ–‡ä»¶
                print(f"ðŸ—‘ï¸  åˆ é™¤æ—§æ¨¡åž‹: {os.path.basename(old_checkpoint_path)}")
                os.remove(old_checkpoint_path)

                # åˆ é™¤å¯¹åº”çš„.txtæ–‡ä»¶
                old_txt_path = old_checkpoint_path.replace('.pt', '.txt').replace('.pth', '.txt')
                if os.path.exists(old_txt_path):
                    print(f"ðŸ—‘ï¸  åˆ é™¤æ—§è¶…å‚æ•°è®°å½•: {os.path.basename(old_txt_path)}")
                    os.remove(old_txt_path)
            except OSError as e:
                # åˆ é™¤å¤±è´¥ä¸åº”ä¸­æ–­è®­ç»ƒï¼Œä»…æ‰“å°è­¦å‘Š
                print(f"âš ï¸  åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥ï¼ˆä¸å½±å“è®­ç»ƒï¼‰: {e}")

        print(f"âœ… Checkpointä¿å­˜æˆåŠŸ")
        return model_path, txt_path

    except Exception as e:
        # ä¿å­˜å¤±è´¥æ—¶æ¸…ç†å¯èƒ½ç”Ÿæˆçš„ä¸å®Œæ•´æ–‡ä»¶
        if os.path.exists(model_path):
            try:
                os.remove(model_path)
            except:
                pass
        if os.path.exists(txt_path):
            try:
                os.remove(txt_path)
            except:
                pass
        raise RuntimeError(f"ä¿å­˜checkpointå¤±è´¥: {e}")


def generate_checkpoint_filename(
    prefix: str,
    accuracy: float,
    timestamp: Optional[str] = None
) -> str:
    """
    ç”Ÿæˆæ ‡å‡†åŒ–çš„checkpointæ–‡ä»¶å

    Args:
        prefix: æ–‡ä»¶åå‰ç¼€ï¼ˆå¦‚ "allacc", "best_acc"ï¼‰
        accuracy: å‡†ç¡®çŽ‡ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
        timestamp: æ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

    Returns:
        æ ‡å‡†åŒ–çš„æ–‡ä»¶åï¼ˆå¦‚ "allacc_79_date_2025_11_13_16_10.pt"ï¼‰

    Examples:
        >>> generate_checkpoint_filename("allacc", 0.7920)
        'allacc_79_date_2025_11_13_16_10.pt'
    """
    if timestamp is None:
        timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M')

    acc_int = int(accuracy * 100)
    filename = f"{prefix}_{acc_int}_date_{timestamp}.pt"
    return filename
