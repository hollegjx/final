#!/usr/bin/env python3
"""
åŸç‰ˆGCDé¢„è®­ç»ƒæ¨¡å‹è¯„ä¼°è„šæœ¬

ä½¿ç”¨åŸç‰ˆGCDè®­ç»ƒå¾—åˆ°çš„model.ptå’Œprojection headï¼Œåœ¨è¶…ç±»å’Œå…¨æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°
æ”¯æŒï¼š
1. å…¨CIFAR-100æ•°æ®é›†è¯„ä¼°
2. æŒ‡å®šè¶…ç±»è¯„ä¼°
3. æ‰¹é‡è¶…ç±»è¯„ä¼°
"""

import argparse
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
import os

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from data.get_datasets import get_datasets, get_class_splits
from data.cifar100_superclass import CIFAR100_SUPERCLASSES
from methods.contrastive_training.contrastive_training import test_kmeans_superclass_eval, test_kmeans
from models import vision_transformer as vits
from config import dino_pretrain_path
from project_utils.general_utils import str2bool
from sklearn.cluster import KMeans


def load_pretrained_gcd_model(model_path, proj_head_path, args, device):
    """
    åŠ è½½åŸç‰ˆGCDé¢„è®­ç»ƒæ¨¡å‹

    Args:
        model_path: ä¸»æ¨¡å‹æƒé‡è·¯å¾„ (model.ptæˆ–model_best.pt)
        proj_head_path: æŠ•å½±å¤´æƒé‡è·¯å¾„ (model_proj_head.ptæˆ–model_proj_head_best.pt)
        args: å‚æ•°é…ç½®
        device: è®¾å¤‡

    Returns:
        model: åŠ è½½æƒé‡åçš„æ¨¡å‹
        projection_head: åŠ è½½æƒé‡åçš„æŠ•å½±å¤´
    """
    print(f"ğŸ”„ åŠ è½½åŸç‰ˆGCDé¢„è®­ç»ƒæ¨¡å‹...")
    print(f"   ä¸»æ¨¡å‹: {model_path}")
    print(f"   æŠ•å½±å¤´: {proj_head_path}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    if not os.path.exists(proj_head_path):
        raise FileNotFoundError(f"æŠ•å½±å¤´æ–‡ä»¶ä¸å­˜åœ¨: {proj_head_path}")

    # æ„å»ºæ¨¡å‹æ¶æ„ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    if args.base_model == 'vit_dino':
        model = vits.__dict__['vit_base']()

        # åŠ è½½DINOé¢„è®­ç»ƒæƒé‡ï¼ˆä½œä¸ºåŸºç¡€ï¼‰
        if os.path.exists(dino_pretrain_path):
            print(f"   åŠ è½½DINOé¢„è®­ç»ƒæƒé‡: {dino_pretrain_path}")
            dino_state_dict = torch.load(dino_pretrain_path, map_location='cpu')
            model.load_state_dict(dino_state_dict, strict=False)

        # åŠ è½½GCDè®­ç»ƒåçš„æƒé‡
        print(f"   åŠ è½½GCDè®­ç»ƒæƒé‡...")
        gcd_state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(gcd_state_dict)

        model.to(device)

        # æ„å»ºæŠ•å½±å¤´
        projection_head = vits.__dict__['DINOHead'](
            in_dim=args.feat_dim,
            out_dim=args.mlp_out_dim,
            nlayers=args.num_mlp_layers
        )

        # åŠ è½½æŠ•å½±å¤´æƒé‡
        print(f"   åŠ è½½æŠ•å½±å¤´æƒé‡...")
        proj_state_dict = torch.load(proj_head_path, map_location='cpu')
        projection_head.load_state_dict(proj_state_dict)
        projection_head.to(device)

        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, projection_head

    else:
        raise NotImplementedError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {args.base_model}")


def create_combined_model(base_model, projection_head):
    """
    åˆ›å»ºç»„åˆæ¨¡å‹ï¼Œä¾¿äºæ¨ç†
    """
    class CombinedModel(nn.Module):
        def __init__(self, base_model, projection_head):
            super().__init__()
            self.base_model = base_model
            self.projection_head = projection_head

        def forward(self, x):
            features = self.base_model(x)
            projected_features = self.projection_head(features)
            return projected_features

    return CombinedModel(base_model, projection_head)


def evaluate_on_full_dataset(model, args, device):
    """
    åœ¨å®Œæ•´CIFAR-100æ•°æ®é›†ä¸Šè¯„ä¼°
    """
    print("\n" + "="*80)
    print("ğŸŒ å®Œæ•´CIFAR-100æ•°æ®é›†è¯„ä¼°")
    print("="*80)

    # è·å–æ•°æ®é›†
    args_eval = argparse.Namespace(**vars(args))
    args_eval.dataset_name = 'cifar100'

    # è·å–ç±»åˆ«åˆ’åˆ†ï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
    args_eval = get_class_splits(args_eval)
    args_eval.num_labeled_classes = len(args_eval.train_classes)
    args_eval.num_unlabeled_classes = len(args_eval.unlabeled_classes)

    # ç¡®ä¿test_classesåŒ…å«æ‰€æœ‰ç±»åˆ«ï¼ˆä¸åŸç‰ˆGCDä¸€è‡´ï¼‰
    all_classes = list(args_eval.train_classes) + list(args_eval.unlabeled_classes)
    args_eval.test_classes = sorted(all_classes)

    print(f"ğŸ“Š å·²çŸ¥ç±»åˆ«: {list(args_eval.train_classes)} (å…±{args_eval.num_labeled_classes}ä¸ª)")
    print(f"ğŸ“Š æœªçŸ¥ç±»åˆ«: {list(args_eval.unlabeled_classes)} (å…±{args_eval.num_unlabeled_classes}ä¸ª)")
    print(f"ğŸ“Š æµ‹è¯•ç±»åˆ«: {len(args_eval.test_classes)}ä¸ªç±»åˆ« (æ‰€æœ‰å·²çŸ¥+æœªçŸ¥ç±»åˆ«)")

    train_loader, test_loader, unlabelled_train_loader, args_eval = get_datasets(
        args_eval.dataset_name,
        train_transform=None,  # ä½¿ç”¨é»˜è®¤transform
        test_transform=None,   # ä½¿ç”¨é»˜è®¤transform
        args=args_eval
    )

    model.eval()

    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    print(f"ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° (å…±{len(args_eval.test_classes)}ä¸ªç±»)...")
    print(f"ğŸ“Š ä½¿ç”¨è¯„ä¼°å‡½æ•°: {args_eval.eval_funcs}")
    print(f"ğŸ“Š K-meansèšç±»æ•°: {args_eval.num_labeled_classes + args_eval.num_unlabeled_classes}")

    all_acc, old_acc, new_acc = test_kmeans(
        model, test_loader, epoch=0, save_name='Full_Dataset_Test', args=args_eval, device=device
    )

    print(f"ğŸ“ˆ å®Œæ•´æ•°æ®é›†è¯„ä¼°ç»“æœ:")
    print(f"   All ACC: {all_acc:.4f}")
    print(f"   Old ACC: {old_acc:.4f}")
    print(f"   New ACC: {new_acc:.4f}")

    return all_acc, old_acc, new_acc


def evaluate_on_superclass(model, superclass_name, args, device):
    """
    åœ¨æŒ‡å®šè¶…ç±»ä¸Šè¯„ä¼°
    """
    print(f"\n" + "="*80)
    print(f"ğŸ¯ è¶…ç±» '{superclass_name}' è¯„ä¼°")
    print("="*80)

    if superclass_name not in CIFAR100_SUPERCLASSES:
        print(f"âŒ é”™è¯¯: æœªçŸ¥çš„è¶…ç±»åç§° '{superclass_name}'")
        return None, None, None

    # è·å–å®Œæ•´æ•°æ®é›†
    args_eval = argparse.Namespace(**vars(args))
    args_eval.dataset_name = 'cifar100'

    # è·å–ç±»åˆ«åˆ’åˆ†ï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
    args_eval = get_class_splits(args_eval)
    args_eval.num_labeled_classes = len(args_eval.train_classes)
    args_eval.num_unlabeled_classes = len(args_eval.unlabeled_classes)

    # ç¡®ä¿test_classesåŒ…å«æ‰€æœ‰ç±»åˆ«ï¼ˆä¸åŸç‰ˆGCDä¸€è‡´ï¼‰
    all_classes = list(args_eval.train_classes) + list(args_eval.unlabeled_classes)
    args_eval.test_classes = sorted(all_classes)

    print(f"ğŸ“Š å·²çŸ¥ç±»åˆ«: {list(args_eval.train_classes)} (å…±{args_eval.num_labeled_classes}ä¸ª)")
    print(f"ğŸ“Š æœªçŸ¥ç±»åˆ«: {list(args_eval.unlabeled_classes)} (å…±{args_eval.num_unlabeled_classes}ä¸ª)")
    print(f"ğŸ“Š æµ‹è¯•ç±»åˆ«: {len(args_eval.test_classes)}ä¸ªç±»åˆ« (æ‰€æœ‰å·²çŸ¥+æœªçŸ¥ç±»åˆ«)")

    train_loader, test_loader, unlabelled_train_loader, args_eval = get_datasets(
        args_eval.dataset_name,
        train_transform=None,
        test_transform=None,
        args=args_eval
    )

    model.eval()

    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œè¶…ç±»è¯„ä¼°
    superclass_classes = CIFAR100_SUPERCLASSES[superclass_name]
    print(f"ğŸ“Š è¶…ç±»åŒ…å«ç±»åˆ«: {superclass_classes}")
    print(f"ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    print(f"ğŸ“Š ä½¿ç”¨è¯„ä¼°å‡½æ•°: {args_eval.eval_funcs}")

    all_acc, old_acc, new_acc = test_kmeans_superclass_eval(
        model, test_loader, epoch=0, save_name=f'Superclass_{superclass_name}_Test',
        args=args_eval, eval_superclass=superclass_name, device=device
    )

    return all_acc, old_acc, new_acc


def evaluate_all_superclasses(model, args, device):
    """
    åœ¨æ‰€æœ‰è¶…ç±»ä¸Šè¿›è¡Œè¯„ä¼°
    """
    print("\n" + "="*80)
    print("ğŸ” æ‰€æœ‰è¶…ç±»æ‰¹é‡è¯„ä¼°")
    print("="*80)

    results = {}

    for superclass_name in CIFAR100_SUPERCLASSES.keys():
        try:
            all_acc, old_acc, new_acc = evaluate_on_superclass(model, superclass_name, args, device)
            if all_acc is not None:
                results[superclass_name] = {
                    'all_acc': all_acc,
                    'old_acc': old_acc,
                    'new_acc': new_acc
                }
                print(f"âœ… {superclass_name}: All {all_acc:.4f} | Old {old_acc:.4f} | New {new_acc:.4f}")
            else:
                print(f"âŒ {superclass_name}: è¯„ä¼°å¤±è´¥")
        except Exception as e:
            print(f"âŒ {superclass_name}: è¯„ä¼°å‡ºé”™ - {e}")

    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    print(f"\nğŸ“Š æ‰€æœ‰è¶…ç±»è¯„ä¼°æ±‡æ€»:")
    print(f"{'è¶…ç±»åç§°':<25} {'All ACC':<10} {'Old ACC':<10} {'New ACC':<10}")
    print("-" * 60)

    for superclass_name, result in results.items():
        print(f"{superclass_name:<25} {result['all_acc']:<10.4f} {result['old_acc']:<10.4f} {result['new_acc']:<10.4f}")

    return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¯„ä¼°åŸç‰ˆGCDé¢„è®­ç»ƒæ¨¡å‹')

    # æ¨¡å‹æ–‡ä»¶è·¯å¾„
    parser.add_argument('--model_path', type=str, required=True,
                        help='GCDæ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ (å¦‚: /path/to/model_best.pt)')
    parser.add_argument('--proj_head_path', type=str, required=True,
                        help='æŠ•å½±å¤´æƒé‡æ–‡ä»¶è·¯å¾„ (å¦‚: /path/to/model_proj_head_best.pt)')

    # è¯„ä¼°æ¨¡å¼
    parser.add_argument('--eval_mode', type=str, choices=['full', 'superclass', 'all_superclasses'],
                        default='full', help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--superclass_name', type=str, default=None,
                        help='æŒ‡å®šè¶…ç±»åç§° (å½“eval_mode=superclassæ—¶ä½¿ç”¨)')

    # æ¨¡å‹é…ç½® (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´)
    parser.add_argument('--base_model', type=str, default='vit_dino')
    parser.add_argument('--feat_dim', default=768, type=int)
    parser.add_argument('--mlp_out_dim', default=65536, type=int)
    parser.add_argument('--num_mlp_layers', default=3, type=int)

    # æ•°æ®é›†é…ç½®
    parser.add_argument('--batch_size', default=256, type=int, help='è¯„ä¼°æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', default=8, type=int)
    parser.add_argument('--eval_funcs', nargs='+', default=['v1'], help='è¯„ä¼°å‡½æ•°')

    # è®¾å¤‡é…ç½®
    parser.add_argument('--gpu', default=0, type=int, help='GPUè®¾å¤‡ID')

    args = parser.parse_args()

    # è®¾å¤‡åˆå§‹åŒ–
    if torch.cuda.is_available():
        device = torch.device(f'cuda:{args.gpu}')
        torch.cuda.set_device(args.gpu)
        print(f"ğŸ’» ä½¿ç”¨GPUè®¾å¤‡: cuda:{args.gpu}")
    else:
        device = torch.device('cpu')
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    # è®¾ç½®ä¸€äº›å¿…è¦çš„å‚æ•°
    args.device = device
    args.writer = None  # ä¸ä½¿ç”¨TensorBoard
    args.use_ssb_splits = False
    args.prop_train_labels = 0.5

    print("ğŸš€ åŸç‰ˆGCDæ¨¡å‹è¯„ä¼°å·¥å…·")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æŠ•å½±å¤´è·¯å¾„: {args.proj_head_path}")
    print(f"è¯„ä¼°æ¨¡å¼: {args.eval_mode}")
    if args.eval_mode == 'superclass':
        print(f"ç›®æ ‡è¶…ç±»: {args.superclass_name}")
    print("=" * 80)

    try:
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        base_model, projection_head = load_pretrained_gcd_model(
            args.model_path, args.proj_head_path, args, device
        )

        # åˆ›å»ºç»„åˆæ¨¡å‹
        model = create_combined_model(base_model, projection_head)

        # æ ¹æ®è¯„ä¼°æ¨¡å¼è¿›è¡Œè¯„ä¼°
        if args.eval_mode == 'full':
            evaluate_on_full_dataset(model, args, device)

        elif args.eval_mode == 'superclass':
            if args.superclass_name is None:
                print("âŒ é”™è¯¯: è¶…ç±»è¯„ä¼°æ¨¡å¼éœ€è¦æŒ‡å®š --superclass_name")
                return
            evaluate_on_superclass(model, args.superclass_name, args, device)

        elif args.eval_mode == 'all_superclasses':
            evaluate_all_superclasses(model, args, device)

        print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")

        # éªŒè¯è¯´æ˜
        print("\n" + "="*80)
        print("ğŸ“‹ è¯„ä¼°ä¸€è‡´æ€§éªŒè¯:")
        print("âœ… ä½¿ç”¨å®Œæ•´æµ‹è¯•é›† (åŒ…å«æ‰€æœ‰å·²çŸ¥+æœªçŸ¥ç±»åˆ«)")
        print("âœ… ä½¿ç”¨åŸç‰ˆGCDçš„ç±»åˆ«åˆ’åˆ† (CIFAR-100: 80å·²çŸ¥ + 20æœªçŸ¥)")
        print("âœ… ä½¿ç”¨ç›¸åŒçš„K-meansèšç±»æ•° (100ä¸ªèšç±»)")
        print("âœ… ä½¿ç”¨ç›¸åŒçš„ACCè®¡ç®—æ–¹æ³• (cluster_acc + Hungarianç®—æ³•)")
        print("âœ… ä½¿ç”¨ç›¸åŒçš„maskå®šä¹‰ (åŸºäºtrain_classes)")
        print("="*80)

    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()